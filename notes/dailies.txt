4/9/21
------
X -update cookiecutter template makefile and notes files
X -choose project name
X -final investigation of dearpygui vs kivy (+kivymd)
    [dearpygui mobile won't be ready in time. Use this if you want to build a desktop app, use kivy if you want to build mobile. Leaning mobile - dearpygui looks cool but I feel if I'm going to do something non-streamlit/dash, I should make it REALLY different. I'm now thinking the gans could be a problem though - gpt3 is just an api so that's fine, but I'd need to host the gans somewhere. This is meant to be pretty open ended though so I could do gan stuff on paperspace and then a voice app or something else for mobile (spoken/written bullet points -> speech in my voice?). Time may be a constraint though if I try to do both - gpt3 trial expires at end of June. Maybe learning fund can help out there.]
X -make new git project
X -write project requirements
X -write premortem
-write lessons from last time

4/10/21
-------
X -write lessons from last time
    X -documented various project ideas
    X -looked through openai repos for more ideas. Briefly tried out dall-e notebook.
X -try out openai pypi package to make sure I can communicate w/ gpt3 programmatically
    -if necessary, write my own helper functions to wrap common actions
X -choose method of loading api key and write func
    [dotfile or env var both supported]
X -define constants to easily access engine names by index and view prices
X -watch video on sirens (in this context, basically a way to create better combinations of inputs (e.g. images) than simple interpolation)
X -run dall-e and deep-daze demo notebooks in colab
-look into how to use various gan packages (install? clone?)
-make sure I can still access paperspace
    -maybe update dotfiles/envs etc. on machine if necessary

4/11/21
-------
-try out chronology lib for programmatic gpt3 access and see if it looks helpful compared to openai lib
    -if necessary, write my own helper functions to wrap common actions
    X -try a couple simple requests
X -make sure I can still access paperspace
    X -maybe update dotfiles/envs etc. on machine if necessary
    X -copy over openai dotfile
    X -create new machine and configure (some things changed)
        X -test that gpu is available
X -create prompts dir and write func to load_prompt
    X -allow data/prompts subdir in git (not very big)
X -write helper to print prompt in bold and resp in normal text
X -add jabberwocky imports to template nb
X -re-watched kilcher Clip video
-choose a first use case to work on
    -download/install a gan package?

4/12/21
-------
~ -more api exploration
    -try out chronology lib for programmatic gpt3 access and see if it looks helpful compared to openai lib
    -if necessary, write my own helper functions to wrap common actions
    ~ -check if there's a free gpt2 endpoint?
        [pypi library booste. API key is in gmail 55.]
        ~ -if not, maybe create mock request func to save credits (though ada is pretty cheap)
X -try free content filter in openai api
    X -write wrapper
X -udpate openai_auth func to set key automatically
X -finish signup for booste lib
-choose a first use case to work on
    -download/install a gan package?

4/13/21
-------
-try chronology lib
X -write booste api key loading func
X -port and finalize text_segment func for youtube transcription idea
X -document functions in utils
X -finish query_gpt3 func
    X -how to handle logprobs?
        [Just let user choose to return everything if they want.]
    X -try stream=True
        -write generator version of query_gpt3 (func must be one or the other)
    -add tab completion for extra params? Would take some py magic
X -create even shorter prompt to save tokens
X -port query_content_filter func
    X -finish documenting
-re-watch dall-e video

4/14/21
-------
X -port query_gpt3
    X -document
    X -save sample response and use it for better mocking
    X -write generator version of query_gpt3 (func must be one or the other)
        X -port and document
    -tab completion magic?
-try chronology lib
-try booste lib
-re-watch dall-e video

4/15/21
-------
_ -tab completion magic for query_gpt3 and generator?
    UPDATE: looked into this but doesn't look like openai api includes this anywhere. Good enough - I included the major kwargs.
X -rewrite 2 query functions into 1 merged function (stream=True returns a generator rather than yielding values)
-try chronology lib
-try booste lib
-look through misc.txt notes and decide on a first use case to try
    -if videos, start messing around with youtube transcription package
    -if gans, select a gan to start with and look into download/installation (dall-e, deep daze, etc.)
-re-watch dall-e video

4/16/21
-------
X -look through misc.txt notes and decide on a first use case to try (leaning heavily towards youtube transcription-related stuff)
    UPDATE: start with youtube transcription stuff. The jabberwocky name may make less sense here, as does the supposed requirement for a mobile/desktop app. I suppose if really necessary I could always push my gan mobile app to a separate project - probably easier to justify gpt3 payment if I demo something useful first.
~ -if youtube, start messing around with youtube transcription package
    ~ -do all/most videos have transcripts?
        UPDATE: I think all/most have auto-generated transcripts, but relatively few have manually generated ones.
    _ -maybe wrap text_segment func to work if we want the PREV n seconds instead of specifying start/end
        UPDATE: unnecessary atm. We can make the user facing version ask for last n seconds, but for now it makes sense to keep it this way.
    -try out "explain to a fifth grader" etc. prompts from docs
    -save some in prompts dir
    -consider cleaning up text_segment and related funcs to act on whole sentences when possible
-if gans, select a gan to start with and look into download/installation (dall-e, deep daze, etc.)

4/17/21
-------
X -try out "explain to a fifth grader" etc. prompts from docs
    X -save some in prompts dir
    X -try tuning ML simplifier prompt
    -consider cleaning up text_segment and related funcs to act on whole sentences when possible
-try some format fixing prompts (autogenerated -> manual transcript. Maybe use chunks from different videos to get some variety. This idea alone actually seems pretty powerful - a way to rapidly improve quality of speech to text.)

4/18/21
-------
X -save more generic ELI5 prompt (mine is very ml-focused. They provide a good jupiter example.)
-start tuning some custom prompts based on youtube transcripts (realized even academic lectures tend to have much more informal language than written papers)
X -update or write new load_prompt func:
    X -some prompts should allow us to embed input text into them (e.g. rather than being static like short_dates, we want to show a few examples (saved in txt file) and then append our new piece of text to generate based on)
    X -store recommended kwargs for calling query_gpt3 (e.g. simplify_ml really requires davinci to work well. Stop sequence should be specified as well.)
    X -add kwarg info to data dir (single json file mapping file name to kwargs? Or make each prompt file a txt/yaml file containing multiple fields?)
        UPDATE: give each prompt a subdir with a config.yaml file and prompt.txt file. Reasons in load_prompt func.
-consider cleaning up text_segment and related funcs to act on whole sentences when possible
-try some format fixing prompts (autogenerated -> manual transcript. Maybe use chunks from different videos to get some variety. This idea alone actually seems pretty powerful - a way to rapidly improve quality of speech to text.)

4/19/21
-------
-start tuning some custom prompts based on youtube transcripts (realized even academic lectures tend to have much more informal language than written papers)
-consider cleaning up text_segment and related funcs to act on whole sentences when possible
    X -error checking for times
X -try some format fixing prompts (autogenerated -> manual transcript. Maybe use chunks from different videos to get some variety. This idea alone actually seems pretty powerful - a way to rapidly improve quality of speech to text.)
    UPDATE: not perfect but this generally works quite well.
    X -try chaining <PUNCTUATE> - <ELI>
        UPDATE: didn't work very well. Maybe the input passage was too simple already, or maybe we need to provide a few examples from youtube transcripts.
X -add some printed outputs to get_transcripts() so I don't always have to check which are None
X -update load_prompt to separate printed reminder from main prompt

4/20/21
-------
-consider cleaning up text_segment and related funcs to act on whole sentences when possible
    ~ -consider: add punctuation once (to whole transcript) or add it to a small segment each time we ask for a summary/eli5 etc?
-start tuning some custom prompts based on youtube transcripts (realized even academic lectures tend to have much more informal language than written papers)
-try some other prompts post punctuation: summarize, simplify_ml (on an ml video)
-try out other tasks and add to prompts dir:
    -key point extraction
    -question generation

4/21/21
-------
~ -continue working on func to re-map punctuated text to timestamps. (Even if we don't do this all at once, we should probably cache results from translating chunks.)
    X -try re.split in place of split
    X -add strip results option to query_gpt3 (and prompt config files)
    X -try fuzzywuzzy-based approach to realignment
-consider cleaning up text_segment and related funcs to act on whole sentences when possible
-start tuning some custom prompts based on youtube transcripts (realized even academic lectures tend to have much more informal language than written papers)
-try some other prompts post punctuation: summarize, simplify_ml (on an ml video)
-try out other tasks and add to prompts dir:
    -key point extraction
    -question generation

4/22/21
-------
X -continue working on func to re-map punctuated text to timestamps. (Even if we don't do this all at once, we should probably cache results from translating chunks.)
    -evaluate fuzzywuzzy realignment approach on a couple more chunks
    -consider ways to improve:
        X -bigram similarity instead of single word
        X -wider candidate window (instead of last 3 words, use whole chunk? Must make sure we reindex correctly when appending to rows if length is less than expected)
-consider cleaning up text_segment and related funcs to act on whole sentences when possible
-start tuning some custom prompts based on youtube transcripts (realized even academic lectures tend to have much more informal language than written papers)
-try some other prompts post punctuation: summarize, simplify_ml (on an ml video)
-try out other tasks and add to prompts dir:
    -key point extraction
    -question generation

4/23/21
-------
~ -evaluate fuzzywuzzy realignment approach on a couple more chunks
~ -consider updating alignment func to update a chunk of a df instead of returning a new df?
    UPDATE: think it may be a good idea to write a class which maintains two copies of the df, original and punctuated. If the user asks for an overlapping chunk, maybe we can punctuate only the portion that needs it.
X -maybe update get_transcripts to check for british english manual transcripts (better than US generated + saves tokens)
X -add warnings to realignment func when max similarity score is low
-consider cleaning up text_segment and related funcs to act on whole sentences when possible

4/24/21
-------
~ -start designing obj that will track unpunct and punct dfs
    ~ -rewrite text_segment if necessary to (optionally?) return df slice/ids so we can avoid punctuating overlaps twice.
        UPDATE: currently returns relevant rows from df rather than str. May adjust interface as needed though.
    -consider cleaning up text_segment and related funcs to act on whole sentences when possible
X -add verbose option to load_prompt

4/25/21
-------
~ -continue building Transcript classes
    -rewrite text_segment if necessary to (optionally?) return df slice/ids so we can avoid punctuating overlaps twice.
    -add option to text_segment-related funcs to act on whole sentences when possible
    ~ -make unpunct transcript auto call gpt3 when accessing an unpunctuated chunk of df? First need a lot of scoping about desired interface.
        UPDATE: adding punctuated_chunk method which just is loc for PunctuatedTranscript and queries gpt3 for UnpunctTranscript. Mostly works but need to troubleshoot the update to self.df_punct (settingwithcopywarning caused error).

4/26/21
-------
~ -continue building Transcript classes
    X -fix settingwithcopyerror when punctuating part of df_punct
    X -rewrite time_segment/related methods to avoid re-punctuating rows we've already punctuated
    -add functionality to make text_segment-related funcs act on whole sentences when possible
-lots of docstrings to write for Transcript classes

4/27/21
-------
-continue building Transcript classes
    X -add option for time_segment to not punctuate text
    -consider: could we make text_segment-related funcs act on whole sentences when possible? generatedTrans can't determine this til after punctuation. Maybe we could select an extra wide window to be safe, punctuate it, and then select whole sentences? Or we could just simplify everything and punctuate the whole transcript from the start (either in chunks or all at once) at the cost of gpt3 tokens.
        -if possible and advisable, start building this functionality
    X -design interface for allowing multiple tasks post punct (just diff methods on generatedTranscript? Or allow user to pass in funcs somehow?)
        X -first draft of PromptManager class
        -select first task to try post-punctuation (maybe tldr?)
            _ -start building method
                UPDATE: new PromptManager class auto-loads all prompts in prompt dir so no need to do this.

4/28/21
-------
-revise PromptManager class
    ~ -add option to skip certain prompts or load dynamically?
        UPDATE: skip dynamic loading, but allow passing in a select number of prompts.
    ~ -clean up dynamic method generation (or find graceful way to do this)
-continue building Transcript classes
    -consider: could we make text_segment-related funcs act on whole sentences when possible? generatedTrans can't determine this til after punctuation. Maybe we could select an extra wide window to be safe, punctuate it, and then select whole sentences? Or we could just simplify everything and punctuate the whole transcript from the start (either in chunks or all at once) at the cost of gpt3 tokens.
        -if possible and advisable, start building this functionality
        -select first task to try post-punctuation (maybe tldr?)
-write draft of function to tie together Transcript.time_range() and PromptManager.query().

4/29/21
-------
-revise PromptManager class
    ~ -work on rigorous_partial issues in nb
        X -handling param order in cases it changes
        X -handling new kwargs
            UPDATE: tentatively mostly working? Need to stop it from mutating old func though.
    -update dynamic method generation in cls

4/30/21
-------
X -stop rigorous_partial from mutating original func (look at old copy_func func)
~ -test on more cases to see if it seems to work
    UPDATE: found and fixed bug where *args was handled incorrectly.
-add dynamic method generation to cls (hopefully going from func to method doesn't break anything)
-return to Transcript classes
    -consider: could we make text_segment-related funcs act on whole sentences when possible? generatedTrans can't determine this til after punctuation. Maybe we could select an extra wide window to be safe, punctuate it, and then select whole sentences? Or we could just simplify everything and punctuate the whole transcript from the start (either in chunks or all at once) at the cost of gpt3 tokens.

5/1/21
------
X -Fix newfound issue when sorting new_pars (instead of using x.default, maybe can use constant or index in old parms) if defaults have mixed types
X -test if renaming func works
    UPDATE: yes, but repr and str are not updated. Wrote class that can do this but haven't integrated it into the partial function yet. Might also provide a simpler way to do a lot stuff - not sure yet.
X -fix __defaults__ and __kwdefaults__ (only worked on wrapped func before)
-test on more cases to try to track down any other bugs
-add dynamic method generation to cls (hopefully going from func to method doesn't break anything)

5/2/21
------
X -revise partial func to use attach_repr
    UPDATE: technically didn't use attach_repr, but used same concept to update repr and str.
X -test on more cases to try to track down any other bugs
_ -add dynamic method generation to cls (hopefully going from func to method doesn't break anything)
    UPDATE: spent some time trying this but we still run into annoying issues (apparently generating methods always has these difficulties, even with functools version of partial). Decided it's good enough for now that I got Partial working and I should really just use a non-hacky method ('query' method) for this use case. No need to burn more GPT3 trial time on this.
X -port getindex
    _ -port attach_repr
        UPDATE: skip for now. Not really a common use case, I think.
    X -port rigorous_partial (rename?)
        UPDATE: renamed to Partial

5/3/21
------
X -finalize PromptManager
    X -port
-maybe clean up notebook a bit
-return to Transcript classes
    -consider: could we make text_segment-related funcs act on whole sentences when possible? generatedTrans can't determine this til after punctuation. Maybe we could select an extra wide window to be safe, punctuate it, and then select whole sentences? Or we could just simplify everything and punctuate the whole transcript from the start (either in chunks or all at once) at the cost of gpt3 tokens.
    -if possible and advisable, start building this functionality
    -select first task to try post-punctuation (maybe tldr?)
-write draft of function to tie together Transcript.time_range() and PromptManager.query().

5/4/21
------
X -maybe clean up notebook a bit
~ -return to Transcript classes
    X -consider: could we make text_segment-related funcs act on whole sentences when possible? generatedTrans can't determine this til after punctuation. Maybe we could select an extra wide window to be safe, punctuate it, and then select whole sentences? Or we could just simplify everything and punctuate the whole transcript from the start (either in chunks or all at once) at the cost of gpt3 tokens.
    ~ -if possible and advisable, start building this functionality
        UPDATE: trimmed off partial sentences at start, still need to do end.
    -select first task to try post-punctuation (maybe tldr?)
    X -update unpunct transcript class w/ promptmanager
    X -cleanup old code a bit
-write draft of function to tie together Transcript.time_range() and PromptManager.query().

5/5/21
------
X -trim off partial sentences from end of time_range_str
    -clean up methods/delete old code
    -test on generated transcript
-write draft of function/interface of some sort tying together transcript and external promptmanager
~ -flex task!: work on less fragile alternative to htools.Args
    UPDATE: see ipython session. Just need to test picklablility.

5/6/21
------
-debug time_range_str issue (not using punctuated rows but time_range is?)
    UPDATE: found bug cause (see nb03) but haven't fixed yet.
    -consider: add another option for punctuate (always, never, if_necessary)? Might be good to be able to be sure we can get an unpunctuated chunk sometimes.
    -clean up methods/delete old code
    -test on generated transcript
-write draft of function/interface of some sort tying together transcript and external promptmanager
X -flex task!: check Object picklability (see ipython session)
    UPDATE: fleshed out Object, renamed to Results, wrote barebones docs, fixed picklability

5/7/21
------
X -write module docstrings
    UPDATE: pretty sick so slept most of the day. Just spent a couple minutes knocking out an easy task.
X -find cause of readme updating issue
    UPDATE: need error handling in case dir has no relevant files. Check if df empty in self._parse_dir_files before sorting.
-consider desired behavior re time_range_str issue
    -consider: add another option for punctuate (always, never, if_necessary)? Might be good to be able to be sure we can get an unpunctuated chunk sometimes.
    -clean up methods/delete old code
    -test on generated transcript
-write draft of function/interface of some sort tying together transcript and external promptmanager

5/8/21
------
X -add error handling for empty dir in readme updater cli
~ -look into live speech recognition options with timestamps to extend beyond youtube
    UPDATE: pocketsphinx, but installing is proving to be a challenge. Come back to this later.
X -consider reorganizing lib structure (some utils probably belong in youtube or openai modules; part of core module belongs in openai and youtube modules)
X -fix time_range_str bug
    UPDATE: just add warning suggesting what to do.
X -fix text realignment bug
    UPDATE: thought I'd set max_tokens somewhere but maybe I deleted it. This meant we were only receving 50 tokens back from gpt3 but many more rows of df, so realignment wasn't working properly.
~ -consider desired behavior re time_range_str issue
    UPDATE: Think I should add an always/never/if_necessary option as suggested below. Haven't implemented yet though.
    -begin implementing
    -consider: add another option for punctuate (always, never, if_necessary)? Might be good to be able to be sure we can get an unpunctuated chunk sometimes.
    -clean up methods/delete old code
    -test on generated transcript
-write draft of function/interface of some sort tying together transcript and external promptmanager

5/9/21
------
X -add another option for punctuate (always, never, if_necessary)? Might be good to be able to be sure we can get an unpunctuated chunk sometimes.
    -clean up methods/delete old code
    -test on generated transcript
-write draft of function/interface of some sort tying together transcript and external promptmanager
-start basic gui
-watch sentdex video to see if that new nvidia package might be a good alternative to the annoyingly hard to install pocketsphinx

5/10/21
-------
X -add option/setting to avoid chopping off start/end of time_segment when fragment is sufficiently long? (Sometimes transcription fails or youtuber has a long run on sentence. Rather than throwing away 50 words (for ex), we might just want to keep it - we just don't want to feed a short meaningless fragment to gpt3).
X -add method to get punctuated indices of transcript
    X -method to get unpunct indices
    X -method to get punct rows
    X -move na_index_rows from staticmethod to function

5/11/21
-------
X -try out various transcript options
    X -passing in different kwargs (e.g. engine_i)
    X -try mock mode
        X -add option for different mock modes in query_gpt3
        X -write mock func for punctuate task
        X -method to reset punct df (useful after mocked calls. Considered making mock calls not update df_punct but that prevents us from testing side effects.)
    X -fix bug where punctuated_rows called unpunct df
_ -clean up nb
    UPDATE: skip, these are all just dev notebooks at this point, not anything polished to display somewhere.
X -port to lib
    ~ -document

5/12/21
-------
X -select first task to try post-punctuation (maybe tldr?)
    UPDATE: didn't work that well. Even the punctuation task alone is struggling. Thinking it may not have been a great idea to work on transcripts - prompt engineering guide specifically mentioned any typos or grammatical errors make it hard to get good responses.
X -write draft of function to tie together Transcript.time_range() and PromptManager.query().
    UPDATE: started session class to handle multiple videos. Thinking we might want to look for backup tasks (identifying similar videos?) in case running standard queries (e.g. tldr, eli5) on the transcripts doesn't work so well.
-choose: desktop gui, mobile, or web? (web makes most sense but part of the point of this project was supposed to be learning one of the others. Maybe start with desktop for learning purposes and if it ends up being particularly cool, I could always rebuild a simple streamlit app.)
    UPDATE: choosing desktop gui. Just want to learn here, even if it's not as practical for sharing as a web app. And initial results are not promising so that probably won't be a concern anyway.
    -maybe start building? Might be good to set up a UI early for easy experimentation.

5/13/21
-------
-consider if there's anything else worth adding to Session
    -port
-code to find videos w/ transcripts? Maybe just try to avoid punctuation task for now.
~ -start building desktop gui
-easier options: document Session
    -document Transcript methods

5/14/21
-------
X -work through rest of dearpygui video
    ~ -look through available widgets and see if it sparks any interesting ideas
X -add microphone recording option to app
X -add skeleton button to punctuate text

5/15/21
-------
X -figure out how to change labels (don't want element id's to be shown as text to user)
    UPDATE: fixed for buttons. Input_text label suppression seems a little hacky but I think it's how the lib intends us to do it.
~ -add menu bar
    UPDATE: layout/alignment still a work in progress.
X -refactor transcribe callback_data
    UPDATE: now can pass in as many ids to show during/after as we want.
X -try adding tooltip
    UPDATE: a little wonky (must use context manager) but it seems to work.
-add option to gpt3 punctuate text?
-add buttons/dropdown menu/etc. for different tasks (keypoints, eli5, summarize, fancier language, etc.)
-look into possibility of longer mic sessions (don't immediately stop at first pause)
    -maybe can translate as we go in a different thread or something? Ideally would be able to make request to google api while user is still talking and process in chunks.

5/16/21
-------
X -add buttons/dropdown menu/etc. for different tasks (keypoints, eli5, summarize, fancier language, etc.)
-add option to gpt3 punctuate text?
-look into possibility of longer mic sessions (don't immediately stop at first pause)
    -maybe can translate as we go in a different thread or something? Ideally would be able to make request to google api while user is still talking and process in chunks.

5/17/21
-------
X -clean up layout and make more flexible
    X -methods to calculate heights/widths
    X -callback to adjust these when window is resized
    -rename windows (would prefer to find way to separate id and label, but I don't think that's available)
-add query button
-look into issue of all text being displayed on same line in text input
-aesthetic improvements
    -diff font?
    -diff font size?
-add prompt warning (if exists) after selecting a task
-add query button
    -add query callback to actually query gpt3 and update output text

5/18/21
-------
X -port Session
X -put prompt text last after other options.
_ -format so no long horizontal scrollbar.
    UPDATE: not supported by dearpygui. Could write a function to break text into fixed length pieces but that's a bit annoying since with text box width is also variable.
X -update options callback to update value when prompt changes.
-rename windows (would prefer to find way to separate id and label, but I don't think that's available)
X -add query button
    X -add query callback to actually query gpt3 and update output text

5/19/21
-------
X -generalize query_callback so it works on passed in data rather than hardcoded IDs
-see if we can add places to enter options like:
    -mock_fn
    -stop_terms (name might be wrong here)
X -make transcription finish trigger update of options (prompt needs to be reformatted)
-add warning in case you try to query with empty text for a prompt that needs input
X -investigate bug where options are no longer updated with each task selection.

5/20/21
-------
X -see if we can add places to enter options like:
    _ -mock_fn
    UPDATE: I think this is only necessary for transcript. Fine to leave out for now.
    X -stop_terms (name might be wrong here)
-add warning in case you try to query with empty text for a prompt that needs input
-consider how we might support other (non-gpt3) tasks
    -what tasks might those be? (translation package, huggingface pretrained models (see py_project.txt notes)
    -how to show in gui? Probably ideal to have all tasks together but maybe it would be okay to separate them from gpt3 since options might be different.
    -could update manager class to support this? Would be a bit messy.
X -prevent user moving/resizing windows?
X -redo layout so options gets full height
-see if there's a way so manually typing input in record box (and then moving focus or something) can trigger options update too. This would also be useful if the user manually cleans up the punctuation after recording.
-add more voice interface features:
    -read gpt3 output
    -allow voice commands? E.g. say "Task: Summarize. Input: ..."

5/21/21
-------
X -add new font
    X -adjust font size
-see if there's a way so manually typing input in record box (and then moving focus or something) can trigger options update too. This would also be useful if the user manually cleans up the punctuation after recording.
X -add output audio (see old translation file)
X -fix bug where empty stop param is list when openai expects None
-add warning from a prompt's config file in options window
-add warning in case you try to query with empty text for a prompt that needs input
-consider image task tie-ins
    -consider ways to interact with internet (i.e. scrape some data based on gpt3 response?)

5/22/21
-------
X -get voice response working (put in thread?)
    UPDATE: tried threads but it sounds like there may be a threading issue on mac specifically. Found workaround using builtin mac os functionality.
X -adjust speaking pace
-maybe make it so we can choose a voice in the app?
-see if there's a way so manually typing input in record box (and then moving focus or something) can trigger options update too. This would also be useful if the user manually cleans up the punctuation after recording.
-add warning in case you try to query with empty text for a prompt that needs input

5/23/21
-------
X -see if we can add slight pauses on newlines when reading responses
X -consider adjusting pace a little
-consider removing auto-punct button since it's already available in prompts? Or remove from prompts menu?
-maybe make it so we can choose a voice in the app?
X -see if there's a way so manually typing input in record box (and then moving focus or something) can trigger options update too. This would also be useful if the user manually cleans up the punctuation after recording.
-add warning in case you try to query with empty text for a prompt that needs input

5/24/21
-------
X -work on prompt mapping term to ELI5 (or an average high schooler, really) passage
    UPDATE: tried mapping term to metaphor, but haven't been able to get this working well yet. Mapping idea (basically a simplified TLDR from semantic scholar) to abstract works reasonably well, however. Unfortunately, abstracts are usually not technically detailed enough for this to be particularly useful as a brainstorming technique.
    X -add this option to gui and experiment
-Think about how we might add a gui option for URL entry (youtube video, arxiv paper, etc.). Might need a menu that allows us to open a separate window since the expected workflow is a bit different (fetch text from URL rather than operate on input directly.)
-consider removing auto-punct button since it's already available in prompts? Or remove from prompts menu?
-add warning in case you try to query with empty text for a prompt that needs input

5/25/21
-------
~ -try gpt neo in colab
    UPDATE: crashed colab due to ram limit. Put on hold in favor of Huggingface api.
X -try huggingface api
~ -start building query_gpt_neo function

5/26/21
-------
X -update query_gpt_neo func
    UPDATE: Found and fixed some bugs. Most importantly, we were passing in parameters wrong which caused them to be ignored.
    _ -work on different length responses (seems to return slightly different format depending on max_tokens)
        UPDATE: This should only happen if we pass in a list of input strings, which I don't do with this interface.
    X -make interface more compatible w/ query_gpt_3?
        UPDATE: made it work as a mock_func.
    X -consider interface: should we hide this inside query_gpt3 (basically another mock mode option) or make interfaces identical so we can swap out functions interchangeably?
        X -work on implementing changes
~ -add gpt neo option to gui
    UPDATE: hardcoded for now when user selects mock mode. Eventually, might want to distinguish between a truly mocked call (faster) and neo.

5/27/21
-------
X -better error handling for query_neo (see error message in pycharm console) 
    UPDATE: also found and fixed bug w/ max_token len and identified possible cause of api errors (input is too long - tried to test this but then started getting errors about rate limits, which is odd because I'm nowhere near the documented limit).
~ -experiment with other information seeking task
    UPDATE: see misc

5/28/21
-------
X -create file to track tough google searches and the ultimately helpful piece of information
X -gui options for true mock call vs. neo call.
    UPDATE: collapsed mock checkbox and model options into one radio button list.
~ -make speech interruptable
    UPDATE: set default to off for dev purposes. Also started building interruptable decorator in ipython session.
X -show more graceful error message in gui if response fails for some reason (or maybe w/ my new error handling in query func this is unlikely enough that it basically never hapepns? Assess.)
X -tweak "how to" prompt params
    UPDATE: higher model, longer length, new stop_word
-make gui show when response is in progress (sometimes unsure if click didn't work or if response is just slow)
-check if we do any auto-updating of max_tokens in query manager when task is Punctuate (pretty sure we don't)
    -maybe adapt logic from UnpunctuatedTranscript to GUI: i.e. if we punctuate, we want the output to be the appropriate length.
-document gpt neo (maybe can largely copy from query_gpt3 or use add_docstring decorator.

5/29/21
-------
X -interruptable decorator
    ~ -test with @callback
        X -consider integrating callbacks natively? So use just 1 deco, not 2.
    X -port to htools
    ~ -integrate into GUI/speaker class to allow speech interrupt
 -make gui show when response is in progress (sometimes unsure if click didn't work or if response is just slow)
-check if we do any auto-updating of max_tokens in query manager when task is Punctuate (pretty sure we don't)
    -maybe adapt logic from UnpunctuatedTranscript to GUI: i.e. if we punctuate, we want the output to be the appropriate length.
-document gpt neo (maybe can largely copy from query_gpt3 or use add_docstring decorator.

5/30/21
-------
X -integrate into GUI/speaker class to allow speech interrupt
    X -troubleshoot threading issues: seems that each callback is run in a separate thread and it's tricky to propagate an exception from one thread to another
-make gui show when response is in progress (sometimes unsure if click didn't work or if response is just slow)
-check if we do any auto-updating of max_tokens in query manager when task is Punctuate (pretty sure we don't)
    -maybe adapt logic from UnpunctuatedTranscript to GUI: i.e. if we punctuate, we want the output to be the appropriate length.
-document gpt neo (maybe can largely copy from query_gpt3 or use add_docstring decorator.

5/31/21
-------
X -clean up code a bit after speaker interrupt ordeal
    X -document new funcs
X -make gui show when response is in progress (sometimes unsure if click didn't work or if response is just slow)
-check if we do any auto-updating of max_tokens in query manager when task is Punctuate (pretty sure we don't)
    -maybe adapt logic from UnpunctuatedTranscript to GUI: i.e. if we punctuate, we want the output to be the appropriate length.
X -document gpt neo (maybe can largely copy from query_gpt3 or use add_docstring decorator.
~ -start generic monitor class (see ipy session)

6/1/21
------
X -check if we do any auto-updating of max_tokens in query manager when task is Punctuate (pretty sure we don't)
    X -maybe adapt logic from UnpunctuatedTranscript to GUI: i.e. if we punctuate, we want the output to be the appropriate length.
X -consider removing auto-punct button since it's already available in prompts? Or remove from prompts menu?
    UPDATE: removed. Updated transcribe task and get_query_kwargs method to do some light autoformatting.
X -add vanilla gpt3 prompt (no instructions, no specific task, just give it some text and let it talk for a bit)
-add chatbot prompt
-add translation prompt?
-continue work on more generic Monitor class for htools

6/2/21
------
X -add tooltips to everything
X -fix bug where empty default task resulted in a colon
-increase max tokens (gpt3 allows up to 2048 between input+output, though neo allows only up to 250 token outputs)

6/3/21
------
-investigate possible bug where manually updating a setting sometimes resets other settings (seems to happen when you type in a number for max_tokens but not when you adjust it with arrow keys)
-investigate and fix possible bug where vanilla query seems to replace apostrophes with question marks
-prototype chatbot prompt
X -build steelman prompt
    X -add config and prompt files to data/prompts dir
    X -integrate into UI
X -fix bug in speaker where hyphens are still interpreted as CLI flags
~ -start looking into potential slack app
-prototype translation prompt
-continue work on more generic Monitor class for htools

6/4/21
------
X -consider ways to avoid unwanted pauses after periods in words like U.S.A.
X -prototype MMA fight predictor prompt
    X- add files to data/prompts dir
    X -integrate into UI
X -investigate and fix possible bug where vanilla query seems to replace apostrophes with question marks

6/5/21
------
~ -prototype "slack replier" prompt
    UPDATE: tried a bit but realized I blocked slack and it's harder without sample messages. Maybe try again tomorrow.
X -prototype translation prompt
    X -update config/prompt files
    X -add to UI
-record some mma examples (text? silent gif? video with audio?)
-investigate possible bug where manually updating a setting sometimes resets other settings (seems to happen when you type in a number for max_tokens but not when you adjust it with arrow keys)
-see if slack app is a possibility (do I have auth to do in GG workspace?)
X -explore possiblity of scraping pro/con website for t5-like dataset.
    UPDATE: No api, but it does look fairly scrapable. Only ~100 issues though, it seems.
-maybe manually add 1 more example to debate prompt to see if it improves response quality? Does mean queries would be more token-intensive though.

6/6/21
------
~ -prototype "slack replier" prompt
    UPDATE: still couldn't find that many great examples. Maybe keep an eye out for useful data that comes up organically in the future.
~ -prototype chat w/ various public figures
    UPDATE: Tried with a few and results were decent. Worth pursuing further.
~ -write basic func to get wiki summary about a person
    UPDATE: still some improvements to be made.
X -prototype writing analyzer prompt, add data files, add to UI
    UPDATE: examples probably could be better - this seems to fixate more on author emotion/feelings than writing style
-prototype cover letter writer from job posting
-record some mma examples (text? silent gif? video with audio?)
-investigate possible bug where manually updating a setting sometimes resets other settings (seems to happen when you type in a number for max_tokens but not when you adjust it with arrow keys)
-see if slack app is a possibility (do I have auth to do in GG workspace?)
-maybe manually add 1 more example to debate prompt to see if it improves response quality? Does mean queries would be more token-intensive though.

6/7/21
------
X -make wiki_summary func more robust
    X -handle situation where page not found
    _ -handle situation where wiki page is disambiguation page (idea for both: ask user for optional *tags args. On disambiguation page, look for semantically similar person. E.g. if I search wiki_summary('john smith', 'wrestling') that should let me find an article named something like 'John_Smith_(Collegiate_Wrestler)'. For case where no disambiguation page, we could try googling 'john smith wrestling wikipedia' and see if something comes up)
        UPDATE: think this isn't necessary with current solution - I'll never intentionally search for a disambiguation page and I skip them in search results. Worked on the google search approach a bit but decided wikipedia api was more reliable and more stable for the future.
    X -add img downloading functionality

6/8/21
------
X -improve text cleanup in wiki summary (name pronunciations don't always parse to the same char; need something more generic to find junk after parenthesis)
    X -select best image
    X -validate page is a reasonable match
    X -port 
    -document
    -see how to load image in gui
-figure out how to integrate conversation prompt into gui
    -consider how we might allow for an ongoing conversation - maybe there should be a single text input field instead of 2?
-investigate possible bug where manually updating a setting sometimes resets other settings (seems to happen when you type in a number for max_tokens but not when you adjust it with arrow keys)

6/9/21
------
~ -document wiki funcs
X -figure out how to make conversation prompt work w/ current setup (may need to add hook func so user to call when loading)
    UPDATE: added option to provide custom formatter. Not the most elegant but seems workable so far.
    ~ -figure out how to integrate conversation prompt into gui
        UPDATE: Sort of works, but I think we might be calling wiki_data every time the user types a character, horribly gumming up the app. Need to work on this.
    -consider how we might allow for an ongoing conversation - maybe there should be a single text input field instead of 2?
-see how to load image in gui
-investigate possible bug where manually updating a setting sometimes resets other settings (seems to happen when you type in a number for max_tokens but not when you adjust it with arrow keys)
X -flex task: monitor_time decorator (see ipython session)

6/10/21
-------
-update gui so conversation wiki_summary isn't run repeatedly (remove reliance on promptmanager.kwargs? Or somehow cache results? Or logic to wait several seconds after user stops typing?)
-consider how we might allow for an ongoing conversation - maybe there should be a single text input field instead of 2?
-investigate possible bug where manually updating a setting sometimes resets other settings (seems to happen when you type in a number for max_tokens but not when you adjust it with arrow keys)
X -see how to load image in gui
X -add frequency_penalty to query_gpt3
X -add freq penalty to conversation default config
X -make conversation formatter sentence tokenize more reliably (prev didn't support names like Dr. Seuss or J.K. Rowling).
X -func to get most recently edited path in dir (workaround for lack of access to downloaded img url)
X -port monitor_time deco
    X -document

6/11/21
-------
X -update img size
X -fix bug in name (period was attached to end)
~ -consider how we might allow for an ongoing conversation - maybe there should be a single text input field instead of 2? Do we need multiple windows/tabs or change window depending on settings?
    UPDATE: Decision: I think we need a separate conversation mode. Everything now is geared towards 1 format (1 input -> 1 output). I could imagine a whole class of tasks that are more conversational, and trying to wedge them into the current setup could get ugly(er). Can plan more tomorrow, but might need a new ConversationManager class (the convo equivalent of PromptManager) and a new gui window/tab.
-update gui so conversation wiki_summary isn't run repeatedly (remove reliance on promptmanager.kwargs? Or somehow cache results? Or logic to wait several seconds after user stops typing?)
    UPDATE: on hold. Wait til we figure out a Conversation setup since this might naturally remove the problem.
X -investigate possible bug where manually updating a setting sometimes resets other settings (seems to happen when you type in a number for max_tokens but not when you adjust it with arrow keys)
    UPDATE: my text_update callback seems to have triggered it. Added an option to only update the text.

6/12/21
-------
X -adjust img_size calcs to account for padding
    X -adjust so it updates on window resize
X -adjust text input size calculations to avoid horizontal scrolling
X -change text input label position from right to top to avoid horizontal scrolling
-research options for adding new tab/window
    -start working on convo tab
    -add URL tab?
-start conversationmanager class
-easier:
    -document session
    -document transcript methods
~ -rewatch yannic siren video
    UPDATE: watched first half and took notes. Trying method of pausing ever 20% and writing down what I remember from the last chunk to try to force recall.

6/13/21
-------
X -write wrapper func to make text input w/ label above instead of to the right
    UPDATE: turned out to be shockingly difficult. Ended up accomplishing with context manager, though still not thrilled with implementation.

6/14/21
-------
X -update other cases that could benefit from new label_above ctx_manager
X -build class to go back and forth between raw text and text with newlines inserted to be under some max_len
    ~ -incorporate into gui
        UPDATE: added button to do this (gets hairy if we try to handle this automatically, and text_edit_callback doesn't seem to let us change outputs in the input item we're monitoring). Still need to update response textbox to use the transform and update the prompt textbox to undo the transform.

6/15/21
-------
X -update prompt textbox (under query kwargs) to undo chunking transform
    X -update response textbox to do the chunking transform
    X -update query_callback to get unchunked text rather than what's shown in transcribed window
-add __contains__ method to chunker

6/16/21
-------
X -check for bugs in yesterday's formatting updates (does transcribed text insert newlines as expected? Does resolved prompt (under kwargs) update as expected? Does the right prompt get sent in query? Does response get displayed correctly?)
X -investigate bug (?) where adding text repeatedly to chunker gradually makes lines shorter and shorter.
X -updates to textchunker: method to check if previously added, split on \n\r as well as spaces
X -port lazy decorator to htools

6/17/21
-------
X -look into bug w/ "?" chars in prompt textbox for analyze_writing
    UPDATE: book used weird dash, replaced it with a more common one.
    ~ -look into possible bug w/ extra newlines still being added to prompt text box sometimes?
        UPDATE: found, fixed, and maybe introduced many bugs here. I THINK this is (hopefully) almost working now, with the exception that double newlines now seem to be single. Investigate tomorrow. Brain fried.

6/18/21
-------
X -investigate double newline bug in textchunker (see yesterday's notes)
    UPDATE: wrote better hsplit alternative to fix this. Maybe port at some point to replace regex method which is apparently a bit shaky logic?
X -consider how to handle autoformat button: auto press it (in which case we need some magic (threaded?) to help in typing case because dearpygui callback can't edit the input box it's monitoring), add instructions to press it and just don't support the other case, or update query_func and other places to be able to get the raw version if it hasn't been added to chunker
    UPDATE: added tooltip encouraging user to do it and explaining repercussions. It's really mostly for human convenience as most text does not have many newlines (except in specific cases where it's important for formatting)). Also made this automatic for voice transcription mode.
X -port new sticky split to htools as hsplit subcase (with slight tweak to allow leading sep(s))
-rm newlines from analyze_writing prompts
    UPDATE: Done, but it looks like we'd really need more examples for this to work well. Right now, we only have 1 for casual/funny, 1 for analytical/scientific, etc. So gpt3 often selects from the exact bullet point descriptions rather than generating novel ones.

6/19/21
-------
X -add logging functionality to save most recent prompt/kwargs used (either in promptmanager or in file). This way after making any changes (such as adding the newline insertion in various places), we can check to make sure that what we ultimately send to gpt3 is what's intended.
X -error handling: press query when no text in transcribed
X -error handling: query_callback always gets text from chunker, what if user hand-typed and didn't auto-format?
    UPDATE: make text_edit_callback update chunker. We don't do anything with the chunked lines but it updates the text in chunker.raw.
X -error handling: saved kwargs suggest query is (sometimes/always?) made with chunked input, not raw input
    UPDATE: kwargs generated by app.get_query_kwargs include a 'prompt' param. This was pre-formatted and overrode the unformatted prompt param in promptmanager. Lesson: validate kwargs if there's a risk of passing unintended names in?
X -fleshed out, ported, and documented sleepy_range to htools

6/20/21
-------
-research options for adding new tab/window
    -start working on convo tab
    -add URL tab?
X -start conversationmanager class
    UPDATE: got pretty far but still need to do some testing, flesh out query/kwargs/prompt behavior, etc.
X -fleshed out, documented, and ported venumerate to htools

6/21/21
-------
X -test adding new personas to ConversationManager
X -improve func that cleans wiki summary
    UPDATE: removes more junk now.
X -flesh out OwnerAccess metaclass
    UPDATE: added option for subset of attrs or all

6/22/21
-------
-add more methods to conversationmanager:
    X -add kwargs method/attr?
    _ -add prompt method/attr?
        UPDATE: promptmanager doesn't actually use this anywhere, just a convenience method for the user which isn't that helpful here bc there's only 1 task.
    ~ -write query func (figure out how to update running conv - maybe another attr needed)
        UPDATE: got 1 query working. Still need to adjust it to handle ongoing conversations (distinguish between first call and subsequent calls).
    ~ -can we remove name2summary or name2base? Pretty redundant.
        UPDATE: prob eventually rm name2summary but keep for a couple more days while the class develops, just in case.
    -clean up old tmp dir of downloaded photos?
-research options for adding new tab/window
    -start working on convo tab
    -add URL tab?
-easier:
    -document session
    -document transcript methods
    -document textchunking class
    -port htools as_df
    ~ -flesh out owneraccess more
        UPDATE: started rebuilding metaclass as cls decorator and so far it seems to work.

6/23/21
-------
X -continue fleshing out conversationmanager
    X -update query_func to work w/ ongoing convos. Need to distinguish between first call and subsequent calls.
    X -rm name2summary
    X -add conversation saving option
    X -add contextmanager option
    X -test out functionality
    -clean up old tmp dir of downloaded photos?
-flesh out owneraccess decorator

6/24/21
-------
~ -add method for prettier formatting of ongoing conversation (auto bold all user-provided text?)
    UPDATE: wrote func to do this but haven't added to class yet.
X -underline func
    UPDATE: tried italicize first but that didn't work.

6/25/21
-------
X -add format_conversation function to class? Could also be staticmethod or standalone function.
X -port conversationmanager to lib
X -document conversationmanager methods
X -add new window/tab to dearpygui
-maybe update converse contextmanager to have input baked in
-flesh out owneraccess decorator

6/26/21
-------
~ -continue fleshing out conversationmanager window
    UPDATE: built out basics of new conv window AND new options window (don't worry about code duplication for now, just get something working)
    X -figure out better way to select which window is shown (menu bar at top? Currently just have a checkbox in main window for testing purposes but that's definitely not a long term solution)
        UPDATE: Added menu bar that lets us select different modes.
    ~ -do we need a different kwargs window or can we figure out a way to adjust the existing one?
        ~ -in fact, might we be able to keep a single interface and obscure any differences between conversation mode and single response mode from user? Not sure yet if that's even desirable.
        UPDATE: for now, just focus on building new functionality that works. Maybe later we can refactor to be more elegant, but let's get something working first.
X -delete old conv prompt and rename new one
X -add persona selection menu
X -dynamically update image based on selected persona
-flesh out owneraccess decorator

6/27/21
-------
X -adjust window size computations to better account for menu
X -add current_summary prop in conv_manager
~ -allow adding new persona
    UPDATE: added button and textbox, no functionality yet though.
X -make image smaller
X -add bio next to image
    X -update when persona changes
X -move query and read widgets to left window
    UPDATE: done, but considering moving back now that I made image much smaller. Consistent UI would be nice.
-adjust name formatting to better pretty-format initials (e.g. TJ, JK, AJ)
-start trying to get conversation query button to work
~ -look through new windows for old names/callback_data params that need to be updated
    UPDATE: cleaned up a few old unnecessary lines
-flesh out owneraccess decorator

6/28/21
-------
X -debug: why is image width not staying constant for different personas?
    UPDATE: refactored into helper function to take care of this.
    X -tune text max_chars for bio (fix after image issue since it's affected by that)
        UPDATE: Also recomputed chunked line length on resize.
X -add_persona button
    UPDATE: also made it update the listbox.
-adjust name formatting to better handle initials (TJ, JK)
-start trying to get conversation query button to work
-flesh out owneraccess decorator

6/29/21
-------
X -update convmanager to handle pages w/out "born" in summary (realized that's more common than I thought - maybe all dead and fictional people?)
X -add progress message/widget when downloading new persona
X -maybe move query/read response widgets back to options panel now that image isn't as tall?
    X -maybe move summary under photo and make it twice as wide? Gets pretty long sometimes with how narrow it is.
X -handle case where there are no existing personas
    UPDATE: Made it easier to check by implementing __len__, then if 0 make gui download a default persona (Obama).
-adjust name formatting to better handle initials (TJ, JK); maybe remember user capitalization (ex: Neil deGrasse Tyson middle name has unusual casing)
-start trying to get conversation query button to work
-flesh out owneraccess decorator

6/30/21
-------
X -look more into how (if?) we can center align image (could make it full width but then height would be giant in many cases). add_dummy() may be the closest we can get.
    UPDATE: yeah, add_dummy works.
X -deal w/ listbox height issues (seems frozen at whatever the initial height is, which is undesirable when we add many. And if we start w/ many there's no scrolling which is annoying.)
    X -see if we can change height
    _ -try radio buttons instead?
    _ -try menu instead?
    UPDATE: fixed by setting num_items in listbox.
X -test on fictional character (should be possible now that I've removed references to "born" in summary)
    UPDATE: so far so good.
X -allow ConvManager to pass in names at the start, like specifying tasks in PromptManager.
~ -adjust name formatting to better handle initials (TJ, JK); maybe remember user capitalization (ex: Neil deGrasse Tyson middle name has unusual casing). 
    UPDATE: inverse transform won't reinsert them but we can handle user-input names w/ periods. However, noticed downloading dillashaw only retrieved a summary, no photo. This happens too if we use wiki_data() directly so I don't think it's related but should investigate more.
    X -Also noticed Dr Seuss worked when I first downloaded it but on subsequent gui runs it was loaded simply as "Dr". Check if that works now.
        UPDATE: yes.
X -add payment info to gpt3 api
    UPDATE: 
-start trying to get conversation query button to work
-flesh out owneraccess decorator

7/1/21
------
X -investigate dillashaw photo download issue (does wiki page have no images? Were none of the names a close enough match?)
    UPDATE: convmanager logic to remove periods was mistakenly placed in the inverse transform case.
    -if applicable, maybe adjust logic to choose a fallback if there's not a close enough match?
    -or diff func to search for that person's face outside of wiki? Might be more reliable anyway.
X -set usage limits on gpt3 api account
X -add transcription to conv mode
~ -start trying to get conversation query button to work
    X -build dummy query w/ chunking
-flesh out owneraccess decorator

7/2/21
------
X -debug failed image download
    UPDATE: wikimedia requires user-agent now to make frequent requests.
X -create save_summary() method (just refactored from end_conversation)
~ -add save_conversation button
    UPDATE: button is there and it saves, but user can't choose location (dearpygui removed its save file dialogue widget?). Also still reflects lack of update if user spoke last. Could either get text from text input directly or make text edits and transcription always trigger an update of prompt? Probably the former makes more sense.

7/3/21
------
X -allow kwarg customization? Or temp change config? Want to be able to test w/ engine_i=0 rather than 3 to save $.
    UPDATE: for now simplest to hardcode in engine_i=0. For real thing, we'll probably want to stick w/ 3 most of the time. Can experiment in openai UI w/ 1 or 2 but 0 is notably poor.
X -get real query working in place of dummy query (conv mode)
    X -how to extract last user text? Cache in transcription callback or extract from full summary?
        UPDATE: instead, allowed query method to pass in a fully-resolved prompt instead of single new line. Seemed less hacky that re-extracting the last user comment from a full prompt.
    -get voice mode working for post-query
    -consider: are we able to easily refactor conv_query to fit into query callback (i.e. remove duplication?)
~ -stop conv text from disappearing during recording
    UPDATE: added functionality but haven't tried it yet.
~ -save button improvements: currently doesn't get last line if user spoke last
    UPDATE: added functionality but haven't tried it yet.
    -show message saying where file saved to
~ -for conv mode, only clear text when conversation ends, not every time we transcribe
    UPDATE: done, but implemented as a side effect of saving. Maybe should support case where we want to end a conversation but not save? Does this happen already if we switch to another persona? Investigate further.
X -text edit callback for conv mode: atm, if we edit text the chunker still only has the pre-edited version
-other options:
    -record some mma prediction examples (maybe next n main events? Predicting on lesser known fights may give more generic or less realistic answers since the model's probably seen less analysis of, for example, the guy debuting against Sean O'Malley than it has of McGregor or other popular fighters)
    -start planning out blog about page and intro
    X -start writing fight predictor blog post

7/4/21
------
X -test new functionality: conv text shouldn't disappearing during recording
    UPDATE: works
~ -test that save button includes last line when it's by the user (built but haven't tried)
    UPDATE: yes, but we should delete the start of a line below containing "{current_persona}:".
    ~ -update saving logic to end a conversation 
        UPDATE: Otherwise, saving temporarily deletes the text but maintains a memory of it internally, so if we speak again without selecting a new persona, it will continue the ghost of our past conversation.) But now that it's done I'm thinking that might be desirable (save as we go along, like saving a doc we're working on in case something crashes). Consider.
    -show message saying where file saved to
X -further cleanup to wiki_summary
    X -replace en dash
    X -see if we can remove more junk that appears in Einstein summary?
X -fix bug in _load_personas where all personas are loaded even if we specified a list of names
    UPDATE: found this bug early in today's session. Because I remove a name every time we process it, if the excluded name is last, the names cache is empty, mimicking a scenario where we didn't specify any names. Created boolean var upfront to address this.
X -add default image when wiki fails to download one
X -update add_persona to check if we have files locally first?
    X -update gui to auto switch to a persona after we add it
        UPDATE: done, though a little hacky. Can't find a way to actually change the selected item in dearpygui so we override it with the callback's data parameter, meaning the name technically isn't selected. Thought this might cause problemes w/ resizing but it doesn't seem to. Updated logic so re-selecting the current name doesn't erase a conversation.
X -allow deletion of item from CHUNKER
X -check if switching personas resets conversation text.
    UPDATE: No. Updated select_persona_callback.
-get voice mode working for post-query
    -check if we can refactor to combine query and conv_query callbacks
-other options:
    -record some mma prediction examples (maybe next n main events? Predicting on lesser known fights may give more generic or less realistic answers since the model's probably seen less analysis of, for example, the guy debuting against Sean O'Malley than it has of McGregor or other popular fighters)
    -start planning out blog about page and intro
    -work more on gpt3 blog post
    X -start bookdown repo

7/5/21
------
X -update save button to trim off partial last line when user talks last (or maybe adjust this in text updating so it only shows up after a query? Would feel more like a natural conversation maybe.)
    UPDATE: changed this in convmanager.format_prompt() instead so we only see the person's name pop up after they respond.
X -update convmanager/chunker so that gpt3 curly quotes are replaced with more standard ones for dearpygui's sake
    UPDATE: chunker now (by default) handles both single and double curly quotes and en dashes. This seems to work well except for if I manually type the curly quotes in (the prompt ends up correct but the input text isn't changed).
X -show message saying where file saved to
    UPDATE: Found that I can mimic most of file dialogue functionality with a popup() item. Basic functionality works but still fleshing out some error handling.
-think more about desired saving functionality. Differentiate between saving+ending vs. saving+continuing a conversation vs. ending without saving? Which of these should we support?
-get voice mode working for post-query
    -check if we can refactor to combine query and conv_query callbacks
-other options:
    X -record some mma prediction examples (maybe next n main events? Predicting on lesser known fights may give more generic or less realistic answers since the model's probably seen less analysis of, for example, the guy debuting against Sean O'Malley than it has of McGregor or other popular fighters)
    -start planning out blog about page and intro
    -work more on gpt3 blog post

7/6/21
------
X -widen save popup (path is overflowing atm)
    X -reset default values for dir and file name after first save? Right now they persist, not sure why.
    X -reset error message
    X -Allow user to override both errors? They're really more like warnings.
        UPDATE: achieved via hidden checkbox that becomes visible on error.
X -think more about desired saving functionality. Differentiate between saving+ending vs. saving+continuing a conversation vs. ending without saving? Which of these should we support?
    UPDATE: added option to not end conversation.

7/7/21
------
X -get voice mode working for post-query
    -check if we can refactor to combine query and conv_query callbacks
X -add em dash replacement to chunker
X -troubleshoot issue where default save filename always seems to be obama (assuming whoever the default persona is when we load the app?)
    UPDATE: added callback for saveas button.
-add options for more voices (at least 1 male 1 female)
    X -try out diff mac voices
        UPDATE: Daniel seems like a reasonable choice for men.
    X -for conv mode, auto-select voice based on prediction of male or female name? Or maybe we can extract pronoun from wiki summary - e.g. space split and lowercase summary and check if he or she is more frequent, and return an extra value from wiki_data(). 
        UPDATE: wrote, ported, and documented func to do this. Use wiki summary pronoun method and worked well on a sample of ~10 names I chose, roughly split between men and women + 1 fictional character.
        -update wiki_data func
        -update convmanager's usage of it

7/8/21
------
X -update wiki_data w/ inferred gender
    X -update convmanage to account for this new returned value
    X -update conv gui to auto select based on inferred gender
        UPDATE: Also found bug where attempting to query w/ no conv throws unhandled error. Also still seems like speaker is chunking sentences on newlines and maybe commas, which I don't think I asked for.
    X -manually update voice after first starting app because we can't call persona_select_callback until right col has been rendered.
X -update speaker to allow setting voice without breaking cmd_prefix

7/9/21
------
X -look into possible tokenization bug (pausing on commas and newlines in addition to sentences, I think. Need to figure out intended behavior that works with various prompt types - How To, for example, often doesn't have periods. Could use diff rule for conv vs default mode, I suppose.)
    UPDATE: seems to just split on sentences as intended. Added behavior where it splits on double newlines as well. Conv model seems to often return short responses anyway.
-possible bug: how to prompt no longer adds colon at end?
X -end conversation w/out saving option
-start planning how to deal with long convs (e.g. summarize past conv, delete it and just use the past couple responses, etc. Realizing my current implementation may present a problem because I pass in the full conv blob to query so we'd have to do some less than ideal surgery to extract the new bits.)
X -try recording a couple conversations with engine_i=3.
X -thought there was a message shown during conv query but I'm not seeing it show up?
    UPDATE: there was but I never added logic to display in in conv_query_callback. Did that now.
-other options:
    -work more on gpt3 blog post
    -start planning out blog about page and intro
    -play around w/ mma prompt variations ("this brilliant analyst", "this casual mma fan", etc.)
    -record conversation w/ joe rogan asking for prediction?

7/10/21
-------
-finish blog post?
X -minor bug: os say cmd read "No" as "number"
    UPDATE: quirk in Daniel voice's functionality. Used some careful string replacements in speech module to eliminate this.
X -error handling when we fail to download a wiki persona
-start planning how to deal with long convs (e.g. summarize past conv, delete it and just use the past couple responses, etc. Realizing my current implementation may present a problem because I pass in the full conv blob to query so we'd have to do some less than ideal surgery to extract the new bits.)
-other options:
    -start planning out blog about page and intro

7/11/21
-------
-start planning how to deal with long convs (e.g. summarize past conv, delete it and just use the past couple responses, etc. Realizing my current implementation may present a problem because I pass in the full conv blob to query so we'd have to do some less than ideal surgery to extract the new bits.)
    UPDATE: wrote htools decorators to get all functions defined in module and optionally decorate them to help debug this (wanted to wrap all with @debug to more easily map out a model of how all the conv components interact to better plan how we might refactor convmanager to deal with long convs. But dearpygui does weird things (I think bc it's largely written in another language and somehow ported to python magically? Unsure.) So we'll probably need to do this manually.
-consider: are we able to easily refactor conv_query to fit into query callback (i.e. remove duplication?)
-text edit callback for conv mode: atm, if we edit text the chunker still only has the pre-edited version
X -add save as button to default mode

7/12/21
-------
X -map out how convmanager prompt is updated and its interactions with CHUNKER, app, etc. (e.g. does it get input from chunker or text box? Does it use that to replace its running prompt or does it just lop off the last new line? Trying to figure out how we might deal with longer convs and I have a vague intuition that the current setup doesn't allow for this easily, but I need a more concrete understanding of why that is.)
    X -start planning potential approaches (summarize past conv, include only last 1 message, only last n messages, etc.)
        UPDATE: summarizing past feels a bit excessive for now. Let's try to get a "last n messages" approach working. If that turns out to not be sufficient we can revisit that decision.
~ -start rebuilding convmanager
    UPDATE: maintain list of user turns and gpt3 turns (as in your turn in a conversation). Then for each query, grab the last n and reconstruct a prompt. Don't allow user to pass in full prompt, just a single text input. Allow them to do this with query_later() as well (lazy isn't appropriate after all because it will return an object that needs to be called, whereas we want it to use the cached value the next time we call the method. I'm sure we could build a decorator like that without too much trouble but this solution is probably simpler).

7/13/21
-------
X -try ProtoConvManager to see what is/isn't working about new system
    UPDATE: lots of things to fix as expected, but tentatively seems to be working okay now.
X -start refactoring format_prompt to allow use in generating full conversation (avoid so much duplicated storage)
    UPDATE: _format_prompt draft is written but untested. Same w/ updated format_prompt and format_conversation. Eventually remove full_conv attr and update refs.
X -draft full_conversation property to replace full_conv

7/14/21
-------
X -test format_prompt
    X -test full_conversation property
    X -once troubleshooting done, remove refs to full_conv and update (not just text replace: can no longer delete property unless I make some tweaks but I don't think that's necessary)
X -port new convmanager
    X -update gui refs to running_prompt

7/15/21
-------
X -update gui to work w/ new turn-limited conv process
    X -check notes: where does conv_query get text from? what conv-related keys does chunker maintain? Where do we need to cache new turn (presumably both in transcribe callback and text edit callback?)
    X -check that saving w/ no conv still produces warning. Might need to adjust full_conversation property to return empty str when no turns.
        UPDATE: gets text from chunker anyway so recent changes didn't affect this.
    X -implement changes
X -show error msg when user tries to edit an old turn
    UPDATE: would have liked popup the user has to close but that seems to need to be tied to a button (i.e. appears on click, not from callback. Newer dearpygui versions supposedly have this but I tried updating and it looks like the api changed significantly and would break all my code so I should stick with 0.6.415.)
X -text edit callback for conv mode: atm, if we edit text the chunker still only has the pre-edited version [should do mapping task (1st bullet point) first]
X -start looking into pyinstaller
    UPDATE: tried running even though not quite done yet just to see what the process looks like and how difficult it might be. Quite slow to run, not sure if this will work yet.

7/16/21
-------
X -follow up on pyinstaller - did it work?
    UPDATE: sort of - pyinstaller completed but we get an error when trying to run the app. See misc.txt.
X -add popup modal on click save btn (default mode)
    X -write save callback
    X -write cancel callback
    X -are we able to refactor to combine w/ existing funcs? Or need to keep separate?
        UPDATE: ended up editing existing funcs for these rather than writing new ones.
-consider: are we able to easily refactor conv_query to fit into query callback (i.e. remove duplication?)
-start considering how we might refactor giant mess of giant gui file

7/17/21
-------
X -update default save mode to combine output w/ PROMPT, not just input in top text box. Maybe can use logged json file or could cache this in CHUNKER.
    UPDATE: realized I don't want prompt either (that often includes lots of examples), more like a specific subset of it. Performed some surgery to reconstruct the relevant portion. Logic is not bulletproof but it seems to work okay for most of my prompts.
-investigate possible interruption checkbox bug (seems to auto-uncheck at end of sentence and keep going. So far only tried in default mode)
    UPDATE: Didn't get to this today but did notice another issue: if we click save as while the speaker is speaking, file names aren't populated and we can't save or cancel until speaking is done. I think this is because callbacks are blocking and the popup's appearance is somehow triggered another way by dearpygui.
X -clean up callback_data for default mode saving. Have some unused kwargs and some missing ones (currently hardcoded chunker keys, for example)
    UPDATE: decided hardcoded keys are okay but deleted some other keys.
X -rm conversation (prev referenced old name conv_proto), shortest, and short_dates from default mode. Testing w/ engine_i =0 is almost free and we're mostly done with testing this mode anyway.
-consider: are we able to easily refactor conv_query to fit into query callback (i.e. remove duplication?)
-start considering how we might refactor giant mess of giant gui file
-document callbacks and other stuff in main.py (large backlog here)

7/18/21
-------
X -investigate possible interruption checkbox bug (seems to auto-uncheck at end of sentence and keep going. So far only tried in default mode)
    UPDATE: nested for loop issue. Changed checkbox monitor to append an exception object to error list and have the speech function raise that inside a try/except block rather than deal with for/else weirdness.
X -add tooltips warning user about end_conversation button and about using save_as during speaking.
X -revisit project requirements to see if anything is missing
	UPDATE: reviewed and ended up filling out the postmortem (except for end date) - slightly cheating but I found I'm at a point where I can address most of the issues laid out there.

7/19/21
-------
X -start considering how we might refactor giant mess of giant gui file
	UPDATE: managed to separate out callbacks and utils to separate files but it took some minor hackery w/ setting globals in imported modules. 

7/20/21
-------
X -document bin/utils functions
X -document bin/callbacks functions (large backlog here)
X -document bin/app functions/methods
X -revisit project backlog to see if there's anything I should get to
	UPDATE: nothing too major I really need to do.

7/21/21
-------
X -check punctuate task kwargs: do we update max_len dynamically? I don't think so but it would be nice.
	UPDATE: max_tokens updates only when we click Punctuate task. Updated it so it updates when typing while still ensuring other kwargs don't update.
~ -see how hard it would be to add a warning in app based on task's config file message.
	UPDATE: this would require changing load_prompt, prompt_manager, and gui (at a minimum). Don't think it's worth it given the low payoff here. Warnings aren't anything too crucial.
X -make requirements for jabberwocky and upload to pypi
	UPDATE: fixed cli bug where sorting can be messed up after sklearn->scikit-learn type conversions, added a couple more of those cases, generated requirements, and uploaded v1.
X -add pyinstaller import hook from stackoverflow to try to resolve numpy error
~ -give pyinstaller another shot (I think I may have saved some notes on the error msg I got last time)
	UPDATE: currently building. We'll see - I'm guessing this will still take a lot of head bashing before success but I'll check back in later.
X -clean up dailies a bit (many copies of backlog and easy task had appeared at bottom, some from accidental vim duplication and maybe some from me forgetting I had an existing section for that)
X -fix bug where dummy prompts were still showing up in gui
	UPDATE: realized my skip_tasks logic in PromptManager wasn't right. Fixed this and updated gui accordingly.
-consider format and content of blog post (lessons learned about gpt3? about modes of programming (declarative vs. OOP)?)
-consider: are we able to easily refactor conv_query to fit into query callback (i.e. remove duplication?)

7/22/21
-------
-check on pyinstaller results
	UPDATE: Same error as before. 
	-troubleshoot if necessary (lol)
	UPDATE: Tried adding pyinstaller hook from stackoverflow and upgrading numpy. Wordninja data file failed to be included so added Tree. Tried to add my data dir but pyinstaller won't accept parent data paths so moved spec file to project root.
~ -consider format and content of blog post (lessons learned about gpt3? about modes of programming (declarative vs. OOP)?)
	UPDATE: misc.txt
-consider: are we able to easily refactor conv_query to fit into query callback (i.e. remove duplication?)

7/23/21
-------
-more pyinstaller troubleshooting
	-try auto-py-to-exe if stuck w/ pyinstaller? Same underlying lib but maybe gui makes some options more intuitive
-clean up readme

-------------------------------------------------------------------------------
backlog (optional)

- htools things:
	-flesh out owneraccess decorator
	-port htools as_df
-watch sentdex video to see if that new nvidia package might be a good alternative to the annoyingly hard to install pocketsphinx
-maybe update converse contextmanager to have input baked in
-see if slack app is a possibility (do I have auth to do in GG workspace?)
-prototype cover letter writer from job posting
-continue work on more generic Monitor class for htools
-consider image task tie-ins
    -consider ways to interact with internet (i.e. scrape some data based on gpt3 response?)
-rename windows (would prefer to find way to separate id and label, but I don't think that's available)

-------------------------------------------------------------------------------
easy tasks for low energy days

- lots of waiting docs in core.py
- dearpygui videos tutorials (https://github.com/hoffstadt/DearPyGui/wiki/Video-Tutorials):
    -parent stack system
    -callbacks
    -value storage system
-document session
-document transcript methods
