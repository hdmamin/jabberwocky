Context: been fiddling around with this project again lately but I'm not quite ready to restart daily contributions. Documenting where I'm at here for now so it's easier to pick up when I come back.

11/15/21 mon
------------
Left off
-talk() now prints responses with pretty formatted persona name.

Next Steps
-adjust query_kwargs so that we don't pass the wrong ones to gptj or gpt-neo mock funcs (maybe something like drop keys in query_kwargs that aren't in the func signature? Might not work if they take kwargs).
-write new mock funcs to support:
    -codex
    -new open source gpt-like model on modelhub (name is something about "sci" IIRC)
-live typing effect?
-consider a "choose from 1 of k responses" mode. Would need to think about how this would work w/ live typing and how it would work w/ conv manager (usually query() updates the conv history).
-maybe replace save/quit options w/ prompt_toolkit menus?

Context: picking up on this again for part 2 (Alexa skill).

1/31/22 mon
-----------
X -write premortem

2/2/22 tues
-----------
X -look through sample alexa conversation skeleton repo
    X -decide whether to use that template or "start from scratch"
        UPDATE: start by trying template.
X -create alexa skill in AWS UI
X -change invocation name
    UPDATE: needed to be 2 words and I wanted it to be short. Jabberwocky -> Voice Chat.
~ -start adding intents
    ~ -start adding slots
        UPDATE: wrote list of possible slots. added Person slot and Model slot.
    UPDATE: wrote list of possible intents in misc.txt. Added choosePersona intent.
-figure out how code will be structured locally (will alexa reference a separate repo if I use the conversation template? Will a single file basically be sufficient? Maybe an `alexa` subdir which can contain whatever I need it to?)
-follow UI instructions to determine next steps

2/3/22 wed
----------
~ -read up on dialog delegations strategy (do I want this?)
    _ -add to choosePersona if necessary
        UPDATE: alexa auto-determines next thing to ask? Most of my interactions should require minimal scaffolding so I don't think we need this.
~ -read up on intent confirmation (do I want this?)
    _ -add to choosePersona if necessary
        UPDATE: unnecessary at least for now. Consider adding later for Model or settings or choosePersona, maybe.
X -create more slots (see misc.txt)
X -create more intents (see misc.txt)
~ -see backlog
    X -fix incorrect usage of intents/slots 
        UPDATE: intents should use a var name and then you set the var type in the section below. Previously I was using the var type in the intent itself. I.e. "call {amazon.Person}" -> "call {Person}", where Person has type amazon.Person
    ~ -start testing interactions
        UPDATE: lots of failures. I realized my code has been filled in with their conv template but I need to make a lot of changes.

2/4/22 thurs
-------------
~ -look through code tab to see how expected interactions look
    X -try out in Test tab
        UPDATE: still having trouble getting this to work. Found docs for ask_sdk and decided to try to follow those locally.
~ -explore possibility of local ngrok
    UPDATE: not sure if this is compatible with lamda style app code. Table this for now and return to amazon hosted endpoint.
X -add code locally
    UPDATE: created new alexa dir.
-start following ask_sdk tutorial
    X -write launch handler
    ~ -write choose model handler
    ~ -write choose person handler
-update code to use my new intents
    -regular conversation
    -choose person
    -change max length
    -change temperature

2/4/22 fri
----------
-continue following ask_sdk tutorial
    X -finish choose person handler
        X -figure out best way to make conv manager available globally
            UPDATE: initially wrote wrapper class that delegates to an attr where is stores the manager, then realized it didn't seem to be doing anything useful. Just load the manager as a global var in the global scope. Can adjust strategy later if needed.
        UPDATE: tentatively done, though will likely need some troubleshooting/extra functionality/logging/error handling once I start testing.
    ~ -finish choose model handler
        UPDATE: tentatively mostly done, though need to figure out how to handle changing model to gptj or other non-int version. I think a better strategy than updating kwargs may be to store some session-level kwargs (I believe handler_input has some kind of session object that may be appropriate for this), then always pass that in to the query method.
X -write convenience func to access slots
X -write convenience func to speak/respond
~ -clean up template file a bit (delete unused classes)
_ -confirm stackoverflow-suggested method of extracting slots works (unclear - pycharm code completion stopped finding it)
    UPDATE: prob need to wait until I figure out how to run this thing. Template has a helper method that does something similar so that's an option too.

2/5/22 sat
----------
[DON'T LET THIS GET PUSHED BACK TOO FAR - don't want to end up having to rewrite a ton of code if something is incompatible.]
~ -figure out how to get this running on lambda (if possible: reliance on local files might be a problem)
    X -how hard would it be to convert this to something we can run with ngrok? Is lambda_function.py still compatible or is it totally different locally?
        UPDATE: decided to try rewriting with flask-ask. We're still early and it seems like dev/testing will be far easier (hopefully avoid slow build time for every change).
X -write new template app with flask-ask
X -try running flask app w/ ngrok
    UPDATE: accessible via generic url.
X -rebuild model 
    ~ -see if I can access flask-ask app through aws console
        UPDATE: no errors but still references console code rather than flask ask local code.
-look into handler_input session object
    -try using this to update kwargs (e.g. model, max len, etc.)
-more intent functions/endpoints:
    X -set temperature
    ~ -set max length
        UPDATE: added skeleton func.

2/6/22 sun
----------
~ -troubleshoot model build (no errors but still references console code rather than flask ask local code)
    ~ -try new flask-ask version in Alexa console (Test tab)
        UPDATE: spent most of time troubleshooting w/ minimal progress. Seems like the test console knows about the right URL (I think) but the api call is failing so it falls back to some generic message. Sample flask-ask code from aws tutorial has the same problem - possible that flask-ask is just no longer compatible ðŸ˜¬. When using test console, I see failed requests show up in app (not to any specific endpoint, looks like?).
-more endpoints:
    ~ -set max_length endpoint
        UPDATE: still need error handling though.
    -conversation end (add extra question offering to email transcript to user?)
    -generic conv response endpoint
X -fix logger (wasn't printing to stdout)
    UPDATE: unsure if fix was related to changing logging level or just restarting app, but it works now.
X -read up on flask-ask session vs. context (readthedocs.io tab)
    X -update it to store default _kwargs attr from conv manager
    X -update chooseModel to use this
    X -update chooseMaxLength to use this
    X -update chooseTemperature to use this

2/7/22 mon
----------
X -add error handling to max_length endpoint
-add more endpoints:
    ~ -conversation end (add extra question offering to email transcript to user?)
        UPDATE: started writing but it's a bit tricky with optional saving.
    X -generic conv response endpoint
    X -debugging intent (repeat user response back)
X -first draft of func to save conv (want it emailed rather than local)
-brainstorm: what else could possibly cause these alexa issues? How to troubleshoot and eventually fix?
-see if there are places I should add reprompt (chained method after question)

2/8/22 tues
-----------
_ -new endpoint for user to set email
    _ -add intent in console
    UPDATE: think I found a way to do this via amazon API rather than through voice.
    X -function to get user email using Amazon API
        UPDATE: untested though because it requires Alexa context object to run, which means I need to get the test tab working to see if it works.
-update other endpoints to check if should_exit
-rename `exit` function to avoid clobbering builtin
-brainstorm: what else could possibly cause these alexa issues? How to troubleshoot and eventually fix?
-see if there are places I should add reprompt (chained method after question)

2/9/22 wed
----------
X -rename `exit` function to avoid clobbering builtin
~ -see if we can easily update htools.quickmail to include text attachment
    UPDATE: can't fully test yet bc of freedom app, but seems to send without error at least. Though worryingly, text never arrives to phone when I tried that using {phonenumber}@vtext.com.
X -make new jabberwocky email for sending transcripts
    X -add info to htools creds file
    X -enable unsafe app access
    X -update email_user func to send transcript as attachment
-see if there are places I should add reprompt (chained method after question)

2/10/22 thurs
-------------
X -check hmamin2 email to see if quickmail works as expected (did emails arrive? did image attachment work? Did text attachment work? What is attached text file named?)
    UPDATE: email worked, sms didn't. Text just got appended to body though, not a real attachment.
    -update to allow multiple attachments
    X -investigate lack of sms showing up in text mode
        UPDATE: do not use leading 1 in phone number email.
    X -look for solution to lack of image attachments in text
        UPDATE: you need to use a different phone number email domain than vtex.com for mms, but it looks like it may not be supported anymore.
    X -update docs to reflect new signature
    -[BLOCKED: check emailed results first to see if attachment worked. Consider adding option to attach vs. embed.] bump version and upload to pypi
~ -look in alexa dev forum for how to direct 1 intent to another (prob need to use session to set temporary key)
    UPDATE: sdk does support "intent chaining". Unclear if flask-ask does, either natively or via compatibility w/ sdk.
    -update other endpoints to check if should_exit

2/11/22 fri
-----------
X -check hmamin2 email to see if quickmail works as expected (did emails arrive? did image attachment work? Did text attachment work? What is attached text file named?)
    _ -fix as needed
    X -[BLOCKED: check emailed results first to see if attachment worked. Consider adding option to attach vs. embed.] bump version and upload to pypi
    UPDATE: emails arrived, image attachment worked, no diff between image attach vs. inline, text attachment worked, text file is named same as source file as intended.
X -update curl to have verbose/non-verbose option
~ -read more of intent chaining tutorial
    UPDATE: not worth getting too far into this yet - wanted to make sure flask-ask provided some way to do this first.
    X -look for how to implement this w/ flask-ask
        UPDATE: can import delegate just like question or statement, but still figuring out how to use it.
    -update other endpoints to check if should_exit
X -update intent decorator to always store prev intent in session
~ -figure out how to test flask-ask locally
    UPDATE: still no progress. Think I need to figure this out before going any further - hard enough to build a hello world app without any mechanism for testing, but impossible for a complex skill w/ intent chaining.
X -brainstorm: what else could possibly cause these http/alexa issues? How to troubleshoot and eventually fix?
    UPDATE: some thoughts below
    -wrong AWS account?
    -start totally new skill in console and try again
    -pare app down to most bare-bones version and see if I can get it working in console (comment out new endpoints that might not have intents defined for them. Also might get rid of some boilerplate, like the "what's your fav color?" intro which I don't know the source of)
    -flask-ask is deprecated (sort of doubtful, since found a tutorial from not that long ago still using it)
    -alexa dev console is broken (sounds like it was at some point, but that was years ago - surely it's not still broken. Could create new skill w/ old console maybe? Forget if that's an option.)

2/12/22 sat
-----------
X -fill in some docstrings in app.py
~ -work through troubleshooting ideas 1 by 1
    ~ -wrong AWS account?
        UPDATE: think this is correct (name Harrison and initial H; couldn't figure out how to easily confirm email but I'm pretty sure it's the right one)
    ~ -pare app down to most bare-bones version and see if I can get it working in console (comment out new endpoints that might not have intents defined for them. Also might get rid of some boilerplate, like the "what's your fav color?" intro which I don't know the source of)
        UPDATE: got launch intent working on dummy app! Still haven't gotten other intents working though.
    _ -flask-ask is deprecated (sort of doubtful, since found a tutorial from not that long ago still using it)
        UPDATE: seems doubtful since bare bones app shows signs of working. Possible question/statement json is no longer compatible, I suppose.
    X -start totally new skill in console and try again
        UPDATE: combined this with bare bones app strategy.
    _ -alexa dev console is broken (sounds like it was at some point, but that was years ago - surely it's not still broken. Could create new skill w/ old console maybe? Forget if that's an option.)
        UPDATE: seems unlikely given positive signs with bare bones app.

2/13/22 sun
-----------
~ -continue trying to get dummy app to work
    X -add HelloWorld endpoint
        UPDATE: This works! 
    ~ -debug YesIntent endpoint (first expected response after launch. Error seems to be with recognizing the right intent, not with the response, based on terminal logs.)
        UPDATE: Still no success. Sincce HelloWorld works, my guess is YesIntent sample words are too short/generic ("yes", "sure"). Maybe AnswerIntent fails for the same reason.
~ -look at jabberwocky launch endpoint and see if I can find any differences that would cause it to fail when dummy app launch endpoint works (see misc.txt for promising idea)
    [IF theory of name clobbering is correct]
    X -rename Voice Chat to something less likely to have duplicates
        UPDATE: trying "Quick Chat". Tested this in dev console and it doesn't seem to be a built-in, unlike Voice Chat.
    X -rebuild model
    X -test launch intent for jabberwocky
        UPDATE: still failing. Think we may need to change template. See misc.txt.
    X -Try non-intent response to see if fallback handler works on jabberwocky (maybe don't even try to actively start app? Could try both ways.)
        UPDATE: no, just no response or built in default of some kind.
    X -If that doesn't work, switch to make skill debugger console the main one and deprecate/delete jabberwocky in console. Choice of vanilla skill rather than conv template might fix things.
        UPDATE: Created new skill since I can't figure out how to rename skill (can change name in invocation but not how it shows up in console, it seems)
-test new skill intents
    X -launch
        UPDATE: Yes! First working endpoint for jabberwocky. Needed to use name "Quick Chat" as alexa interprets "Voice Chat" as some kind of existing functionality which I haven't enabled, apparently.
    ~ -fallback intent
        UPDATE: not working yet. It did in dummy app so need to figure out what the difference is here.
X -map out conv flow model
    UPDATE: Drew out basic flowchart of interactions. See notebook.

2/14/22 mon
-----------
~ -troubleshoot fallbackIntent in new skill (worked in dummy app, so why not here?)
    UPDATE: my intent deco was implemented wrong. After fixing it, the fallback endpoint does work as far as reading the right response. However, while logging inside the fallback endpoint wokrs, logging inside deco does not. Unsure if this means the deco is somehow going unused (seems unlikely given recent behavior) or if this is a matter of logging output being hidden. Tried using diff logger than the default app one but this didn't resolve the issue. Tried checking prev_intent and it's not being updated either (supposed to be done in deco). However, if I manually update it in the func itself, it works.
X -add endpoint to list out all personas

2/15/22 tues
------------
X -add sys.exit in various stages of intent deco to try to get to the bottom of whether it's being called
    UPDATE: it wasn't. ask.intent deco is quite unusual and overriding it is confusing. Took a different approach.
X -write new ask subclass and redefine intent deco
    X -write custom IntentCallback
    UPDATE: ask.intent wrapper is odd, appears to never actually be called but it must be sometime. Must do some weird registration magic or something on the ask object. Managed to get my prev_intent tracking working by writing custom callback and having custom ask class wrap the input function when it's passed in (rather than explicitly defining on start/end behavior in decorator, define it via callback methods).

2/16/22 wed
-----------
X -add intents to console for new skill (jabberwocky-voice-chat, not jabberwocky)
X -add slots to console for new skill
X -test app in test console and see what doesn't work (sure there will be something)
    UPDATE: looks like it can send a request to choosePerson and changeTemperature endpoints but they receive no arguments (e.g. slots aren't working, or at least not how I expected them to).
    X -write next steps based on observed errors
        UPDATE: see tomorrow's to do list.
X -docs for custom ask (in particular: note to self not to delete seemingly useless wrapper inside intent deco. Flask-ask version uses that so I don't know what else is reliant on it)
-start updating endpoints in py file based on observed errors (see above)

2/17/22 thurs
-------------
-fix missing slots issue with choosePerson
    _ -replace actual logic w/ debugging/logging to figure out what (if any) arg(s) func is receiving
        UPDATE: looked at existing logs and error indicatded no arg of any kind was received.
    X -update parsing to get correct object
        UPDATE: found blog post implying custom slot might have to be extracted from request manually. Wrote new code to do that and it works for choosePerson (successfully selected an existing persona!).
    X -reintroduce actual logic and test
        UPDATE: new persona doesn't work yet (relies on YesIntent, which I added now, but much more new logic is needed).
~ -start updating endpoints in py file based on observed errors (see above)
    ~ -changeModel
        X -rename chooseModel -> changeModel (forgot I chose diff name in console for this skill)
        X -fix ModelType slot (4 -> 0)
        UPDATE: added some str handling to chooseModel endpoint. Model is currently rebuilding in console.
    -changeTemp
        UPDATE: brief testing reminded me of issue where alexa thinks I'm trying to change temperature of an actual device. Will need to consider options for how to handle this.
    -changeMaxLen

2/18/22 fri
-----------
~ -continue updating endpoints in py file based on observed errors (see above)
    -changeModel
        X -check that build succeeded
        X -try out zero, one, two, three
        _ -try out j
        _ -try out neo
        UPDATE: might want to rewrite j/neo query funcs w/ availability of new open source models (and possible deprecation of old gpt-j api? Unclear.) Do this later.
    -changeTemp
        ~ -consider how to deal with issue noted in prev day's notes (could rename to Temp/something else, or maybe there's a way to override default commands when in a custom skill)
            UPDATE: tentatively seems okay now - I think maybe before I was saying this after returning a Statement rather than a Question so we had exited the skill?
        ~ -figure out how to convert str to num
            UPDATE: considered requiring an int in [0, 99] and hardcoding list of strings. Was curious though and decided to try gpt way - got this working reasonably well but only with a disappointingly big model (smaller ones prob could do well with more prompt tuning but I didn't quite get there yet). Added prompt text and config files and added API call to app script. However, still having some trouble parsing decimals.
    -changeMaxLen
    -reply

2/19/22 sat
-----------
-continue fleshing out endpoint logic
    ~ -changeTemp
        X -consider options for improving number parsing and choose one (i.e. require int from 0-99 and divide answer by 100, or update slots to expect "point {number}".)
            X -make necessary changes in console and/or script
            _ -update gpt word2number prompt files accordingly (e.g. maybe no decimals needed after all, could just be word2int)
                UPDATE: unused now so not important. For general purpose future use, better to leave it more broadly capable.
        UPDATE: changed to use hard-coded dict with some fuzzy matching. Couldn't test yet since I accidentally blocked dev console w/ Freedom.
    -changeMaxLen
    -reply
~ -planning re maintaining settings
    UPDATE: initially started as just trying to alias session.attributes with something shorter since it's used so much. Surprisingly trick. Ended up causing me to think about settings and realizing we really want 3 diff levels (see misc.txt notes from today). Started building new class to maintain this.

2/20/22 sun
-----------
X -continue fleshing out endpoint logic
    X -changeTemp
        X -test new changes in console.
            UPDATE: updating temperature works for valid integers. Non-number word rightfully raised an error which was handled gracefully.
    X -changeMaxLen
        X -look more if there's a better way to do this with alexa (surely yes, but seems like no???)
        _ -add gpt3 parsing (too many ints to hardcode cleanly)
            UPDATE: Turns out it does this automatically after all. Previous failures might have been due to flask ask argument issue. Tested this on both valid and invalid values.
X -update settings intents to accept optional slot for scope (global/person/conversation)
    UPDATE: still need to fully make use of this in app.py.
X -Settings class
    X -think more about desired interface (important - need to know what I'm trying to implement)
    X -continue fleshing out attr access methods
    X -write resolve() method
    X -update app.py accordingly
X -start building query_gpt_j variant using banana.dev backend (prob smallish version, but good to have another free option in case random git one gets deprecated)
    UPDATE: need to add support for 'stop' param, maybe better arg validation to ensure no nnaming issues around topK vs top_k, etc.

2/21/22 mon
-----------
X -consider: should Settings include kwargs only or also things like prev_intent/should_end/should_save?
    X -update app.py and/or utils.py accordingly
    X -document Settings
    UPDATE: include both in Settings. Make kwargs accessible via bracket notation and general settings accessible via dot notation. Also try to cut down on number of general (non model query) settings - rely more on prev_intent and yes/no. Might have to change yes/no intent endpoints to not update prev_intent though.
X -test new changes in test console
-flesh out `reply` endpoint

2/22/22 tues
------------
-debug/fix: choose person intent seems to soak up any reply containing a name (or at least sometimes). Can try:
    X -adjusting sample utterances for choosePerson intent in console and re-test
        UPDATE: found "Call {person}" no longer is clobbered by default behavior. Guessing I previously exited the skill session w/out realizing it before trying to trigger that intent.
    ~ -add some logic using prev_intent to infer when a reply has mistakenly been identified as choosePerson
        UPDATE: added check in choose_person if conv is active but this still chops off the first word of the response  we can't recover the full text.
    X -Add new `reply` endpoint w/ AMAZON.SearchQuery slot (this is supposed to soak up the whole text response)
        UPDATE: had to use leading space to avoid errors.
-try refactoring out base functionality of endpoint funcs (bc flask_ask not passing args to intent funcs, we need to get slots inside func, which makes it hard for one intent to call another)
    UPDATE: Ended up being unnecessary so far but might have to reintroduce at some point. Another option is to make endpoint accept an optional arg and then use it if provided and extract slot if not.

2/23/22 wed
-----------
X -try updating reply intent to match what I made in console
    X -test in console
    _ -update/rm fallback intent in code
        _ -and rm in console if necessary
        UPDATE: decided keep for now. Might need an extra layer of fallback beyond SearchQuery.
    UPDATE: works, though identified possible bug on second query (I also called changeModel in between). Conv didn't think a conv was in progress - does conv object get recreated after each skill call? Need to investigate. Also found possible bug where sessionstate str appeared empty in console even though mock_func should have been set.
X -update ask deco to log settings state on each intent call
-consider yes/no intent logic: need to prevent them from updating prevIntent? Consider tracking a longer intent history list rather than just prev.
    -make updates accordingly
-see if we can move IntentCallback and CustomAsk to utils (bit tricky w/ reliance on session?)
X -make use of Scope var in settings intents
    UPDATE: also added `default` option to slots func to make this work better when user doesn't specify level.

2/24/22 thurs
-------------
X -investigate issue where second reply causes error due to "conv not in progress". (see yesterday's notes documenting this error and the one below)
    UPDATE: rewrote slots func a bit (I think I was doing several things wrong - list(values())[0] logic was not safe, changeTemp func passed wrong slot name to slots for Temperature). Seems to work now.
~ -investigate bug where Settings obj __str__ appears empty in logged message
    UPDATE: could not reproduce. Keep an eye out for this.
X -add readContacts intent in console
    X -test in test console
-update end of conv to erase conv-level settings
    -update change of person to change person-level settings
    -maybe update settings.clear() to allow clearing a single (or finite number) of state levels.

2/25/22 fri
-----------
X -add readSettings intent to console
    X -add code endpoint
    X -launch rebuild
    X -test in console
X -add endChat endpoint
    UPDATE: also had idea to prefix settings commands with "Alexa" to distinguish them from replies. But when testing, alexa did not recognize these utterances (just fell back to reply intent). Need to work on this more.
-update end of conv to erase conv-level settings
    ~ -consider whether settings resolution order is right/desirable (i.e. should global level take priority or should conv level?)
        UPDATE: thought a bit and this is quite a complex task. Still not at a solution yet but my sense is that priority should be more about sequence (most recent = highest priority) and scope should just be thought of as "when do we undo this setting change action"? Need to hold off on other changes until I figure out desired system.
    -update change of person to change person-level settings
    -maybe update settings.clear() to allow clearing a single (or finite number) of state levels.
~ -start prototyping prompt routing system (a way to implement intent chaining, basically)
    UPDATE: some progress here. Plan is to have an endpoint func's follow up func by named identically except with a leading underscore. Could become problematic if we need multiple followups though. Started testing on end_chat but ran into utterance problems (see above).
-tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)
    
2/26/22 sat
-----------
[LEAVE EMAIL UNBLOCKED - testing email sending]
X -try adjusting end_chat utterances since curr ones aren't getting recognized
    X -maybe adjust other settings intent sample utterances to be prefixed by something distinctive. Looks like 'alexa' might not have been the best choice.
        UPDATE: noticed my attempt to trigger end_chat was getting sent to SearchQuery WITHOUT the leading "Alexa", adding to my theory that this word is treated differently. Decided on the name "Lou" for the skill "assistant" (one syllable, inspired by Lewis Carroll, gender neutral) and prefixed sample utterances for all settings-related commands with it.
~ -continue testing end_chat intent + prompt chaining
    ~ -troubleshoot transcript sending as needed
        UPDATE: initially was getting 401 status code due to 'Bearer: {token}' instead of 'Bearer {token}'. Fixing this now gives me a 403 status code (also added permission for user first name and email in console, and gave my personal permission in iphone app and alexa website). Still not working though.
X -look into EndSession more
    _ -When is it called? I.e. is it triggered by "stop" or something else?
    X -What should it return? I think this is correct based on an official tutorial but should confirm.
    UPDATE: every example I've seen handles this identically, so I think there's no need to look into this any more.

2/27/22 sun
-----------
-try sending transcript w/ hardcoded email (need break from debugging permissions issues)
    -debug formatting, etc.
-look into email access issues (api returns 403 Forbidden status)
    -maybe need to add card/audio asking for permission in the skill itself?
X -write choose_person followup func (do you want to download new persona?)
~ -deep dive into intent chaining
    UPDATE: flask-ask does have delegate() and elicit_slot() functionality which seems to be related to what I want, but I couldn't quite figure out how to use it do what I want (no docs :( ). Instead, started overhauling this process in new `chain` branch. Renamed searchIntent intent from reply -> delegate and have that point to enqueued funcs. Made Ask object handle this enqueue/deque process. Still need to test this more.

2/28/22 mon
-----------
~ -test new chain workflow
    -selecting a user that exists
    -selecting a user that does NOT exist and NOT downloading them
    -selecting a user that does NOT exist and downloading them
    UPDATE: made some major changes and saw some positive signs but haven't thoroughly validated that any of the above cases work post-changes.
X -auto avoid duplicate calls from queue (realized alexa often matches intents without help of queue/delegate, so enqueueing intents was problematic)
-rm remnants of old chaining system (followup_func, check for other '_' prefixed funcs, etc.)
    -consider if I should remove/update prev_func state attr? Maybe it should store func name instead of intent name since we're now often calling non-intent funcs.
-look for other intent chains that need to be implemented and document below
    -implement the following intent chains:

3/1/22 tues
-----------
X -test new chain workflow
    X -selecting a user that exists
    X -selecting a user that does NOT exist and NOT downloading them
    X -selecting a user that does NOT exist and downloading them
X -rm remnants of old chaining system (followup_func, check for other '_' prefixed funcs, etc.)
    X -consider if I should remove/update prev_func state attr? Maybe it should store func name instead of intent name since we're now often calling non-intent funcs.
        UPDATE: fine for now, not causing any problems. Maybe revisit later if I find a good reason to store more steps.
X -try sending transcript w/ hardcoded email (need break from debugging permissions issues)
    X -debug formatting, etc.
    UPDATE: Had wrong email in alexa config. Once I fixed that and updated htools.quickmail to allow Paths for attach files, no problems with email or formatting.
-look for other intent chains that need to be implemented and document below
    -implement the following intent chains:
~ -debug issues where changing settings wasn't working
    UPDATE: 1 issue was that MaxLength had wrong slot name (should be Number, not maxLength). Another was that "change model" sample utterances didn't include the exact order I was saying and I guess they're pretty rigid, so I added a couple more variants. Also changed to debug mode using htools deco to print when any func is called, bc I'm still finding it hard to follow flow sometimes.

3/2/22 wed
----------
X -look into ask.logger log file (is it just continually growing longer and longer? Maybe reset.)
    UPDATE: reset each time we instantiate Ask so it's just one session's worth of logs.
X -update send_transcript() to leave file in alexa/conversations by default rather than deleting.
X -maybe update deco to print queue/state etc. on function end as well. Sometimes confusing not having this info.
X -move some constants to config
X -clean up imports a bit
X -fix casing bug when generating a person
    UPDATE: prob should update wiki func itself to handle this, but for now just passed in a titlecased name instead to avoid updating lib.
X -adjust slots strategy to only use "resolutions" as a backup (saw some cases where resolutions worked worse than "value")
X -rm some old files in alexa dir from abandoned appraoches
    UPDATE: just moved to trash in case I need to recover later.
-look for other intent chains that need to be implemented and document below
    -implement the following intent chains:
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/3/22 thurs
------------
X -investigate: possible to get alexa to maintain casing/punctuation?
    X -if no, consider running punctuate on transcriptions before sending to gpt3. All lowercase may change results (more casual vibe?).
        UPDATE: no native support. I copied my punctuate_transcription prompt to punctuate_alexa and lowercased the first letter of each transcription, then added this to reply(). Seems to be working as expected so far.
X -tweak logging to be a bit easier to grok.
    UPDATE: some intent calls are nested and it was hard to tell which ON END log statement corresponded to which intent. Updated on_end call to print prev intent as well.
X -look into flask-ask reprompt (does this exist and where can I apply it if so?)
    UPDATE: added reprompts to all question() uses that seemed appropriate. Still need to test this in UI. Also updated some of the settings reading options to auto redirect to choose_person if the user's not in the midst of a conversation already.
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/4/22 fri
----------
X -add intent to change settings to auto-punctuate or not
    UPDATE: realized adding separate enable/disable endpoints was easier.
X -convenience func to maybe prompt user to choose_person depending on if in active conv
X -fix bug in _reply logger (can't pass multiple strings unlike print)
X -fix bug w/ func queue not being cleared on launch
-add support for banana.dev version of gptj 
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/5/22 sat
----------
~ -look into email access issues (api returns 403 Forbidden status)
    ~ -maybe need to add card/audio asking for permission in the skill itself?
    UPDATE: got personal email working via manual access and got consent card to display, but no haven't figured out how to process response yet. Included more notes in app.py (search tmp_email_me; eventually prob need to delete or change this, both in code and in UI. Just used a dummy intent triggered by "email me" for testing purposes.)
X -update get_user_email func to get name too
-add support for banana.dev version of gptj 
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/6/22 sun
----------
X -consider refactoring to make it cleaner to reset all state (queue, attrs, query kwargs, etc.). Periodically finding new things that aren't reset when I update code and flask reloads.
X -look for ways to integrate user name into skill
    X -any time Lou speaks, e.g. welcome message?
    X -maybe edit transcript to replace '\n\nMe: ' with '\n\n{name}: '?
        UPDATE: better to change ConversationPersona itself to use name in prompts. That way, generated responses can actually refer to you by name.
-allow fuzzy match for choosePerson, or at least catch last names (e.g. einstein -> albert einstein; felt like a questionably useful feature initially but even just for dev purposes it would be nice to not have to type out the full name each time)

3/7/22 mon
----------
~ -look into user email/name access issues (worked on sat but on sunday, had to provide access in alexa app again (i.e. it revoked access at some point?). Might be related to consent card, i.e. user has to provide consent for every new conv?)
    UPDATE: permissions were revoked again.
    X -add privacy policy (necessary for skills that request name/email)
        UPDATE: Also filled in some other details like skill description and sample utterances.
X -allow fuzzy match for choosePerson, or at least catch last names (e.g. einstein -> albert einstein; felt like a questionably useful feature initially but even just for dev purposes it would be nice to not have to type out the full name each time)
    UPDATE: for simplicity, just check for str equality w/ last name. Don't want any crazy assumptions being made and choosing the wrong person.

3/8/22 tues
-----------
X -add small skill icon (128x128 px) to alexa Distribution tab
    X -add large skill icon (512x512 px) to alexa Distribution tab
    X -check that icon shows up in iphone app
X -maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.
    UPDATE: mostl converted some print statements to log statements with the help of a slightly hacky getattr-inspired function that works with nested attributes and globals.
X -see if we can move IntentCallback and CustomAsk to utils (bit tricky w/ reliance on session?)
X -update end_chat flow to not ask about sending transcript if no email has been provided

3/9/22 wed
----------
X -should conv prompt config be updated to use "\n\nMe:" instead of "Me:"? More robust if we replace w/ name. Would need to update ConversationManager as well, probably.
~ -update query_gpt3 to support goose.ai models (e.g. neo 20b)
    UPDATE: wrote gooseai_backend contextmanager func and started openai_backend contextmanager class. The latter still needs some work and might end up replacing the former.
X -sign up for banana.dev and store api key
-add support for banana.dev version of gptj (see ipy in tab 1)

3/10/22 thurs
-------------
X -investigate issue where gpt3 (davinci model ðŸ˜±) seems to be used by alexa when I thought I set it to use gptj mock func. (See openai costs tab.)
    UPDATE: failed to set mock=True, both in alexa and gui, for gpt-j usage. Updated query_gpt3 to automatically do that when mock_func is not none. Still concerned that func's interface might need to change dramatically.
X -flesh out openai_backend support
    X -test openai_backend class in ipython
        X -switch from openai to gooseai
        X -switch from gooseai to openai
        X -persist mode works
        X -no persist mode works
    X -can this replace my gooseai_backend contextmanager func?
    UPDATE: major rewrite -> BackendHandler class. Deleted old contextmanager func.
-figure out best interface to use gooseai (just use query_gpt3 w/ diff backend? Kind of annoying that I have to pass mock_func in some cases, but in this I change the backend.)
    -look into docs on api_backend and api_key. Maybe all my mock_funcs could use this? Would be nice to avoid passing in mock_func.
    -make sure we can still track (via kwargs or other attr, perhaps) what backend is being used. Don't want this to be assumed.
-[BLOCKED: confirm possible redesign of backend switching and/or mock funcs] make dist and upload to pypi w/ new query_gpt3 changes?
-add support for banana.dev version of gptj (see ipy in tab 1)

3/11/22 fri
-----------
~ -easy option: write docs for...
    X -IntentCallback
    -look in alexa/utils and alexa/app for others
-figure out best interface to use gooseai (just use query_gpt3 w/ diff backend? Kind of annoying that I have to pass mock_func in some cases, but in this version I change the backend. Maybe can adjust BackendHandler and/or other query_gpt_{x} funcs to handle everything automatically.)
    ~ -look into docs on api_backend and api_key. Maybe all my mock_funcs could use this? Would be nice to avoid passing in mock_func.
        UPDATE: didn't find any formal docs besides a mention of some azure endpoints - nothing with huggingface.
    -make sure we can still track (via kwargs or other attr, perhaps) what backend is being used. Don't want this to be assumed.
    -[BLOCKED: confirm possible redesign of backend switching and/or mock funcs] make dist and upload to pypi w/ new query_gpt3 changes
-add support for banana.dev version of gptj (see ipy in tab 1)
X -add uses of _maybe_choose_person to other settings funcs
X -add str(int) versions of model_i values for changeModel (observed some errors where model change failed or wrong intent was chosen because transcription used "2" instead of 2).
X -debug new unexpected uses of gpt3 vs. gptj
    UPDATE: realized when I change code, flask refreshes some things but doesn't call launch() automatically. So app state does not fully refresh. Set debug=False since refresh func doesn't work before app.run() is called and that call is blocking.
X -make changeModel set mock_func=None when changing engine_i to an int (realized if prev was gpt-j, changing to engine_i=0 wasn't removing that and so we just queried gptj and igonred the engine_i arg)

3/12/22 sat
-----------
~ -figure out best interface to use gooseai (just use query_gpt3 w/ diff backend? Kind of annoying that I have to pass mock_func in some cases, but in this version I change the backend. Maybe can adjust BackendHandler and/or other query_gpt_{x} funcs to handle everything automatically. Maybe need more separation between the concept of a backend (e.g. openai, gooseai, banana, huggingface) and model (e.g. openai-davinci, gptj-6b, gooseai-6b, huggingface-6b))
    UPDATE: yes, keep backend and model separate. Maybe can adjust neo function to accept engine_i too (with gpt-j-6b, they technically have 4 solid options now too).
    X -create new changeBackend intent in console
    X -create changeBackend endpoint
    ~ -test
        UPDATE: initial attempt failed to recognize endpoint but can't remember if I built model since most recent changes. Launched build and can test tomorrow.
    X -new backendselector functionality:
        X -get current api name
        X -get model name from user-specified engine_i and current api name
-easier option: look for docs in app.py, utils.py

3/13/22 sun
-----------
~ -test changeBackend endpoint in console
    UPDATE: mixed results. Switching to openai works more reliably than gooseai, and typing works more reliably than speaking. Speaking gooseai never worked - always just defaulted to SearchIntent. Decided to write my own routing logic as a fallback in case alexa doesn't recognize certain intents.
X -write function to convert json editor -> fuzzy dict mapping utterance to intent. (handle all combos of sample slot values for all sample utterances for each intent)
    UPDATE: saved in data/alexa dir.
-update query_gpt_neo function in jabberwocky.openai_utils
    -support 6b param model
    -see if we can adjust interface to use engine_i rather than size
-gui updates?
    -decide if that's within the scope of this project. Could always pin jabberwocky version for gui.
    -if yes to prev point, adjust gui accordingly (cur offer different radio buttons for each neo size; could instead have 1 radio and have it actually use the engine_i slider)
    -add gooseai backend option to gui?

3/14/22 mon
-----------
X -write infer_intent func to guess based on fuzzy dict simlarity results
    X -update delegate to use it when appropriate
X -write CustomAsk method to get func (not just name) from intent name
X -move prev_intent update to occur post pre-func logging but before post-func logic
~ -continue testing changeBackend in console
    UPDATE: some success w/ inferring intent, but need to figure out how to handle slot vals. Could try to extract them using regex and pass to funcs directly, or extract and then mutate slots object, or pass in whole str and use some sort of partial_token_set_ratio or scorer that supports partial matches, or always raise error.
    -for diff utterances, see what matching intents are printed out by fuzzykeydict
    -see what common similarity scores are
    -determine a good way to map from similar intents to an actual choice (i.e. do all need to match? Do we just care that the top one is over some similarity threshold, and if so what? Do we take some weighted score, e.g. if top match is 80% changeBackend but #2 and #3 are both 79% changeModel?)
        -what other logic (if any) do we need to implement to make this work? In some cases we may want to trust the queued function, in some maybe not? Might also be able to hardcode some logic, i.e. "startswith(Lou)".
    -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -try some other intents and assess routing logic quality.
    -maybe assess whether n=3 is a good choice for similar keys

3/15/22 tues
------------
~ -Consider how to handle slot vals when calling an inferred function from delegates(). Could try to extract them using regex and pass to funcs directly, or extract and then mutate slots object, or pass in whole str and use some sort of partial_token_set_ratio or scorer that supports partial matches, or always raise error.
    UPDATE: decided to try manually extracting slots - atm, there are only 5 intents w/ slots and most seem pretty feasible to extract. Wrote draft function for names (choosePerson), numbers (maxLength, temperature), and backend (e.g. gooseai).
-continue testing changeBackend in console
    -for diff utterances, see what matching intents are printed out by fuzzykeydict
    -see what common similarity scores are
    -determine a good way to map from similar intents to an actual choice (i.e. do all need to match? Do we just care that the top one is over some similarity threshold, and if so what? Do we take some weighted score, e.g. if top match is 80% changeBackend but #2 and #3 are both 79% changeModel?)
        -what other logic (if any) do we need to implement to make this work? In some cases we may want to trust the queued function, in some maybe not? Might also be able to hardcode some logic, i.e. "startswith(Lou)".
    -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -try some other intents and assess routing logic quality.
    -maybe assess whether n=3 is a good choice for similar keys

3/16/22 wed
-----------
X -decide if that's within the scope of this project. Could always pin jabberwocky version for gui.
    UPDATE: no, just pin version of jabberwocky for gui requirements.txt and make a separate one for alexa.
X -update project files to account for decision to pin gui to diff version of jabberwocky
    X -make 2 requirements files
    X -add make install_dev cmd
    X -update readme accordingly
-update query_gpt_neo function in jabberwocky.openai_utils
    X -support 6b param model
    X -see if we can adjust interface to use engine_i rather than size
    ~ -test how performance (speed, reliability) compares to free gpt-j api. Trying to decide if one or both can be deprecated. (Would make settings easier to handle and generally more intuitive if "model" slot was always a number.)
        UPDATE: gptj api is down atm. Neo function appears to work, though slowly - haven't timed yet since gptj is down.
~ -look into adapting openai Completion api to use neo and vicgalle as backends
    UPDATE: I'm sure we *could* do this but after a bit of digging, it seems sufficiently complicated that I I'm not sure it's the easiest way. Might be easier to have backend provide a query() method that inserts mock_func into query_gpt3 kwargs and use that in place of query_gpt3? But that may not work with convmanager/promptmanager. Could update those, of course.
_ -write slot extraction func for chooseModel (see bottom of alexa/utils.py for other sample slot extraction funcs)
    UPDATE: plan is to make huggingface and vicgalle backends rather than models, so model can always be a number. No need for a new parsing function.
    -consider interface. Should this extraction occur w/in each func separately, or create 1 SlotExtractor that does this in delegates() after inferring a function?
    -implement chosen method
-continue testing changeBackend in console
    -for diff utterances, see what matching intents are printed out by fuzzykeydict
    -see what common similarity scores are
    -determine a good way to map from similar intents to an actual choice (i.e. do all need to match? Do we just care that the top one is over some similarity threshold, and if so what? Do we take some weighted score, e.g. if top match is 80% changeBackend but #2 and #3 are both 79% changeModel?)
        -what other logic (if any) do we need to implement to make this work? In some cases we may want to trust the queued function, in some maybe not? Might also be able to hardcode some logic, i.e. "startswith(Lou)".
    -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -try some other intents and assess routing logic quality.
    -maybe assess whether n=3 is a good choice for similar keys

3/17/22 thurs
-------------
-updates to reflect recharacterization of neo and j as backends rather than models
    X -in console, rm J and Neo as model options
    X -in console, add j and neo as backend options
        UPDATE: renamed J to Vic since it specifically refers to the vicgalle api (I assumed this was a name Vic Galle but I don't actually know. Not important though.).
    X -update saved fuzzy dict from new skill json (see func in alexa/utils)
    ~ -update app.py to account for changes
X -add slot func option to intent decorator and update intent funcs accordingly
-consider how to support j and neo as backends in BackendSelector
    -implement

3/18/22 fri
-----------
X -consider if any intents should be removed from fuzzy dict of inferrable intents (e.g. Yes, No, ones without slots?)
    _ -implement
    UPDATE: Yes and No are already not included. Other intents without slots could still be worth inferring so leave those in.
X -update slot processing code for each intent in fuzzy dict (don't worry about composing slot funcs yet, just update signatures)
    X -changeModel
    X -readContacts
    _ -emailMe
        UPDATE: intend to rm this eventually anyway.
    X -changeTemperature
    X -changeMaxLength
    X -readSettings
    X -enableAutoPunctuation
    X -disableAutoPunctuation
    X -endChat
    X -changeBackend
X -consider how to handle intents w/ multiple slots (maybe need to compose funcs like get_num, get_scope)
    UPDATE: change slot_func to slot_funcs which maps each slot name to a function.
    X -implement
    UPDATE: overhauled whole slot_func interface. Specify using type annotations, then make ask.intent auto-construct the slot parsing func based on that. Haven't tested yet outside of toy examples in ipython.
-work on new get_backend func (jump off of alexa/utils version) to handle new vals (see fuzzy dict "fd" in ipython sess below this pane)
     -consider approaches: __contains__, fuzzywuzzy, fuzzyregex or spacy matcher, gpt infer
     -implement
-test each inferrable intent in console (e.g. type something like "Lou, change backend to Open I" or speak "Lou change backend to goose AI" (hard for it to recognize))

3/19/22 sat
-----------
~ -work on new get_backend func (jump off of alexa/utils version) to handle new vals (see fuzzy dict "fd" in ipython sess below this pane)
     ~ -consider approaches: __contains__, fuzzywuzzy, fuzzyregex or spacy matcher, gpt infer
     ~ -implement
    UPDATE: tried a ton of stuff combining str similarity, phonetic similarity, gpt prompting, etc. Still haven't quite gotten something satisfactory.
X -update valid backend slot utts
    X -rename vic -> hobby (real word, not a name)
    X -regenerate fuzzy dict
-work on get_scope function
-test each inferrable intent in console (e.g. type something like "Lou, change backend to Open I" or speak "Lou change backend to goose AI" (hard for it to recognize))
    -test ability to extract slots in each case (above task refers to our ability to infer intents, which is a distinct step)

3/20/22 sun
-----------
-adjust changeBackend intent func - slot utts allow both with and without spaces. Need to account for that.
~ -investigate phonetic similarity->str sim approach in ipython below. Why all score=86? Maybe need diff str sim method.
    ~ -go through ideas in misc.txt until I find a get_backend I'm happy with
    ~ -look into Adrienne idea re formnat values
        UPDATE: lots of missing values in table I found compared to eng_to_ipa encoding. Also couldn't quickly find good way to encode consonant similarity.
    ~ -try patternomatic spacy package
        UPDATE: lots of code issues, seems like not compatible w/ current version of spacy.
    UPDATE: fairly promising method is to store slot vals used for each utt and then just use those from closest utt. Avoids lots of extraction hassle, but might not have really solved the poor transcription issue. That might be better solved with gpt3 - perhaps asking it to correct spellings will work better.

3/21/22 mon
-----------
X -update infer_intent to provide slots in weighted case (see TODO)
_ -prototype new gpt prompt asking to correct spelling rather than correct+extract all in 1
    UPDATE: let's hold off on this until I see more evidence that it's actually needed. Maybe new method will work already.
X -write deprecated deco and mark slot extraction funcs as such
    UPDATE: put this in htools rather than jabberwocky.
X -adjust changeBackend intent func - slot utts allow both with and without spaces. Need to account for that.
    UPDATE: changed BackendSelector to lowercase input and remove spaces. Think that should be sufficient.

3/22/22 tues
------------
~ -consider how to support hobby and huggingface as backends in BackendSelector
    X -implement
    UPDATE: wrote something that I think should work but needs much more testing.
-continue testing changeBackend in console
    -is the intent getting recognized?
    -if not, what intents (if any) are getting inferred?

3/23/22 wed
-----------
~ -test backendselector more in jupyter (not ipython, want to see docstring)
    X -do base and key change when we expect them to?
    X -does query() show docstring as expected?
        UPDATE: had to add modified fastai decorator but now it works.
    ~ -try actual queries w/ all functions wrapped in debug. Make sure mock funcs are getting called when appropriate.
        UPDATE: Seems to work but both gptj and huggingface eleuther apis are down or deprecated - unclear.
X -renew goose.ai api key
-continue testing changeBackend in console
    -is the intent getting recognized?
    -if not, what intents (if any) are getting inferred?

3/24/22 thurs
-------------
X -rename query neo to match backend name
X -maybe rename backendselector class? No longer just a backend selector exactly.
X -make ls() (from nb) a method of backendselector?
X -add gptbackend method to get current mock_func
X -make other extra simple mock funcs supported by backendselector? With 2 apis down, there's no free option for testing atm. (Note: make sure you don't break query_gpt3 if so, since mock_funcs currently edit the object loaded by our regular mock response).
X -update convManager to use new backend.query api
X -update promptManager to use new backend.query api
    UPDATE: updated query and kwargs methods, then briefly kwargs tested in jupyter. Seems all good. But I then went back and made gpt.query a classmethod so need to test this again.

3/25/22 fri
-----------
X -test convManager and promptManager in jupyter after latest change to gpt.query (see last bullet point yesterday)
    X -convManager.kwargs
    X -promptManager.kwargs
    X -convManager.query
    X -promptManager.query
X -continue testing changeBackend in console
    X -is the intent getting recognized?
    X -if not, what intents (if any) are getting inferred?
    UPDATE: intent inference seemed to work very well initially (magically, no bugs!) but at end of session realized problem w/ weighted inference strategy.
~ -test each inferrable intent in console (e.g. type something like "Lou, change backend to Open I" or speak "Lou change backend to goose AI" (hard for it to recognize))
    _-test ability to extract slots in each case (above task refers to our ability to infer intents, which is a distinct step)
    _ -for diff utterances, see what matching intents are printed out by fuzzykeydict
    ~ -see what common similarity scores are
    ~ -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -maybe assess whether n_keys should change in fuzzy_dict.similar() call
-easier(?) task: add support for banana.dev version of gptj (see ipy in tab 1)
-fun task: adjust Ask to track how many calls deep we are for a single user turn and adjust logging indentation accordingly. Make it visually easier to see when an intent call is really done vs. when it was just a nested call inside another intent.

2/26/22 sat
-----------
[NOTE: currently have gooseai enabled for both punctuation and replies. Hobby backend still down and huggingface backend seems too slow to do 2 tasks (punct + reply) in time for alexa.]

X -add "repeat" option to dialog model in UI
    X -generate new utt2meta.pkl
X -update readSettings to include defaults (i.e. if user hasn't explicitly set Model, this intent won't read it)
    X -update
    UPDATE: adjusted settings object to store these by default.
    X -test in console
X -add makefile commands for run_alexa and ngrok (realized I forgot ngrok cmd after restarting computer)
-add a "back to your conversation" msg for when changing settings mid conv
    X -implement
    -test in console
X -fix weighted strategy in infer_intent (e.g. if all 5 closest matches are changeModel but the closest match is .4, we wouldn't want to delegate to that, but we do atm)
    X -implement
    X -test
~ -troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    ~ -see if we can increase timeout
        UPDATE: sounds like default timeout is 10sec for skill to provide a response (incidentally, user has 8sec to provide a response) and these values are not configurable. However, it may be possible to hack a workaround using progressive responses, but this would not be ideal for every response. I.e. if we know a longer completion is coming, we could probably have alexa say "hmm, let me think about that" and then follow up with the real response. But we don't want alexa to say that every turn.
~ -adjust Ask to track how many calls deep we are for a single user turn and adjust logging indentation accordingly. Make it visually easier to see when an intent call is really done vs. when it was just a nested call inside another intent.
    X -implement
    ~ -test
        UPDATE: didn't see any indentation yet but I didn't check thoroughly if the sequence of actions/intents called for it. Do this tomorrow.
        

3/27/22 sun
-----------
X -test call stack logging more thoroughly in console (does it indent messages when it should?)
    UPDATE: ended up taking different approach since indenting was not affecting other logged statments - wrote custom formatter to address this. Also updated on_end to show which intent just ended since it wasn't before. Non intent-callback-induced log statements are indented more than intent-callback-induced log statements but I think that's okay and some of those (e.g. slots, the most common and annoying) will likel be removed anyway at some point.
-investigate potential issue where ' Me:' is getting attached to end of gpt responses. Unsure if weaker model is skipping the double newline and therefore not being caught by the stop phrase, or if conv.me set op isn't working right, or if gooseai includes the stop phrase(s) in the response.
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    -check if openai supports this too or just gooseai
    -implement
-update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    -check which backends support this
    -implement

3/28/22 mon
-----------
X -move logging to query_gpt3 or GPTBackend.query (rather than in conv/prompt managers only)?
    _ -first investigate if this is already done natively - vaguely recall seeing something in package code suggesting it was
    UPDATE: whoops, forgot to check for native solution. But already finished this so I guess it's fine.
~ -investigate potential issue where ' Me:' is getting attached to end of gpt responses. Unsure if weaker model is skipping the double newline and therefore not being caught by the stop phrase, or if conv.me set op isn't working right, or if gooseai includes the stop phrase(s) in the response.
    UPDATE: tentatively confirmed that gooseai does not truncate before stop word. Started refactoring this functionality out of huggingface mock func but found new issue re partial stop phrases. Documented in nb 12 for tomorrow.
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    -check if openai supports this too or just gooseai
    -implement
-update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    -check which backends support this
    -implement

3/29/22 tues
------------
X -add more params to query_gpt3 and update documentation
~ -finish stop phrase truncation function in jupyter
    X -support truncating on partial stop phrases when completion ended with reason "length"
    ~ -check which backends provide stopping 'reason' (affects which we can rm partial stop words for)
        UPDATE: openai and gooseai do. Haven't confirmed others yet but I doubt they do.
    -refactor functionality out of huggingface mock func
    -figure out how to do this for gooseai (does it need a mockfunc that just ONLY does this? Or should we put this GPTBackend.query and skip it depending on the current backend?)
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    -check if openai supports this too or just gooseai
    -implement
-update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    ~ -check which backends support this
        UPDATE: openai does w/ param n, same as gooseai. Still need to check HF and Hobby.
    -implement

3/30/22 wed
----------~
~ -update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    X -check which backends support this
        UPDATE: gptj still broken but from docs, looks like it does NOT support this natively. 3 others do though.
    -implement
~ -update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    X -check if HF and Hobby (J) backends implement this (openai and gooseai do w/ param n)
        UPDATE: gptj still broken but from docs, looks like it does NOT support this natively. 3 others do (wip hf func includes this, need to remap name from n->num_return_sequences).
    -implement
-stop phrase truncation function
    -refactor functionality out of huggingface mock func
    -figure out how to do this for gooseai (does it need a mockfunc that just ONLY does this? Or should we put this GPTBackend.query and skip it depending on the current backend?)
    UPDATE: realized I need a better understanding of all the diff api response objects first (e.g. which offer finish_reason, what do responses look like when n_completions>1 and/or when n_prompts>1?). These factors will influece how to refactor. 
-allow (or force?) query_gpt3 to make mock functions overwriting other parts of mocked response besides text, e.g. finish_reason and logprobs. (Currently only text is changed.)
X -allow GPTBackend mock_func and engine methods to specify a backend besides current
    UPDATE: realized this was a problem when calling mock funcs directly rather than through backend.query. Now we're sure hf mock func uses right engine based on i.
X -add gptbackend.backends() method for convenience

3/31/22 thurs
-------------
X -add support for banana.dev version of gptj
    UPDATE: will need additional work to support stop phrases, etc. but all the funcs will undergo changes when that happens.
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 AND n_completions > 1
    ~ -consider desired interface. What should be returned when np=1 and nc=1, np>1 and nc>1, np>1 and nc=1, etc.
        UPDATE: wrote some sample usage code in nb12 markdown cell. Still undecided. Considering larger rewrite - the whole mock_func scheme seems kind of annoying. Might be easier to have GPTBackend just choose the right query_func and forget about the "passing a mock func to query_gpt3" paradigm. Also used mark decorator to label which funcs require manual stop phrase truncation - I think that will help refactor that step out.
    -examine responses from various and consider how best to map from these to desired interface
    -implement
~ -stop phrase truncation function
    UPDATE: started noodling on what this could look like. See end of GPTBackend.query method, which uses attr from mark() decorator to do exra truncation if necessary.
    -refactor functionality out of huggingface mock func
    -figure out how to do this for gooseai (does it need a mockfunc that just ONLY does this? Or should we put this GPTBackend.query and skip it depending on the current backend?)

4/1/22 fri
----------
X -add banana backend in aws console
    X -re-generate json and save (remember to save both raw json and generated')
-continue considering query redesign 
    X -settle on removing mock_func construct or not (try to think: is there really a good reason for this?). If yes:
        X -decide what each func should return
            UPDATE: decided to let each function return either (str, dict-like) OR (list[str], list[dict-like]). This allows for my most common use case so far (n_prompts=1, n_completions=1) to continue having a convenient interface w/out extra indexing, while also supporting a straightforward interface for n_completions > 1. Tentatively decided to handle n_prompts > 1 and do that for all funcs using threading/multiprocessing (these completions should be indepedent so no NEED to use the api built-in implementation). Plan is to have a separate GPTBackend.query_multi method where we return a list of tuples. This way we can unpack each completion as (text, resp) rather than the weird zip stuff that forcing this to fit my current interface would entail.
        ~ -start implementing
            UPDATE: tentatively updated each mock_func in nb, but still some cleaning up to do.
        -make sure redesign doesn't break ConvManager or PromptManager too badly (I'm okay with updating these - the api will clearly change too much to be compatible w/ the gui anyway - but make sure stuff like the hooked generator doesn't TOTALLY break in hard to solve ways.) Might want to change how we get kwargs(), i.e. the default query function could change and we should use the active one.
        -add check in GPTBackend.query for if we need to manually trunc stop words

4/2/22 sat
----------
~ -gpt query redesign
    X -finish implementing each mock_func in nb
    X -port to lib
    -update docs for various funcs
    ~ -polish draft of backend query method so it:
        ~ -does the warnings that used to be in query_gpt3
            UPDATE: added some, prob need more.
        X -handles the warning/error if user tries to pass in a list of prompts
        X -does post-query stripping (see gpt3 query func in pycharm, which I removed from current nb version)
        X -does post-query stopword truncation when appropriate
        X -test changes so far
            UPDATE: good so far. Still need to test stream mode.
        -update convmanager (careful w/ hookedgenerator)
            -maybe change kwargs() depending on active backend?
        -update promptmanager 
            -maybe change kwargs() depending on active backend?
X -update truncate_at_first_stop to make full trunc and partial trunc both optional
    X -test

4/3/22 sun
----------
~ -gpt query redesign
    -update docs for various funcs
    ~ -polish draft of backend query method so it:
        -add more warnings that used to be in query_gpt3
        X -write Thread that returns value
            UPDATE: may need to adjust this to ensure order is correct. Also, logging may not be threadsafe? Unsure.
            X -check if I log results with logger or htools.save
                X -check if the used method is threadsafe and/or multiproc-safe
                X -check if the alternative is threadsafe and/or multiproc-safe
                UPDATE: I use htools.save, which is not thread-safe. Logging is, though we couldn't use jq anymore to view pretty results, which is annoying. Can use threads if I use a lock - need to look into these more.
            -update to return in order (may require adjustments depending on bullets above)
        ~ -flesh out streaming mode
            X -think more about desired interface
            ~ -test stream mode
            X -write generic stream func for backends that don't support it
                UPDATE: currently supports np=nc=1 mode and np=1,nc>1. Unsure about others yet.
            X -check how backends w/ streaming support handle np>1 situations if at all
                X -check goose docs
                X -check openai docs
                UPDATE: both seem to support it, though looks like I'll need to try it to confirm structure of response.
                X -run func calls if necessary
                    UPDATE: first tried w/ nc > 1 (maybe unintentional? I forget.). Gooseai seemed to work but openai responses seemed to jumble the tokens without an obvious way to reconstruct each completion. If that's true, 2 of the 3 options with np>1 or nc>1 would be impossible to implement, so I decided to just make it simple and say stream mode is only supported when np=nc=1.
                    UPDATE 2: above is not true after all. They provide res['choices'][0]['index'] which seems to point to which completion the new token belongs to. Gooseai provides this too, they just also return completions in order (e.g. all 0s, then all 1s, etc.).
              X -update backend.query to use new mock stream func
              ~ -write new mock stream func to handle when query return val is (list, list) rather than (str, dict)
                    UPDATE: need to check if it needs to handle empty lists differently and figure out how to integrate into query.backend, but the basic functionality seems to be there.
        -update convmanager (careful w/ hookedgenerator)
            -maybe change kwargs() depending on active backend?
        -update promptmanager 
            -maybe change kwargs() depending on active backend?
        -think about moving hardcoded params to backend.query and leaving most new funcs to accept mostly kwargs?
        -general cleanup of mock_funcs, backend methods, etc. - very messy atm
X -give gpt_repeat func ability to return n>1 completions

4/4/22 mon
----------
~ -gpt query redesign
    X -test if new stream_multi_response in nb handles empty lists okay (or needs to)
    X -integrate into backend.query (replace stream_response? Or select 1 depending on returned type?)
        UPDATE: repeat, huggingface, and banana all work with stream mode now. Confirmed stream mode works w/ banana, repeat (both nc=1 and nc>1), and huggingface (both nc=1 and nc>1).
X -update returnable thread to return query responses in order (or write diff way to do this)
    UPDATE: wrote something to do this if we pass in a dict but turns out it works even without that! Guess join() blocks and ensures we return in order.
    ~ -start implementing query_batch for np > 1 (use ReturningThread).
        UPDATE: So far only tested this on funcs w/ no native streaming mode. (np>1, nc=1, stream=False), (np>1, nc>1, stream=False), and (np>1, nc=1, stream=True) seem good so far. Still need to test (np>1, nc>1, stream=True) - or do we? Didn't I decide that had no forseeable use case?
-update backend.query logging to file to work w/ threads (or whatever method I use in above bullet point)
X -email openai about api key exposure

4/5/22 tues
-----------
X -generate new openai key
X -generate new gooseai key
X -adjust gpt.ls() to not expose api key
X -write some sort of script to check for api key exposure before pushing/committing to git
_ -fix requirements-dev missing jabberwocky version
    UPDATE: realized this is intended. requirements.txt DOES already pin the version.
-qpt query redesign
    -query_batch
        -update backend.query logging to file to work w/ threads (or whatever method I use in above bullet point)
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/6/22 wed
----------
X -add line in readme about using make hooks
    X -add line mentioning diff versions for gui and alexa? Or could wait til I have scripts to handle alexa dev.
-qpt query redesign
    -query_batch
        -update backend.query logging to file to work w/ threads (or whatever method I use in above bullet point)
            X -write JsonlinesFormatter
            X -write JsonlinesLogger
            UPDATE: decided writing custom logger might be good way to go since logging (IIRC) natively supports multithreading. Could have used lock approach but let's try this first. Also like the idea of keeping all calls rather than overwriting each time.
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/7/22 thurs
------------
X -update gptbackend to log using jsonlines logger instead of htools.save
    UPDATE: had to update JsonlinesFormatter to allow logfile name switch, but after that it seems to work well.
-qpt query redesign
    -query_batch
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/8/22 fri
----------
X -easy: clean up all the old messy query funcs in openai_utils.py (mostly deleting)
-qpt query redesign
    ~ -query_batch
        X -in nb13, test if new logging functionality works with multi-threaded approach
            UPDATE: found bug where logging to file fails silently if file DNE. Added `touch` func and check if logfile exists before logging. Also had to add a threading Lock here, which probably means I could switch back to htools.save if I wanted since GPTBackend has a lock assigned now anyway. Would have to update htools to be compatible w/ jsonlines which I think would be good to support generally, and I think this would remove a lot of the icky logic around recreating JsonlinesFormatter every query. Downside: if I want to print kwargs to stdout, that would now need to be a separate step from logging to file. I think this would mostly be useful if I change logic to fully resolve kwargs w/ current query func - right now it just logs the user-specified params.
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/9/22 sat
----------
[NOTE: most up to date version of GPTBackend is in nb13 cell]
_ -update htools.save to support jsonlines format
    UPDATE: looked into options for implementing this and it's not super straightforward with the current interface. Could add jsonlines lib but I'd like to cut down on deps if anything.
-qpt query redesign
    -query_batch
        _ -consider switching to use htools.save in log_query_kwargs? See discussion in yesterday's daily notes.
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    ~ -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        UPDATE: think maybe can just return the native response (note: can't yield from since that makes python treat the method as a generator and all responses are lazily evaluated, even in stream=False mode).
        ~ -implement
            UPDATE: very simple for reasons described above and need to test more, but I think this should work.
        -build default support for nc>1 for backends that don't provide it
            UPDATE: solution is to make all of those backend query funcs return (str, dict) and not provide n in params. Then use new threaded_starmap function to make nc calls. Tentatively seems to work, though I'm surprised - query_batch also uses threads and I read threads within threads are not possible. UPDATE 2: actually might have spoken too soon. Investigate tomorrow.
X -fix new threading bug after deleting files
    UPDATE: realized I needed to move lock to include a bit more logic. Might just want to lock the whole method if another bug pops up in the future, but seems ok for now.

4/10/22 sun
-----------
-query_batch
    X -investigate: does np>1 and nc>1 actually work for funcs w/out native support? Unclear. See cell 129 in nb13.
        UPDATE: unable to reproduce error. Seems fine.
    ~ -adjust interface so returned val has same format for backends w/ native support for np or nc > 1 and those without (realized gooseai and maybe openai charge by the request too, so it's more efficient if we can use the native functionality when possible instead of always using multithreaded approach).
        UPDATE: WIP. Adjusted query_batch to return (texts, fulls) instead of old behavior so we now match openai/gooseai. Still need better testing.
    ~ -add prompt_index (decided rather than overriding openai/gooseai index to create prompt_index and completion_index, just keep index (which is effectively "overall_index") and add prompt_index.)
        UPDATE: seems okay for some cases but still very buggy. See query(str, n=1).
    -write tests in new clean nb to figure out which backends/kwargs combos work (so many variants, need to do programmatically)
        -start with all free backends to root out simplest bugs
        -then do paid backends
            X -write script to save sample response from all relevant combinations of params
                UPDATE: did this for gooseai. Shouldn't need to for openai since it's nearly identical (only known difference I recall is that in streaming mode, openai may shuffle completions together, which I account for elsewhere).
    -figure out how to integrate into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
        -implement

4/11/22 mon
-----------
-query_batch
    X -start new clean nb and use pickled responses in data/misc instead of adhoc
    _ -try passing each pickled response through postprocess_gpt_response
    _ -try passing each of those ^ outputs through postprocess_response
        UPDATE: wrote new query_gpt_mock func that makes it easier to get fake openai/gooseai responses.
    -add prompt_index to all gooseai/openai responses
    ~ -write tests in new clean nb to figure out which backends/kwargs combos work (so many variants, need to do programmatically)
        X -start with all free backends to root out simplest bugs
        -then do paid backends using pickled responses
    X -figure out how to integrate into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
        ~ -implement
        UPDATE: query() calls _query_batch under the hood and _query_batch calls query n times (str prompt each time instead of list).
X -fix bug in stream_response
    UPDATE: itertools.cycle was returning the same dict repeatedly. We want it to return a new dict at each step.
X -kwarg_grid func to iterate over all relevant kwarg combos

4/12/22 tues
------------
~ -look into issue where logfile is being generated in notebooks dir
    UPDATE: could not reproduce.
X -add prompt_index when stream=True
    X -figure out what the issue is
        UPDATE: prompt_index was only assigned in query method, but we return early when stream=True. Needed to add logic to stream_multi_response().
    X -implement
~ -add support for backends w/ builtin batch queries? (Recall: at least w/ gooseai, it's cheaper to make 1 query w/ 100 prompts than 100 queries w/ 1 prompt each)
    UPDATE: fiddled around a bit and it seems like maybe query should work anyway? Probably easier to diagnose by just trying it.
~ -[BLOCKED: need to figure out if the above bullet is necessary first. Not sure if we can proceed with this yet.] test_backend('mock') and examine results
    UPDATE: initially failed on first set of kwargs so I switched kwarg grid func to do them in an easier order. Bug turned out to be I was checking if n the number rather than 'n' the param name was in query signature. Now they all execute BUT doesn't work correctly when np > 1. Seems to repeat completions - I sense it's related to the fact that query_batch is still being used on it. So we're making 2 calls to query_mock w/ n=1 rather than 1 call to it with n=2, which is why we get the results for the n=1 response. Fix this tomorrow.

4/13/22 wed
-----------
X -update query to only use query_batch if the backend doesn't natively support list prompts (or update query_batch to handle that itself)
~ -make sure prompt_index is correct when stream=True
    X -works for backend does NOT support stream natively case
        UPDATE: added assert in nb test_backend() func to check this. Repeat and banana pass.
    X -works for backend DOES support stream natively case
        UPDATE: wrote new stream_openai_generator func. All tests pass.
    -test huggingface backend
        UPDATE: tried to run but unsurprisingly got 400 error. Maybe if I manually insert wait times or alternate between engine_i values it would work? Try again later.
    -test gptj backend (if back up)
        X -check if back up
        -[BLOCKED: api down] test
-clean up query/query_batch so prompt_index isn't set in so many places (currently 3? end of query, end of query_batch, stream_mluti_response)

4/14/22 thurs
-------------
X -notebooks/data/logs is back. Want to stop this from happening.
    X -Try to reproduce error and identify cause
    X -fix
    UPDATE: because I made jsonlogger a class attribute, the path was resolved before I ran cd_root, i.e. on import. Added C.root var in config. Used this to update load_prompt func and PromptManager so paths are a little less fragile (arguably).
X -investigate what happens when 1 returningthread fails. Think it might not raise the error to the calling context.
    X -try w/ toy example (no query gpt)
    UPDATE: simply returns None in the corresponding list index. Switched thread_starmap to use PropagatingThread by default, though ReturningThread is still an option.
X -think about moving hardcoded params to backend.query and leaving most new funcs to accept mostly kwargs?
    UPDATE: No. Wrote explanation in query_gpt3 docstring.
X -general cleanup of mock_funcs, backend methods, etc. - very messy atm
    X -better document what these funcs need to return
    UPDATE: wrote pretty thorough section on how to implement a new backend func in the module docstring.
-clean up query/query_batch so prompt_index isn't set in so many places (currently 3? end of query, end of query_batch, stream_mluti_response)
_ -check if there are any more warnings from query_gpt3 that need to be moved to gpt.query()

4/15/22 fri
-----------
X -clean up query/query_batch so prompt_index isn't set in so many places (currently 3? end of query, end of query_batch, stream_mluti_response)
    UPDATE: technically it was 4 before and I got it down to 3 (removed query_batch one). Also refactored stream_response (formerly stream_multi_response) to include stream_openai_generator (the 4th case I previously forgot) so it's sort of like 2 places.
-repercussions of gpt.query redesign
    -update convmanager (careful w/ hookedgenerator)
        -maybe change kwargs() depending on active backend?
-repercussions of gpt.query redesign
    -update convmanager (careful w/ hookedgenerator)
        -maybe change kwargs() depending on active backend?
    -update promptmanager 
        -maybe change kwargs() depending on active backend?
X -fix docs in stream_openai_generator explaining how calculation works

4/16/22 sat
-----------
X -clean up misc todos in alexa/app.py (rm old commented out code, etc.)
    X -update app to use banana.dev by default for now
X -update engine names in config
    X -add banana.dev engine names
    X -replace hobby empty strings with engine names
X -document alexa.utils.slot function
X -document a couple CustomAsk methods
-make logging use fully resolved kwargs?
-handle error when wiki person generation fails
-repercussions of gpt.query redesign
    -update convmanager (careful w/ hookedgenerator)
        -maybe change kwargs() method depending on active backend?
    -update promptmanager 
        -maybe change kwargs() method depending on active backend?

4/17/22 sun
-----------
X -add datetime to log meta
X -make logging use fully resolved kwargs?
    UPDATE: investigated this and decided this is no different - individual query_func defaults are basically all overridden by gpt.query defaults.
X -update convmanager and promptmanager kwargs()
    UPDATE: rm refs to mock_func, use GPTBackend.query for bound args. Also added keep=True to with_signature deco so kwargs that are not in query_gpt3 are accepted in bound_args().
-handle error when wiki person generation fails
X -repercussions of gpt.query redesign
    X -update convmanager (careful w/ hookedgenerator)
        _ -maybe change kwargs() method depending on active backend?
    X -update promptmanager 
        _ -maybe change kwargs() method depending on active backend?
~ -test convmanager
    UPDATE: after some fixes to handle new list outputs, seems to work. One remaining issue: confirmed my suspicion that stream mode does not respect my stop_words.

4/18/22 mon
-----------
X -test promptmanager
    UPDATE: seems fine.
~ -handle error when wiki person generation fails
    UPDATE: the error actually does seem to be handled. The problem is if we try to add a new contact and fail, and then try to add another new contact that ALSO does not exist. If we add an existing contact or a new contact that DOES exist, we seem to be okay. Possible fix: I updated _generate_person to use _maybe_choose_persona(), which it was previously missing. Haven't tested this yet.
X -shorten conversation default max_tokens a bit
    UPDATE: dropped this down from 250 to 100 in prompt config - unsure if this will really help or not. I was annoyed at responses getting cut off and I suppose this might actually make it worse - there doesn't seem to be any ability to ask for gpt to actually generate a shorter response, we can just interrupt it sooner.
X -fix bug where choose_person no longer checks response kwarg
    UPDATE: this was that case in delegates where I forgot what it was doing for a while. I think any func that gets called with func_push needs to accept either kwargs or a "response" arg and I'd removed it from choose_person when building out the intent inference system.
-make stream mode respect stop words

4/19/22 tues
------------
X -test possible solution to wiki person generation (recall that trying to generate 2 people without wiki pages in a row was causing an error, but I THINK I fixed this).
    UPDATE: confirmed this works. Also updated queueing interface to push kwargs directly into the same queue - this way, we don't have to worry about forgetting to reset state.kwargs (previously, I was seeing that the generate_person attempt after a failed generation used the same name as the previous call because I wasn't resetting kwargs). Removed state.kwargs entirely.
X -generate new single file prompts from my current 2 file setup
    UPDATE: func to do so is in ipython - might want to save that somewhere. Still need to figure out what to do with old prompt dirs (keep around somewhere for legacy gui) and update load_prompt to use new cfg files.
-make stream mode respect stop words

4/20/22 wed
-----------
X -port yaml generation func to lib/scripts somewhere
X -consider best way to store new prompts (maybe should have a separate ~/prompts dir and repo entirely?)
    UPDATE: eventually, a ~/.jabberwocky or ~/.prompts would probably be a good idea, but for now let's keep everything in 1 repo. Old format prompts are now in data/legacy_prompts, new single-file prompts are in data/prompts.
X -mv either old or new prompts to a new dir (i.e. 1 dir should hold only legacy 2-file prompts, another should hold only new format 1-file prompts)
X -update load_prompt to use new single-file format
    UPDATE: supports both formats (and infers v1 vs v2 format automatically). Also supports different prompt dirs in case I decide to move this eventually.
-resolve 2 utterance conflicts (changePerson apparently soaking up 2 unintended commands in Hush and Oh No)

4/21/22 thurs
-------------
_ -rewrite engine mappings to map from i to "nearest" match? I.e. engine_i=3 for huggingface is currently more like engine_i=1 (or something lower than 3, anyway) for openai. Or maybe support passing in either a name OR a number?
	UPDATE: decided I need a better understanding of the available engines and api endpoints before settling on a method.
~ -read up on newly released engines, maybe take some notes
	UPDATE: took notes in misc.txt. Still have more reading to do on classification, maybe others.
~ -function to upload files to openai (used for search, q&a, classification)
	UPDATE: needs some tweaks to support passing in metadata, labels.
X -write random_str func and add to htools
	UPDATE: haven't uploaded to pypi yet though. This was a used in my upload_openai_files func.
X -skim post on gpt instruct
	UPDATE: sounds like you don't need to use it that differently, the results should just be a bit better for instruct-style prompts.
-resolve 2 utterance conflicts (changePerson apparently soaking up 2 unintended commands in Hush and Oh No)
-update jabberwocky version and upload to pypi
-Harder option: make stream mode respect stop words

4/22/22 fri
-----------
X -finish upload_openai_files func
	X -support labels for clf mode
	X -support metadata for all modes
~ -finish reading clf docs re using logprobs
	UPDATE: read a bit more but I'm finding the classification endpoint kind of puzzling, and I don't have a pressing need for it. Cut this short.
~ -finish reading any other docs that seem relevant in near term
~ -revisit question of how to specify engines in prompt config files (note: some tasks require multiple engines ðŸ˜±. This may require some changes. I guess we could have a diff format for completion prompts vs. search prompts, for instance.)
	UPDATE: openai is up to 50 models now and will probably continue to grow (though it's basically still 4 for the instruct use case), so specifying engines as ints may be hard to maintain. Also: looked at gooseai pricing again since that's the only one besides openai that is fairly reliable (over, say, 80% uptime) and has multiple engines. And idk if they changed prices or I'm screwing up the math somehow but it seems like it is often more expensive (sometimes 10x!?) than openai. Maybe the answer is just say forget all the other backends and just use openai. But anyway, I think it's clear we should be passing in engine rather than engine_i, perhaps with some extra logic to handle a few special ints.
X -mark some old todos as "NOFIX"

4/23/22 sat
-----------
X -easy: write docs (including updating returns section) for gpt._query_batch method
X -rename np vars in openai_utils to np_ to avoid collision w/ numpy
~ -look into codex "invalid url" error (see ipython pane below)
	UPDATES: curl works, python doesn't.
	Not just codex, now now openai engines work w/ python. Maybe due to updating pip package? Temporarily upped billing limit to try other models.
	Restarted kernel and gpt query works again w/ ada. BUT after I import openai explicitly, that fails too. That must be a clue.
	Tried uninstalling, reinstalling, opened new tmux pane. Still same error.
	Tried deleting 'openai' object and then importing jabberwocky. This does work!?
	If I re-import openai after that, gpt.query still works. But openai.completion with codex does not.
	If I import openai FROM jabberwocky openai_utils, codex query still fails. But gpt.query works. And openai.Completion works w/ engine ada!
	Conclusion: maybe it is codex-specific then?
-resolve 2 utterance conflicts (changePerson apparently soaking up 2 unintended commands in Hush and Oh No)

4/24/22 sun
-----------
X -continue investigating inability to query any openai engine (invalid URL error) after importing openai explicitly
	X -try uninstall and reinstall old version from lib/requirements.txt
	UPDATE: Could not reproduce. Everything seems to work now though /shrug.
X -resolve 2 utterance conflicts (changePerson apparently soaking up 2 unintended commands in Hush and Oh No)
	UPDATE: two collisions are "oh no" and "hush", which are both mistakenly mapped to changePerson rather than built in intents (StopIntent and NoIntent.) Couldn't figure out a godo way to stop this in the UI but I updated choosePerson endpoint to handle it (either use as a response when a conv is in progress or reroute to choosing a person if not).
X -fix prompt loading bug in PromptManager (caused by switching from prompt dirs to single files)
X -write some thoughts on dropping partial sentences due to max length
	UPDATE: see bullets in backlog.

4/25/22 mon
-----------
~ -make stream mode respect stop words
	X -look into pickled stream response format - is it really 1 token per item? How often are common words split up?
		UPDATE: yeah, 1 token.
	X -use gpt tokenizer to see if I can identify a more reliable single token stop word, consistent for all prompts (i.e. let us put some STOP_TOKEN in every prompt and not have to dynamically update that param)
		UPDATE: 1 token prompt is risky in case we accidentally generate it. Uncommon tokens aren't ideal either - mostly random things like "[space]informants". "<|endoftext|>" might work though...I wonder if gooseai would know to omit that. Probably not.
	~ -start prototyping function
		UPDATE: see stream_with_stop func in nb15. Works decently for np=1, nc=1, 1 hardcoded stopword case. May be a bit tricky to generalize to all cases and integrate into existing codebase.
X -fix bug in openai stream mode
	UPDATE: turned out I haven't been passing stream=True to postprocess_gpt_response func in query_gpt3. No clue how those pickled responses work.
X -fix bug #2 in query_gpt3
	UPDATE: turns out zipping a generator with itself was too good to be true. It ends up doing something weird, as documented in the comment above my fix.
X -easier option: email gooseai asking about claims of being cheaper than openai
-try to reduce amount of repetition in gpt responses
    -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    -try tweaking prompt a bit.
    -try with better model (maybe this task just really needs davinci?)
    -try one of the "instruct" models
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary

4/26/22 tues
------------
-stream_with_stop func
	X -update nb15 stream_with_stop func to use <|endoftext|> as stopword (this conveniently gets tokenized as 1 token. While I suspect gooseai will still return the token itself, it should actually stop it from generating more tokens, saving us money.).
	-determine if it needs to support np>1 and/or nc>1
	-determine how to integrate into gpt.query
	-determine how/whether to support more stopwords - can we feasibly update prompts to make every task use the same stopword?
		UPDATE: most few shot prompts would support this no problem. Zero shot prompts probably won't, though they should lean less heavily on stop words anywa.
~ -write func to compute gooseai cost vs. openai cost, given a prompt of known length and an output of max_tokens length (realized gooseai's claim is still plausible given that they don't charge for input tokens, but it's not intuitive in a given situation which backend will be cheaper)
	UPDATE: decided it would help to do this before deciding how much time/effort to devote into stop word removal in stream mode, since that really only helps gooseai. Wrote basic function that doesn't yet support passing in an engine and doesn't make an explicit recommendation of the better choice, but it does return a df with engines/backends sorted by cost.
X -rm use of conversation_formatter func (no longer ideal to use that prompt this way) from openai_utils promptmanager etc.

4/27/22 wed
-----------
-cost estimator func
	~ -support openai names like ada AND 'text-ada-001'
	~ -support passing in engine_i int
		UPDATE: tried a few different approaches and ended up writing a new EngineMap class to try to make this simpler. Almost satisfied but still a bit puzzled as to how to handle names like 'code-ada-001'. Deal with this tomorrow.
	-add sugestion of which backend/engine to use
	-come up with some helpful visualizations/metrics to begin to understand results
		-plot heatmap/3d scatter (something w/ meshgrid, probably) w/ input len and output len as x and y axes and price as color/z dim (prob need to pick single engine "size" to compare? Maybe 1 plot for each "pair".)
		-similar to above bullet but something heatmap-like where each box is 1 of 2 colors, just shows which backend was cheaper? Have to think about how to implement.
-update config backend engine map to align w/ openai idx (i.e. huggingface now has '' for i=2 and 3); need to update other files to reflect this

4/28/22 thurs
-------------
X -finish EngineMap
	X -write down desired input-output pairs (given an engine name and backend, what should output be? Focus on cases like 'code-ada-001'.
	X -based on input-output pairs, implement changes (latest version is in pychar, not jupyter)
X -integrate EngineMap into rest of openai_utils and alexa and anywhere else in project that refs it (recall now some engine() callscould return None)
-cost estimator func
	-integrate EngineMap
	-support openai names like ada AND 'text-ada-001'
	-support passing in engine_i int
	-add sugestion of which backend/engine to use

4/29/22 fri
-----------
X -cost estimator func
	X -integrate EngineMap into estimate_cost func for engine resolution
	X -support option for output name to be base only (e.g. 'ada' rather than 'text-ada'001')
        UPDATE: realized this only makes sense in openai backend case since gooseai names don't contain any of these. Added basify option for when backend is openai.
	X -add suggestion of which backend/engine to use
        UPDATE: now returns dict where everything corresponds to cheapest option, plus a 'full' key where value is df of all results.
X -cost visualizations
	X -come up with some helpful visualizations/metrics to begin to understand results
        X -plot heatmap/3d scatter (something w/ meshgrid, probably) w/ input len and output len as x and y axes and price as color/z dim (prob need to pick single engine "size" to compare? Maybe 1 plot for each "pair".)
		_ -similar to above bullet but something heatmap-like where each box is 1 of 2 colors, just shows which backend was cheaper? Have to think about how to implement.
        X -histogram of diffs between equivalent engines

4/30/22 sat
-----------
X -integrate estimate_cost func into EngineMap? (prices as class vars, func as a classmethod?)
-port iter_engine_names and iter_paid_engines
    -document
_ -consider porting visualization funcs? Maybe not.
    UPDATE: I did do some more plots and cost estimation for conv-specific prompts though. Seems like I start using much longer prompts (which is highly possible if I work on a NLCA style agent in the future), it's fine to stick with openai for the most part.
~ -add auto-backend select mode to gpt? (We usually don't know prompt length so we might need to tokenize, which could slow things down a bit. But actually gpt tokenizer isn't that slow, IIRC.)
    UPDATE: not yet, but did add a dev_mode option to print out estimated costs for now.

5/1/22 sun
-----------
X -port iter_engine_names and iter_paid_engines
    X -document
X -try out dev_mode a bit (debug, also build intuition)
~ -add auto-backend select mode to gpt? (We usually don't know prompt length so we might need to tokenize, which could slow things down a bit. But actually gpt tokenizer isn't that slow, IIRC.)
    UPDATE: a bit tricky. Converted gpt.query and query_batch to instance methods and made GPTBackend instance part of the module automatically so we have access to the contextmanager in query. Then started implementing optimized() contextmanager. Still needs work.
-continue stream_with_stop func
    -consider: is this still necessary given my findings that gooseai isn't really saving money in most cases?
	-determine if it needs to support np>1 and/or nc>1
	-determine how to integrate into gpt.query
	-how to deal w/ zero shot prompts that can't easily incorporate <|endoftext|>
	-test my <|endoftext|> stopword in gooseai ui - need to make sure this is even a valid strategy

5/2/22 mon
----------
X -finish optimized() contextmanager to auto-optimize query cost
    X -consider interface more: should this instead RETURN a ctx manager (either self(best_backend) or dummy()) and then run the rest of the query in that? Or should it actually allow us to enter the manager itself (as I have with the current implementation)?
        X -key consideration: how to avoid calling this on every query call in query_batch case? Previous integration into query method achieved this by setting optimize_cost=True, but with my current implementation that var is obscured inside the optimized() method so I don't think that works.
    X -briefly test out new functionality (2 changes: separate query/_query methods and optimize_cost option).
    UPDATE: separated functionality into _query and query methods, where the latter is the only one to possess an optimize_cost arg.
X -bug(ish) fix: replace all old cls refs w/ self in query since its no longer a classmethod
-alternate option: try to reduce amount of repetition in gpt responses
    -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    -try tweaking prompt a bit.
    -try with better model (maybe this task just really needs davinci?)
    -try one of the "instruct" models

5/3/22 tues
-----------
-continue stream_with_stop func
    ~ -consider: is this still necessary given my findings that gooseai isn't really saving money in most cases?
        UPDATE: marginally useful and strongly considered skipping, but this felt like an excuse to avoid having to get it working. Ended up writing StopWordStreamer class using trie that supports multiple stop words and seems to work pretty well. Still needs some work to integrate it into lib though.
	-determine if it needs to support np>1 and/or nc>1
	-determine how to integrate into gpt.query
	~ -how to deal w/ zero shot prompts that can't easily incorporate <|endoftext|>
        UPDATE: seems like we can't rely ONLY on endoftext as stopword. If we're really going to support stop words for streaming, we need to allow any stopwords.
	~ -test my <|endoftext|> stopword in gooseai ui - need to make sure this is even a valid strategy
        UPDATE: tried briefly and it seemed ok in the cases I tried, but in the openai forums an openai employee recommended against using this (David Shapiro was actually the asker lol). They said the instruct models should be pretty good at this already. But I suppose I'm thinking more about the gooseai case, which doesn't yet have instruct-tuned models.
-fun option: support custom persona (speak bio yourself) in alexa
-yet another alternate option: try to reduce amount of repetition in gpt responses
    -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    -try tweaking prompt a bit.
    -try with better model (maybe this task just really needs davinci?)
    -try one of the "instruct" models

5/5/22 wed
----------
-continue StopWordStreamer class
    X -does this need to implement its own logic about index/prompt_index/finish_reason? Or is it possible to sneak this into an existing function?
        UPDATE: we either need to add it here or integrate it into another func. This interface seems more promising tbh so I'm thinking we can try to add it here.
    X -determine if it needs to support np>1 and/or nc>1
        UPDATE: yes, just pay attention to index (NOT prompt_index - the decision to stop should affect a single completion, not all completions for a prompt). Threading is used when we have np > 1 but that's irrelevant here.
    X -need to make stream method accept a (text, full) tuple?
        UPDATE: yes, we'd be streaming from a generator of (text, full) tuples.
    X -update finish_reason
    X -accept (text, full) tuple
    X -add support for nc > 1
    -integrate into gpt.query (replace/build into one of the stream funcs?)

5/5/22 thurs
------------
-StopWordStreamer
    X -consider which interface I like: create streamer once and reset each time, or create new streamer for each completion. Depends where I envision making/storing this obj - if it's in GPT (which can use any prompt), we may want to create new obj each time since stop words are diff for each prompt.
        UPDATE: decided to revert to pattern of new object for each query, since stopwords can vary not just by prompt but also by query (see extra_kwargs).
    X -fix bug: doesn't work w/ multiple stopwords of diff lengths
        UPDATE: quite a difficult bug but I think I got it. Updated htools.TrieNode to track its own depth AND allow passing a new TrieClass to htools.Trie. Then make the streaming method return an int instead of bool, telling us how many tokens the found stop word was.
    ~ -integrate into gpt.query (replace/build into one of the stream funcs?)
        UPDATE: I think the easiest/safest thing to do would be keep this just for the gooseai case. Useless and potentially buggy for openai, and the rest of stream_response does a bunch of weird logic to assign indices which gooseai doesn't need. Just keep it as a subcase of real_stream I think. Also: I'm realizing I could potentially add this as an intermediate step before calling stream_openai_response, i.e. yield from stream_openai_response(gen=streamer.stream(gen)) but it's unclear if that would be good. Have to consider.
X -write wrapper func to avoid directly dealing w/ stopwordstreamer
    UPDATE: written but not yet ported or documented.
-fun option: support custom persona (speak bio yourself) in alexa

5/6/22 fri
----------
~ -finish stopword streaming functionality
    X -consider: should index handling occur here? Or reuse stream_openai's functionality (see yesterday's notes on integration).
        UPDATE: set index in _stream_gooseai_generator(). Sufficiently different from stream_openai that I don't want to rely on that - cleaner to just create new stream gen for each class of backend behavior.
    X -support np>1 and nc>1 scenario
        UPDATE: realized streamer can only act on 1 prompt unless we even more deeply nest self.q/self.done/etc. Updated stream_gooseai to create np streamers and call them a step at a time. This required refactoring stream() into something that can be called step by step. Also fixed bug with full['index']: need to modulo nc when there are multiple prompts, even though each streamer only deals with 1, because of how openai assigns indices.
    -port stopword streamer cls
    -port stream_gooseai func
    -add as option to utils.stream_response
    -update call in GPT.query
    -update warning messages in GPT.query? Now some stream modes support stop words.
    -maybe refactor rest of stream_response into stream_static() and make stream_response just delegate to diff generators (which it will already mostly be doing)? Could also potentially make this FAR simpler since these inputs aren't actually generators.
-integrate into gpt.query (replace/build into one of the stream funcs?)
-fun option: support custom persona (speak bio yourself) in alexa

5/7/22 sat
----------
X -finish support for stopword filtering in streaming mode
    X -port stopword streamer cls
        X -document
        X -add constructor for from_trie
        X -add constructor for from_stopwords
    X -port stream_gooseai func
    X -add as option to utils.stream_response
    X -update call in GPT.query
    X -update warning messages in GPT.query? Now some stream modes support stop words.
    X -maybe refactor rest of stream_response into stream_static() and make stream_response just delegate to diff generators (which it will already mostly be doing)? Could also potentially make this FAR simpler since these inputs aren't actually generators.
    UPDATE: implemented as planned but untested - prob a bunch of small bugs to iron out. Do that tomorrow.
X -give gptbackend a tokenizer attr to pass to _stream_gooseai_generator
-fun option: support custom persona (speak bio yourself) in alexa

5/8/22 sun
----------
X -refactor method call in gpt.query so call only occurs once instead of repeating with _query and _query_batch.
~ -finish stopword streamer integration
    X -confirm: is the stop I pass to stream_response from gpt._query() fully resolved? Should it be? 
        UPDATE: no, but query funcs don't have default stop values so there's no difference.
    X -confirm: what happens if I pass empty list to StopWordStreamer?
        X -what about if I pass None? There could plausibly be no stopwords, but I guess gpt._query() call uses empty list as backup val.
        UPDATE: added check to error out if we try to create a streamer with no stopwords - no point. Just hardcoded if/else in stream gooseai generator. Considered making a dummy mode for stopword streamer for when no stopwords are present but it seemed kind of pointless.
    ~ -test new stopword streaming functionality for each backend
        -openai
        -gooseai
        X -hf
        X -hobby
        X -mock
        X -repeat
X -rewrite pretty_tokenize to avoid bug with special characters
    X -update stream_gooseai gen to be compatible
    X -update gpt.query
    X -update Stopwordstreamer
    X -remove tokenizer loading from gpt
X -try removing jupyter files from github language calculations (may need to wait for github to reindex it before changes show up)
-maybe apply static stopword truncation func to each text response in stream_fake_generator. We actually do have the full strings upfront in this case so that should be very doable.


5/9/22 mon
-----------
X -add subwords option to gpt.query (or somewhere?) so users can choose between streaming words and subwords for static backends.
_ -update static stopword truncation func to use ByteLevel() like in pretty_tokenize? (looks like HF, for one, might actually try to truncate before the stopword but it only removes the last 1 token, which doesn't do what we want if stopword is multiple tokens - at least that's what it looks like).
    UPDATE: After some testing, I think my partial_trunc option already takes care of this satisfactory degree.
X -fix bug in stopwordstreamer (wasn't truncating as expected on a new sample case)
    UPDATE: Added strip_spaces option to deal with this - turned out the issue was I specified a stopword with a trailing space (to try to minimize the chance of unintentional truncation) but spaces are typically assigned to the next token. A more thorough solution might be to completely rewrite streamer trie to be character level but I think this solution works well enough, especially since specifying stopwords with trailing spaces seems difficult with yaml file.)
X -maybe apply static stopword truncation func to each text response in stream_fake_generator. We actually do have the full strings upfront in this case so that should be very doable.
-test new stopword streaming functionality for paid backends
    -openai
    -gooseai
-test backends on other cases
    -np > 1
    -nc > 1
    -nc > 1 AND np > 1

5/10/22 tues
------------
X -consider how to avoid ending a reply mid-sentence? (Ideally api would do this automatically but it doesn't seem to)
	X -option 1: update postprocessing to check for cutoff sentences and leave off the end bit. Maybe simple rule, maybe nltk/spacy - depends what I want to use this for. Some completions may be valid when not ending in a period/full sentence, others may not be.
        UPDATE: decided to just base this off punctuation and disable it by default, along with some other limitations on when it's used (see misc.txt for today).
	~ -option 2: try prompting gpt3 to generate a specific number of sentences/words, and/or only generate full sentences. (Basically, making it "aware" of its own hyperparameters.)
        UPDATE: still like the idea of prompt mixins, but found this particular one didn't seem all that effective, at least with my initial phrasing/settings.
	_ -option 3: additional model (could be gpt) to trim off any partial sentences.
        UPDATE: probably effective but don't want to slow things down any further with additional queries - we're already pushing it time-wise.
-alternate option: try to reduce amount of repetition in gpt responses
    -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    -try tweaking prompt a bit.
    -try with better model (maybe this task just really needs davinci?)
~ -briefly try out my NL unit test idea and some other misc ideas in openai playround
    UPDATE: playground does not support the logit_bias arg, which I think is key here. Not perfect but there's some promise. Not sure if was just the prmopts I happened to try today but I was getting WAY better results with openai than gooseai.

5/11/22 wed
-----------
X -try a few short calls in aws console - got sidetracked a bit with library dev tasks so it's been a while, and I want to make sure everything still works
    X -assess repetition problems with different backends/engines
    UPDATE: also found and removed old bug where change_model changed nonexistant "model_i" param (now "engine"). Also updated error message to use "model" instead of "backend".
~ -try to reduce amount of repetition in gpt responses
    X -try with better model (maybe this task just really needs davinci?)
    X -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    _ -try tweaking prompt a bit.
    UPDATE: Was seeing a lot of repetition in alexa responses but less in console. Tried upping frequency_penalty anyway (NLCA book recommends .5-1, while openai playground default is 0. I was using 0.1 and moved it to 0.5 now.) Also experimented with bio cleanup prompt (sometimes my regex-based parsing doesn't work perfectly). Had some success in playground but it's also not perfect.
X -add new bio cleanup prompt
    UPDATE: "wiki_bio_cleanup". Also added a "social_hypotheses" prompt.
X -start prototyping func to support remote prompt loading (would be good if jabberwocky was less wedded to local files)

5/12/22 thurs
-------------
X -add support for remote prompts
    X -select desired interface (maybe one func where you can either pass a local prompt dir or a url fmt)
        UPDATE: load local by default, but if url is provided that takes priority.
    X -finish implementing (see load_remote_prompt draft in ipython below)
-incorporate new bio cleanup prompt into convmanager
~ -add support for "default" section in load_prompt (e.g. if we have a list prompt of n items, we might want to let the user adjust n but fallback to a default of 3)
	UPDATE: seems like best way to do this would be by writing a Prompt class that can store defaults and adding custom logic in the format method. Made a lot of progress on this but prob still have some work to do.
-add "docs" arg for prompt yaml files containing more details on what they do?
-review old notes and online sources re adding email integration. 

5/13/22 fri
-----------
~ -finish Prompt class (see data/tmp/prompt.py - maybe good to move to nb now. Getting annoying scrolling through ipython cells.)
	X -test adding default args in prompt
	X -add ability to store more kwargs (e.g. engine, max_tokens, temperature)
	-try incorporating into a prototype version of load_prompt
	UPDATE: for now, added classmethod to Prompt which loads yaml using load_prompt. This is a little weird so might end up changing it later.
	-if interface seems promising, maybe update prompt_manager/conv_manager etc. to use this instead of storing dicts
	X -potential extensions: specify unit tests (natural language or otherwise)
		UPDATE: added postprocessors and validators, 2 lists of functions to apply after querying.

5/14/22 sat
-----------
X -move tmp.py elsewhere and adjust prompt tmp.yaml accordingly
X -more updates to Prompt class (see data/tmp/prompt.py - maybe good to move to nb now. Getting annoying scrolling through ipython cells.)
	X -validate that if defaults are provided, they are a subset of the available fields
	X -allow passing in *args or **kwargs to resolve. Passing in a dict is a little clunky.
	X -refactor resolve() to use Signature and bind()
	-consider: should postprocessors and validators be called automatically after a query? (Either for all queries in GPT or in prompt/conv manager(s)?)
-if I like Prompt interface, update load_prompt/prompt_manager/conv_manager etc. to use this instead of storing dicts

5/15/22 sun
-----------
~ -more prompt class updates
	_ -consider: should postprocessors and validators be called automatically after a query? (Either for all queries in GPT or in prompt/conv manager(s)?)
		_ -should validator raise an error when broken or return false? Should it return true or the completion str when it passes?
	UPDATE: made some updates but there are still a lot of issues to work out which will take a while, and because there's no immediate use case, I made the call to postpone this. I'm getting sidetracked with all these openai library wrapper changes which are out of scope of the original alexa project plan.
_ -if I like Prompt interface, update load_prompt/prompt_manager/conv_manager etc. to use this instead of storing dicts
-alternate option: review old notes and online sources re adding email integration. 
	-get consent_card working on use_email endpoint (then refactor to have this automatically done on skill launch?)
X -move tmp.yaml to a new experimental prompts dir (don't want unsupported fields causing any surprise errors)

5/16/22 mon
-----------
[THEME: return focus to alexa and start to think about how to wrap up skill v1. Enough getting sidetracked on (admittedly interesting) openai wrapper stuff.]
~ -review old notes and online sources re adding email integration. 
	-get consent_card working on use_email endpoint (then refactor to have this automatically done on skill launch?)
	UPDATE: tried again but still get 403 error (basically unauthorized to access info) when trying to retrieve user email. Also tried voice consent again but still get that weird mystery "audio only" blip response and then the app exits. Enabling permissions on that amazon website seems to briefly work (sometimes?) but it gets reset very easily - maybe after a few minutes or when one session ends?
X -make default option in alexa conv to drop last fragment
X -fix bug where auto punct was performed regardless of setting
-alternate: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

6/17/22 tues
------------
X -find gmail alternative for sending emails (less secure app access option expires 5/30/22, and even without that it's proving difficult to disable 2FA for just the jabberwocky account)
	UPDATE: outlook seems to work fine.
~ -try adding account linking instead of user permissions method
	UPDATE: found good tutorial (link in misc.txt) but I don't think it ended up being used bc for some reason now the regular permissions are still working (recall in the past they would frequently get turned off by amazon for some reason).
~ -allow user to add api key (openai/gooseai etc.) (Note: maybe this would be an alternate way to get user name/email? Allow them to type it in in app?)
	UPDATE: did more digging on this and it sounds like there's not a good way to do this natively. Only options are to have the user read it 1 character at a time (very error prone, though maybe doable with some custom logic in the app) or create a site where user can enter it in.
-alternate: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/18/22 wed
-----------
~ -see if I can access polly.ai voices
	X -change speaker gender based on current persona
	UPDATE: currently just uses 1 pre-selected voice for each gender, but I just 3 voice options for gender. Need to extract/infer nationality for each speaker for this to work.
	-add emotion markup (start w/ hardcoded)
	-prototype prompt: gpt3 infers emotion tags based on response text
-revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/19/22 thurs
-------------
~ -polly.ai voices
	X -see if alexa lets us check which default voice is currently in use (thinking we could select 1 paid voice and 1 free one, i.e. use default alexa voice for women and polly for men or vice versa)
		UPDATE: couldn't find any way to get this info from alexa.
	~ -support both neural and standard modes in alexa.utils.voice() func
		~ -find how to specify neural vs. standard in markup
			UPDATE: found "engine" parama in polly python api. Haven't yet found if/how this is exposed to alexa.
		-implement
	-consider how to infer/extract nationality
		X -choose first method to try (e.g. ask gpt, Q&A model on wiki summary, google search)
		~ -implement
			UPDATE: started with HF QA model. Results look pretty solid from wiki summary. Need to clean up a bit to align w/ polly voices (map Greek to European, Texas to American, etc.) Also tried Q/A on whole wiki summary rather than my truncated version and it did work better for beyonce. Wiki does have a "Born" section - might be more reliable to use this, or a combination of the two.
	~ -try emotion markup (see if console lets us do this)
		UPDATE: only supported by native voices and only supports disappointed and excited, but it is cool. Spent some time trying various sentiment scorers/zero shot models/emotion classification models to see if we can detect these emotions quickly and accurately. Started evaluating on sms dataset. Also messed around a bit in polly console w/ pitch/prosody. See misc.txt for settings I liked for salli voice.
	X -see if we can find somewhere in console showing usage/charges (prob not, knowing amazon)
		UPDATE: Looks like no :(
-[BLOCKED: try manual markup first to get a sense of how this works] prototype emotion markup (Note: most gpt options will likely be too slow for alexa, but still could be cool to prototype for future use in ???)
	-have gpt3 insert tags directly (i.e. Edit rather than complete)
	-have gpt3 select from options (almost like classification. Maybe just use logit_bias.) then fill in template programmatically.
	~ -smaller model running locally (Q&A model, sentiment scorer, etc.). Limit to a much smaller subset of markup patterns.

5/20/22 fri
-----------
X -debug why two personas are loaded when only 1 was specified
	UPDATE: forgot that howard was a custom persona. Deleted him from that dir and removed the limitation for loading 1 person, since it takes ~.03seconds and only executes once on app start anyway.
-polly.ai voices
	X -check if polly response includes an "engine" param where we can specify neural or standard
		_ -if yes, add support for both
		UPDATE: all I could find was a message from 3-4 years ago from an amazon rep saying alexa did not support the engine param yet and only provided the standard voice. No clue if that's still true but I guess I'm stuck with whatever it is.
	X -continue work on extracting nationality from wiki (see nb17)
		X -fetch wiki page for each persona
			_ -might need to fix func since disambiguation doesn't seem to be working how I expected
		UPDATE: realized lib's builtin fallback method only fixed 1 error but introduced 12 new ones (for 27 people) compared to my method. Stick with mine.
		X -try computing QA on full summary
		X -try doing my html extraction technique
		_ -check if this q/a model is purely extractive and if so, try one that isn't
		X -postprocess results to be able to map to a polly voice (american/english/australian)
		UPDATE: finished prototyping func but need to figure out where to fit it into lib.
	-emotion classifier for polly emotion tags
		-review more emotion samples at various predicted scores (nb17). Try to get a sense of what decision threshold to use.
		-is this even worth it? Might only be <5% of responses. But maybe that will make things feel more realistic.
	-update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.

5/21/22 sat
-----------
~ -integration voice/nationality extraction
	X -port func to lib (maybe rm scores? Or just ignore them in wiki_data func or whatever calls it. Not bad to have for inforational purposes.)
	X -where should this go? Loading q/a model could be slow.
		UPDATE: made it loaded lazily but updating openai_utils (or modules it imports) is still super slow. That could be an autoreload issue though - my lazy loading seemed to remove the model load message so I think it's working.
	X -update various openai classes accordingly (wiki_data() is used in a few places)
	X -update alexa voice selection function
		X -test in console
			UPDATE: works, though the male australian voice sounds a lot more robotic than in the sample.
	_ -document how to adjust this if polly adds more voice options
		UPDATE: my implementation method actually makes this happen automatically. Adding new attrs to wiki_data() is another story, but I think there are ways around that. (I.e. if we want to add more attrs in the future, maybe just compute them externally to ConvManager).
-nb17
	-emotion classifier for polly emotion tags
		-review more emotion samples at various predicted scores (nb17). Try to get a sense of what decision threshold to use.
		-is this even worth it? Might only be <5% of responses. But maybe that will make things feel more realistic.
	-update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.

5/22/22 sun
-----------
X -troubleshoot why openai_utils is SO slow to reload now
	UPDATE: ~2 seconds seem to be external deps that have been here for a while, ~2 seconds was the addition of a transformers import, ~1s was everything else, and ~3 seconds seems to be from using autoreload in a big notebook (i.e. it's not present when importing the module fresh for the first time). Added default html parsing option for birthplace extraction and adjusted convmanager to handle passing in a qa pipe.
-nb17
	-emotion classifier for polly emotion tags
		X -review more emotion samples at various predicted scores (nb17). Try to get a sense of what decision threshold to use.
		UPDATE: decided this dataset is just not right for my use case. Found a better one in amazon common sense conversations and computed new scores on the validation set (1k samples; train would have been a bit slow at roughly 10x that). Still pinning down rules but results are looking better so far.
		~ -is this even worth it? Might only be <5% of responses. But maybe that will make things feel more realistic.
			UPDATE: with commonsense dataset, looks like it could be quite a bit higher. (Probably shouldn't go TOO much higher though - alexa cautions against over-using emotion tags).	
	-update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.
-makefile/cli command to generate new prompt skeleton
-makefile/cli command to generate new custom persona skeleton

5/23/22 mon
-----------
X -nb17
	X -review more samples from commonsense dataset to select thresholds/heuristics for excitement (closest option in classifier is "joy") (NOTE: the level=High rows look pretty good, but the level=Low are a bit weak. Maybe should up that threshold.)
		UPDATE: in my eyes this really seems to hinge on the exclamation point. Withouto that, require score >= .9. Now we have ~9% excited turns which seems more reasonable than the 14% I was getting at the start of this work session.
	X -follow a similar process to define thresholds/heuristics for "disappointed" (closest option in classifier is "sadness")
		UPDATE: about 6% excited, 6% disappointed, 88% neutral. Sounds reasonable enough?
-update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.

5/24/22 tues
------------
X -port emotion func to lib
	X -consider: should emotion pipeline go inside that or be separate? Maybe just one emotion_tagger func that maps from text -> tag.
X -update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.
	UPDATE: works, though we seem to be forced to choose between emotion tags and custom polly voices. I favor the latter atm since emotion tags affect only a small % of responses and are not all that noticeable, compared to speaker gender/accent.

5/25/22 wed
-----------
X -troubleshoot why saying just last name in choose_person is not working
	UPDATE: I was lowercasing personas() but not the prompt slot name.
X -makefile/cli command to generate new prompt skeleton
	UPDATE: also tweaked query_gpt3 defaults as a result: top_p 1->.99.
X -makefile/cli command to generate new custom persona skeleton
	UPDATE: also changed from storing meta.json to meta.yaml to make it easier to create custom personas manually. Then generated all 31 personas over again (even with no qa pipe, this still takes ~3 min - slower than I remembered).
~ -prototype gpt Emotion Markup Language prompt
	UPDATE: not really necessary atm since polly doesn't provide many emotions and we wouldn't have time for another gpt query anyway, but this is pretty cool. GPT3 sort of made up its own markup language based on 1-2 examples! Technically it turns out emotion markup language is a real thing, but the syntax is different than what I specified and gpt matched my syntax, not the real language's (i.e. it's inferring syntax rather than regurgitating it). That being said, if I actually wanted to use this for something, the real EML syntax (or something closer to it) is probably better - I think that would allow us to parse it as xml or something like it, whereas my current syntax would be trickier to parse. Might be desirable to use logit_bias to promote a fixed set of tags but it depends on the use case (currently: None) and I don't think it natively supports multi-token biases so we'd either be out of luck or have to devise a hacky workaround. I am curious to see if we could use gpt to invent other more useful languages though.

5/26/22 thurs
-------------
X -easy option: add "doc" field to prompt
	X -update template
	X -update existing
	X -update load_prompt if necessary
	UPDATE: atm load_prompt just pops it off and does nothing with it. If I revisit my idea of a Prompt class, it could be useful to keep around but not pass to gpt.query().
-provide makefile command to update prompt dir readmes using new "doc" field
	X -figure out plan (bash? jabberwocky? htools?) for how to implement
		UPDATE: write py func in jabberwocky and call it from makefile inside `make readmes`.
	X -implement
		UPDATE: basically done, though slight tweak needed: '<|endoftext|>' is used in one prompt 'doc' and gets cut off, I think because it's displayed in a markdown table in the readme. Need to figure out some way to escape it.
-try tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/27/22 fri
-----------
X -try to escape text like '<|endoftext|>' so it can show up in a markdown table.
X -decide where to store last night's prompt prototype script; maybe copy to or ref in other dev nb so I can see both if I return to it at some point
X -try out other templating engines
	X -jinja2
	X -mako
	UPDATE: see nb16 for notes on this, as well as pasted code from new prompt class noodling.
	~ -watch textx walkthrough
	UPDATE: not convinced the level of completity:utilty is worth it here.
-consider how to deal with prompt versioning (i.e. should we explicitly version prompts inside the yaml file? Or just rely on github commits and dates from logs? Probably a good idea to include a version number and include it in meta field of logs.)
-try tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/28/22 sat
-----------
X -see if I can publish a WIP private skill for me only (so I can test it on echo rather than the test UI)
	UPDATE: already available on my echo, no need to publish actually.
X -generally try interacting with it on echo to find new bugs to fix
X -add fuzzy matching for mis-transcribed names
X -use correct name from wiki page in cases where I misspelled name (e.g. "apollo ohno")
X -regenerate ohno persona
~ -troubleshoot outlook email sending
	UPDATE: was unable to reproduce issue after initial failure. Keep an eye out for future issues.
-consider how to deal with prompt versioning (i.e. should we explicitly version prompts inside the yaml file? Or just rely on github commits and dates from logs? Probably a good idea to include a version number and include it in meta field of logs.)
~ -try tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
    UPDATE: not immediately clear if this makes a difference.
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/29/22 sun
-----------
X -get skill back up and running after mac os updates
    X -troubleshoot xcrun error when running 'make ngrok'
    X -see if we need to update IP address in alexa console
        UPDATE: yes. Seems we need to rebuild too for new endpoint to be used.
~ -add error handling for if email send fails
    UPDATE: seems to be already there. I recall seeing an error message in the console but I that may have just been logged by one of the intermediate functions. Nothing to do here atm.
X -add method to give a cleaner way to check if ConversationHandler is mid-conv
~ -look into outlook email server name in htools. Should I be using "smtp.office365.com" instead?
    UPDATE: seems like maybe they use slightly diff protocols (?) or something to send emails. Tried again with current setup and it executed w/out error so /shrug.
X -add "nobody"/"noone" to skill model in UI as another valid way to quit.
    X -figure out how to implement
        UPDATE: originally started to add new intent but I want to capture the whole utterance which is basically just delegate(). Decided to try integrating it into that first.
    X -implement
    X -test
~ -consider how to deal with prompt versioning (i.e. should we explicitly version prompts inside the yaml file? Or just rely on github commits and dates from logs? Probably a good idea to include a version number and include it in meta field of logs.)
    UPDATE: gave dvc a brief try but didn't seem particularly useful for my purposes. I think for now maybe I'll just add a version field to everything, log it, and in the future if I really need to dig into past versions I can dig up the relevant git commands.
-further experiments with tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/30/22 mon
-----------
-add Version as prompt field
    -update existing prompts
    -update prompt template
    -update load_prompt() and/or gpt.query() logging to handle this
        -consider: should we have a "meta" field in yaml file and make things like reminder/doc/version subfields of that? Maybe easier to handle in load_prompt/logging.
-further experiments with tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 


Backlog
-------
-add more docs/help on how to invoke each intent
    -add spoken examples to docstrings
    -add "help" intent that reads out some sample commands (e.g. "To change your settings, you can say...")
-look into raspberry pi as long term option for hosting
-[BLOCKED: try manual markup first to get a sense of how this works] prototype emotion markup (Note: most gpt options will likely be too slow for alexa, but still could be cool to prototype for future use in ???)
	-have gpt3 insert tags directly (i.e. Edit rather than complete)
	-have gpt3 select from options (almost like classification. Maybe just use logit_bias.) then fill in template programmatically.
-incorporate new bio cleanup prompt into convmanager
-[ON HOLD: so far this seems to work, maybe just let myself stumble upon bugs if they arise during further development] test new stopword streaming functionality for paid backends
    -openai
    -gooseai
    -test backends on other cases
        -np > 1
        -nc > 1
        -nc > 1 AND np > 1
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary
-try to reproduce error w/ gpt banana: querying w/ np=2 and n=3 only produced 2 text responses. Second try worked as expected (6 responses - unsure why the difference. Maybe autoreload in ipython reset something since I edited docs first call?)
-idea: prompt mixin? I.e. add snippet of text to start/end of a prompt, e.g. "You are a friendly, polite AI assistant.".
~ -revisit question of how to specify engines in prompt config files (note: some tasks require multiple engines ðŸ˜±. This may require some changes. I guess we could have a diff format for completion prompts vs. search prompts, for instance.)
-update jabberwocky version and upload to pypi
-update query_gpt3 to support codex, embeddings, etc. (not necessary here, probably, but good to do in general)
-update end of conv to erase conv-level settings
    ~ -consider whether settings resolution order is right/desirable (i.e. should global level take priority or should conv level?)
        UPDATE: thought a bit and this is quite a complex task. Still not at a solution yet but my sense is that priority should be more about sequence (most recent = highest priority) and scope should just be thought of as "when do we undo this setting change action"? Need to hold off on other changes until I figure out desired system.
    -update change of person to change person-level settings
    -maybe update settings.clear() to allow clearing a single (or finite number) of state levels.
-idea: natural language unit tests? E.g. if we want a response to be cheerful, we can write the instructions to increase the probability of that happening, but the prompt may have multiple parts and tone may get lost in the shuffle. We could increase our odds of a cheerful response by generating k responses, passing each through an additional prompt ("natural language unit test") asking "is this response cheerful?". We should be able to use the logit_bias arg with some positive weight to heavily incentivize the test to output "pass/fail" or "yes/no".

Easy Backlog
------------
-look for things to document

Long term backlog (i.e. things I decided are outside the current scope)
-----------------------------------------------------------------------
-update gui to support newer jabberwocky version
    -adjust gui accordingly (cur offer different radio buttons for each neo size; could instead have 1 radio and have it actually use the engine_i slider)
    -add gooseai backend option

