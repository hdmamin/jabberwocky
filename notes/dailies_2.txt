Context: been fiddling around with this project again lately but I'm not quite ready to restart daily contributions. Documenting where I'm at here for now so it's easier to pick up when I come back.

11/15/21 mon
------------
Left off
-talk() now prints responses with pretty formatted persona name.

Next Steps
-adjust query_kwargs so that we don't pass the wrong ones to gptj or gpt-neo mock funcs (maybe something like drop keys in query_kwargs that aren't in the func signature? Might not work if they take kwargs).
-write new mock funcs to support:
    -codex
    -new open source gpt-like model on modelhub (name is something about "sci" IIRC)
-live typing effect?
-consider a "choose from 1 of k responses" mode. Would need to think about how this would work w/ live typing and how it would work w/ conv manager (usually query() updates the conv history).
-maybe replace save/quit options w/ prompt_toolkit menus?

Context: picking up on this again for part 2 (Alexa skill).

1/31/22 mon
-----------
X -write premortem

2/2/22 tues
-----------
X -look through sample alexa conversation skeleton repo
    X -decide whether to use that template or "start from scratch"
        UPDATE: start by trying template.
X -create alexa skill in AWS UI
X -change invocation name
    UPDATE: needed to be 2 words and I wanted it to be short. Jabberwocky -> Voice Chat.
~ -start adding intents
    ~ -start adding slots
        UPDATE: wrote list of possible slots. added Person slot and Model slot.
    UPDATE: wrote list of possible intents in misc.txt. Added choosePersona intent.
-figure out how code will be structured locally (will alexa reference a separate repo if I use the conversation template? Will a single file basically be sufficient? Maybe an `alexa` subdir which can contain whatever I need it to?)
-follow UI instructions to determine next steps

2/3/22 wed
----------
~ -read up on dialog delegations strategy (do I want this?)
    _ -add to choosePersona if necessary
        UPDATE: alexa auto-determines next thing to ask? Most of my interactions should require minimal scaffolding so I don't think we need this.
~ -read up on intent confirmation (do I want this?)
    _ -add to choosePersona if necessary
        UPDATE: unnecessary at least for now. Consider adding later for Model or settings or choosePersona, maybe.
X -create more slots (see misc.txt)
X -create more intents (see misc.txt)
~ -see backlog
    X -fix incorrect usage of intents/slots 
        UPDATE: intents should use a var name and then you set the var type in the section below. Previously I was using the var type in the intent itself. I.e. "call {amazon.Person}" -> "call {Person}", where Person has type amazon.Person
    ~ -start testing interactions
        UPDATE: lots of failures. I realized my code has been filled in with their conv template but I need to make a lot of changes.

2/4/22 thurs
-------------
~ -look through code tab to see how expected interactions look
    X -try out in Test tab
        UPDATE: still having trouble getting this to work. Found docs for ask_sdk and decided to try to follow those locally.
~ -explore possibility of local ngrok
    UPDATE: not sure if this is compatible with lamda style app code. Table this for now and return to amazon hosted endpoint.
X -add code locally
    UPDATE: created new alexa dir.
-start following ask_sdk tutorial
    X -write launch handler
    ~ -write choose model handler
    ~ -write choose person handler
-update code to use my new intents
    -regular conversation
    -choose person
    -change max length
    -change temperature

2/4/22 fri
----------
-continue following ask_sdk tutorial
    X -finish choose person handler
        X -figure out best way to make conv manager available globally
            UPDATE: initially wrote wrapper class that delegates to an attr where is stores the manager, then realized it didn't seem to be doing anything useful. Just load the manager as a global var in the global scope. Can adjust strategy later if needed.
        UPDATE: tentatively done, though will likely need some troubleshooting/extra functionality/logging/error handling once I start testing.
    ~ -finish choose model handler
        UPDATE: tentatively mostly done, though need to figure out how to handle changing model to gptj or other non-int version. I think a better strategy than updating kwargs may be to store some session-level kwargs (I believe handler_input has some kind of session object that may be appropriate for this), then always pass that in to the query method.
X -write convenience func to access slots
X -write convenience func to speak/respond
~ -clean up template file a bit (delete unused classes)
_ -confirm stackoverflow-suggested method of extracting slots works (unclear - pycharm code completion stopped finding it)
    UPDATE: prob need to wait until I figure out how to run this thing. Template has a helper method that does something similar so that's an option too.

2/5/22 sat
----------
[DON'T LET THIS GET PUSHED BACK TOO FAR - don't want to end up having to rewrite a ton of code if something is incompatible.]
~ -figure out how to get this running on lambda (if possible: reliance on local files might be a problem)
    X -how hard would it be to convert this to something we can run with ngrok? Is lambda_function.py still compatible or is it totally different locally?
        UPDATE: decided to try rewriting with flask-ask. We're still early and it seems like dev/testing will be far easier (hopefully avoid slow build time for every change).
X -write new template app with flask-ask
X -try running flask app w/ ngrok
    UPDATE: accessible via generic url.
X -rebuild model 
    ~ -see if I can access flask-ask app through aws console
        UPDATE: no errors but still references console code rather than flask ask local code.
-look into handler_input session object
    -try using this to update kwargs (e.g. model, max len, etc.)
-more intent functions/endpoints:
    X -set temperature
    ~ -set max length
        UPDATE: added skeleton func.

2/6/22 sun
----------
~ -troubleshoot model build (no errors but still references console code rather than flask ask local code)
    ~ -try new flask-ask version in Alexa console (Test tab)
        UPDATE: spent most of time troubleshooting w/ minimal progress. Seems like the test console knows about the right URL (I think) but the api call is failing so it falls back to some generic message. Sample flask-ask code from aws tutorial has the same problem - possible that flask-ask is just no longer compatible ðŸ˜¬. When using test console, I see failed requests show up in app (not to any specific endpoint, looks like?).
-more endpoints:
    ~ -set max_length endpoint
        UPDATE: still need error handling though.
    -conversation end (add extra question offering to email transcript to user?)
    -generic conv response endpoint
X -fix logger (wasn't printing to stdout)
    UPDATE: unsure if fix was related to changing logging level or just restarting app, but it works now.
X -read up on flask-ask session vs. context (readthedocs.io tab)
    X -update it to store default _kwargs attr from conv manager
    X -update chooseModel to use this
    X -update chooseMaxLength to use this
    X -update chooseTemperature to use this

2/7/22 mon
----------
X -add error handling to max_length endpoint
-add more endpoints:
    ~ -conversation end (add extra question offering to email transcript to user?)
        UPDATE: started writing but it's a bit tricky with optional saving.
    X -generic conv response endpoint
    X -debugging intent (repeat user response back)
X -first draft of func to save conv (want it emailed rather than local)
-brainstorm: what else could possibly cause these alexa issues? How to troubleshoot and eventually fix?
-see if there are places I should add reprompt (chained method after question)

2/8/22 tues
-----------
_ -new endpoint for user to set email
    _ -add intent in console
    UPDATE: think I found a way to do this via amazon API rather than through voice.
    X -function to get user email using Amazon API
        UPDATE: untested though because it requires Alexa context object to run, which means I need to get the test tab working to see if it works.
-update other endpoints to check if should_exit
-rename `exit` function to avoid clobbering builtin
-brainstorm: what else could possibly cause these alexa issues? How to troubleshoot and eventually fix?
-see if there are places I should add reprompt (chained method after question)

2/9/22 wed
----------
X -rename `exit` function to avoid clobbering builtin
~ -see if we can easily update htools.quickmail to include text attachment
    UPDATE: can't fully test yet bc of freedom app, but seems to send without error at least. Though worryingly, text never arrives to phone when I tried that using {phonenumber}@vtext.com.
X -make new jabberwocky email for sending transcripts
    X -add info to htools creds file
    X -enable unsafe app access
    X -update email_user func to send transcript as attachment
-see if there are places I should add reprompt (chained method after question)

2/10/22 thurs
-------------
X -check hmamin2 email to see if quickmail works as expected (did emails arrive? did image attachment work? Did text attachment work? What is attached text file named?)
    UPDATE: email worked, sms didn't. Text just got appended to body though, not a real attachment.
    -update to allow multiple attachments
    X -investigate lack of sms showing up in text mode
        UPDATE: do not use leading 1 in phone number email.
    X -look for solution to lack of image attachments in text
        UPDATE: you need to use a different phone number email domain than vtex.com for mms, but it looks like it may not be supported anymore.
    X -update docs to reflect new signature
    -[BLOCKED: check emailed results first to see if attachment worked. Consider adding option to attach vs. embed.] bump version and upload to pypi
~ -look in alexa dev forum for how to direct 1 intent to another (prob need to use session to set temporary key)
    UPDATE: sdk does support "intent chaining". Unclear if flask-ask does, either natively or via compatibility w/ sdk.
    -update other endpoints to check if should_exit

2/11/22 fri
-----------
X -check hmamin2 email to see if quickmail works as expected (did emails arrive? did image attachment work? Did text attachment work? What is attached text file named?)
    _ -fix as needed
    X -[BLOCKED: check emailed results first to see if attachment worked. Consider adding option to attach vs. embed.] bump version and upload to pypi
    UPDATE: emails arrived, image attachment worked, no diff between image attach vs. inline, text attachment worked, text file is named same as source file as intended.
X -update curl to have verbose/non-verbose option
~ -read more of intent chaining tutorial
    UPDATE: not worth getting too far into this yet - wanted to make sure flask-ask provided some way to do this first.
    X -look for how to implement this w/ flask-ask
        UPDATE: can import delegate just like question or statement, but still figuring out how to use it.
    -update other endpoints to check if should_exit
X -update intent decorator to always store prev intent in session
~ -figure out how to test flask-ask locally
    UPDATE: still no progress. Think I need to figure this out before going any further - hard enough to build a hello world app without any mechanism for testing, but impossible for a complex skill w/ intent chaining.
X -brainstorm: what else could possibly cause these http/alexa issues? How to troubleshoot and eventually fix?
    UPDATE: some thoughts below
    -wrong AWS account?
    -start totally new skill in console and try again
    -pare app down to most bare-bones version and see if I can get it working in console (comment out new endpoints that might not have intents defined for them. Also might get rid of some boilerplate, like the "what's your fav color?" intro which I don't know the source of)
    -flask-ask is deprecated (sort of doubtful, since found a tutorial from not that long ago still using it)
    -alexa dev console is broken (sounds like it was at some point, but that was years ago - surely it's not still broken. Could create new skill w/ old console maybe? Forget if that's an option.)

2/12/22 sat
-----------
X -fill in some docstrings in app.py
~ -work through troubleshooting ideas 1 by 1
    ~ -wrong AWS account?
        UPDATE: think this is correct (name Harrison and initial H; couldn't figure out how to easily confirm email but I'm pretty sure it's the right one)
    ~ -pare app down to most bare-bones version and see if I can get it working in console (comment out new endpoints that might not have intents defined for them. Also might get rid of some boilerplate, like the "what's your fav color?" intro which I don't know the source of)
        UPDATE: got launch intent working on dummy app! Still haven't gotten other intents working though.
    _ -flask-ask is deprecated (sort of doubtful, since found a tutorial from not that long ago still using it)
        UPDATE: seems doubtful since bare bones app shows signs of working. Possible question/statement json is no longer compatible, I suppose.
    X -start totally new skill in console and try again
        UPDATE: combined this with bare bones app strategy.
    _ -alexa dev console is broken (sounds like it was at some point, but that was years ago - surely it's not still broken. Could create new skill w/ old console maybe? Forget if that's an option.)
        UPDATE: seems unlikely given positive signs with bare bones app.

2/13/22 sun
-----------
~ -continue trying to get dummy app to work
    X -add HelloWorld endpoint
        UPDATE: This works! 
    ~ -debug YesIntent endpoint (first expected response after launch. Error seems to be with recognizing the right intent, not with the response, based on terminal logs.)
        UPDATE: Still no success. Sincce HelloWorld works, my guess is YesIntent sample words are too short/generic ("yes", "sure"). Maybe AnswerIntent fails for the same reason.
~ -look at jabberwocky launch endpoint and see if I can find any differences that would cause it to fail when dummy app launch endpoint works (see misc.txt for promising idea)
    [IF theory of name clobbering is correct]
    X -rename Voice Chat to something less likely to have duplicates
        UPDATE: trying "Quick Chat". Tested this in dev console and it doesn't seem to be a built-in, unlike Voice Chat.
    X -rebuild model
    X -test launch intent for jabberwocky
        UPDATE: still failing. Think we may need to change template. See misc.txt.
    X -Try non-intent response to see if fallback handler works on jabberwocky (maybe don't even try to actively start app? Could try both ways.)
        UPDATE: no, just no response or built in default of some kind.
    X -If that doesn't work, switch to make skill debugger console the main one and deprecate/delete jabberwocky in console. Choice of vanilla skill rather than conv template might fix things.
        UPDATE: Created new skill since I can't figure out how to rename skill (can change name in invocation but not how it shows up in console, it seems)
-test new skill intents
    X -launch
        UPDATE: Yes! First working endpoint for jabberwocky. Needed to use name "Quick Chat" as alexa interprets "Voice Chat" as some kind of existing functionality which I haven't enabled, apparently.
    ~ -fallback intent
        UPDATE: not working yet. It did in dummy app so need to figure out what the difference is here.
X -map out conv flow model
    UPDATE: Drew out basic flowchart of interactions. See notebook.

2/14/22 mon
-----------
~ -troubleshoot fallbackIntent in new skill (worked in dummy app, so why not here?)
    UPDATE: my intent deco was implemented wrong. After fixing it, the fallback endpoint does work as far as reading the right response. However, while logging inside the fallback endpoint wokrs, logging inside deco does not. Unsure if this means the deco is somehow going unused (seems unlikely given recent behavior) or if this is a matter of logging output being hidden. Tried using diff logger than the default app one but this didn't resolve the issue. Tried checking prev_intent and it's not being updated either (supposed to be done in deco). However, if I manually update it in the func itself, it works.
X -add endpoint to list out all personas

2/15/22 tues
------------
X -add sys.exit in various stages of intent deco to try to get to the bottom of whether it's being called
    UPDATE: it wasn't. ask.intent deco is quite unusual and overriding it is confusing. Took a different approach.
X -write new ask subclass and redefine intent deco
    X -write custom IntentCallback
    UPDATE: ask.intent wrapper is odd, appears to never actually be called but it must be sometime. Must do some weird registration magic or something on the ask object. Managed to get my prev_intent tracking working by writing custom callback and having custom ask class wrap the input function when it's passed in (rather than explicitly defining on start/end behavior in decorator, define it via callback methods).

2/16/22 wed
-----------
X -add intents to console for new skill (jabberwocky-voice-chat, not jabberwocky)
X -add slots to console for new skill
X -test app in test console and see what doesn't work (sure there will be something)
    UPDATE: looks like it can send a request to choosePerson and changeTemperature endpoints but they receive no arguments (e.g. slots aren't working, or at least not how I expected them to).
    X -write next steps based on observed errors
        UPDATE: see tomorrow's to do list.
X -docs for custom ask (in particular: note to self not to delete seemingly useless wrapper inside intent deco. Flask-ask version uses that so I don't know what else is reliant on it)
-start updating endpoints in py file based on observed errors (see above)

2/17/22 thurs
-------------
-fix missing slots issue with choosePerson
    _ -replace actual logic w/ debugging/logging to figure out what (if any) arg(s) func is receiving
        UPDATE: looked at existing logs and error indicatded no arg of any kind was received.
    X -update parsing to get correct object
        UPDATE: found blog post implying custom slot might have to be extracted from request manually. Wrote new code to do that and it works for choosePerson (successfully selected an existing persona!).
    X -reintroduce actual logic and test
        UPDATE: new persona doesn't work yet (relies on YesIntent, which I added now, but much more new logic is needed).
~ -start updating endpoints in py file based on observed errors (see above)
    ~ -changeModel
        X -rename chooseModel -> changeModel (forgot I chose diff name in console for this skill)
        X -fix ModelType slot (4 -> 0)
        UPDATE: added some str handling to chooseModel endpoint. Model is currently rebuilding in console.
    -changeTemp
        UPDATE: brief testing reminded me of issue where alexa thinks I'm trying to change temperature of an actual device. Will need to consider options for how to handle this.
    -changeMaxLen

2/18/22 fri
-----------
~ -continue updating endpoints in py file based on observed errors (see above)
    -changeModel
        X -check that build succeeded
        X -try out zero, one, two, three
        _ -try out j
        _ -try out neo
        UPDATE: might want to rewrite j/neo query funcs w/ availability of new open source models (and possible deprecation of old gpt-j api? Unclear.) Do this later.
    -changeTemp
        ~ -consider how to deal with issue noted in prev day's notes (could rename to Temp/something else, or maybe there's a way to override default commands when in a custom skill)
            UPDATE: tentatively seems okay now - I think maybe before I was saying this after returning a Statement rather than a Question so we had exited the skill?
        ~ -figure out how to convert str to num
            UPDATE: considered requiring an int in [0, 99] and hardcoding list of strings. Was curious though and decided to try gpt way - got this working reasonably well but only with a disappointingly big model (smaller ones prob could do well with more prompt tuning but I didn't quite get there yet). Added prompt text and config files and added API call to app script. However, still having some trouble parsing decimals.
    -changeMaxLen
    -reply

2/19/22 sat
-----------
-continue fleshing out endpoint logic
    ~ -changeTemp
        X -consider options for improving number parsing and choose one (i.e. require int from 0-99 and divide answer by 100, or update slots to expect "point {number}".)
            X -make necessary changes in console and/or script
            _ -update gpt word2number prompt files accordingly (e.g. maybe no decimals needed after all, could just be word2int)
                UPDATE: unused now so not important. For general purpose future use, better to leave it more broadly capable.
        UPDATE: changed to use hard-coded dict with some fuzzy matching. Couldn't test yet since I accidentally blocked dev console w/ Freedom.
    -changeMaxLen
    -reply
~ -planning re maintaining settings
    UPDATE: initially started as just trying to alias session.attributes with something shorter since it's used so much. Surprisingly trick. Ended up causing me to think about settings and realizing we really want 3 diff levels (see misc.txt notes from today). Started building new class to maintain this.

2/20/22 sun
-----------
X -continue fleshing out endpoint logic
    X -changeTemp
        X -test new changes in console.
            UPDATE: updating temperature works for valid integers. Non-number word rightfully raised an error which was handled gracefully.
    X -changeMaxLen
        X -look more if there's a better way to do this with alexa (surely yes, but seems like no???)
        _ -add gpt3 parsing (too many ints to hardcode cleanly)
            UPDATE: Turns out it does this automatically after all. Previous failures might have been due to flask ask argument issue. Tested this on both valid and invalid values.
X -update settings intents to accept optional slot for scope (global/person/conversation)
    UPDATE: still need to fully make use of this in app.py.
X -Settings class
    X -think more about desired interface (important - need to know what I'm trying to implement)
    X -continue fleshing out attr access methods
    X -write resolve() method
    X -update app.py accordingly
X -start building query_gpt_j variant using banana.dev backend (prob smallish version, but good to have another free option in case random git one gets deprecated)
    UPDATE: need to add support for 'stop' param, maybe better arg validation to ensure no nnaming issues around topK vs top_k, etc.

2/21/22 mon
-----------
X -consider: should Settings include kwargs only or also things like prev_intent/should_end/should_save?
    X -update app.py and/or utils.py accordingly
    X -document Settings
    UPDATE: include both in Settings. Make kwargs accessible via bracket notation and general settings accessible via dot notation. Also try to cut down on number of general (non model query) settings - rely more on prev_intent and yes/no. Might have to change yes/no intent endpoints to not update prev_intent though.
X -test new changes in test console
-flesh out `reply` endpoint

2/22/22 tues
------------
-debug/fix: choose person intent seems to soak up any reply containing a name (or at least sometimes). Can try:
    X -adjusting sample utterances for choosePerson intent in console and re-test
        UPDATE: found "Call {person}" no longer is clobbered by default behavior. Guessing I previously exited the skill session w/out realizing it before trying to trigger that intent.
    ~ -add some logic using prev_intent to infer when a reply has mistakenly been identified as choosePerson
        UPDATE: added check in choose_person if conv is active but this still chops off the first word of the response  we can't recover the full text.
    X -Add new `reply` endpoint w/ AMAZON.SearchQuery slot (this is supposed to soak up the whole text response)
        UPDATE: had to use leading space to avoid errors.
-try refactoring out base functionality of endpoint funcs (bc flask_ask not passing args to intent funcs, we need to get slots inside func, which makes it hard for one intent to call another)
    UPDATE: Ended up being unnecessary so far but might have to reintroduce at some point. Another option is to make endpoint accept an optional arg and then use it if provided and extract slot if not.

2/23/22 wed
-----------
X -try updating reply intent to match what I made in console
    X -test in console
    _ -update/rm fallback intent in code
        _ -and rm in console if necessary
        UPDATE: decided keep for now. Might need an extra layer of fallback beyond SearchQuery.
    UPDATE: works, though identified possible bug on second query (I also called changeModel in between). Conv didn't think a conv was in progress - does conv object get recreated after each skill call? Need to investigate. Also found possible bug where sessionstate str appeared empty in console even though mock_func should have been set.
X -update ask deco to log settings state on each intent call
-consider yes/no intent logic: need to prevent them from updating prevIntent? Consider tracking a longer intent history list rather than just prev.
    -make updates accordingly
-see if we can move IntentCallback and CustomAsk to utils (bit tricky w/ reliance on session?)
X -make use of Scope var in settings intents
    UPDATE: also added `default` option to slots func to make this work better when user doesn't specify level.

2/24/22 thurs
-------------
X -investigate issue where second reply causes error due to "conv not in progress". (see yesterday's notes documenting this error and the one below)
    UPDATE: rewrote slots func a bit (I think I was doing several things wrong - list(values())[0] logic was not safe, changeTemp func passed wrong slot name to slots for Temperature). Seems to work now.
~ -investigate bug where Settings obj __str__ appears empty in logged message
    UPDATE: could not reproduce. Keep an eye out for this.
X -add readContacts intent in console
    X -test in test console
-update end of conv to erase conv-level settings
    -update change of person to change person-level settings
    -maybe update settings.clear() to allow clearing a single (or finite number) of state levels.

2/25/22 fri
-----------
X -add readSettings intent to console
    X -add code endpoint
    X -launch rebuild
    X -test in console
X -add endChat endpoint
    UPDATE: also had idea to prefix settings commands with "Alexa" to distinguish them from replies. But when testing, alexa did not recognize these utterances (just fell back to reply intent). Need to work on this more.
-update end of conv to erase conv-level settings
    ~ -consider whether settings resolution order is right/desirable (i.e. should global level take priority or should conv level?)
        UPDATE: thought a bit and this is quite a complex task. Still not at a solution yet but my sense is that priority should be more about sequence (most recent = highest priority) and scope should just be thought of as "when do we undo this setting change action"? Need to hold off on other changes until I figure out desired system.
    -update change of person to change person-level settings
    -maybe update settings.clear() to allow clearing a single (or finite number) of state levels.
~ -start prototyping prompt routing system (a way to implement intent chaining, basically)
    UPDATE: some progress here. Plan is to have an endpoint func's follow up func by named identically except with a leading underscore. Could become problematic if we need multiple followups though. Started testing on end_chat but ran into utterance problems (see above).
-tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)
    
2/26/22 sat
-----------
[LEAVE EMAIL UNBLOCKED - testing email sending]
X -try adjusting end_chat utterances since curr ones aren't getting recognized
    X -maybe adjust other settings intent sample utterances to be prefixed by something distinctive. Looks like 'alexa' might not have been the best choice.
        UPDATE: noticed my attempt to trigger end_chat was getting sent to SearchQuery WITHOUT the leading "Alexa", adding to my theory that this word is treated differently. Decided on the name "Lou" for the skill "assistant" (one syllable, inspired by Lewis Carroll, gender neutral) and prefixed sample utterances for all settings-related commands with it.
~ -continue testing end_chat intent + prompt chaining
    ~ -troubleshoot transcript sending as needed
        UPDATE: initially was getting 401 status code due to 'Bearer: {token}' instead of 'Bearer {token}'. Fixing this now gives me a 403 status code (also added permission for user first name and email in console, and gave my personal permission in iphone app and alexa website). Still not working though.
X -look into EndSession more
    _ -When is it called? I.e. is it triggered by "stop" or something else?
    X -What should it return? I think this is correct based on an official tutorial but should confirm.
    UPDATE: every example I've seen handles this identically, so I think there's no need to look into this any more.

2/27/22 sun
-----------
-try sending transcript w/ hardcoded email (need break from debugging permissions issues)
    -debug formatting, etc.
-look into email access issues (api returns 403 Forbidden status)
    -maybe need to add card/audio asking for permission in the skill itself?
X -write choose_person followup func (do you want to download new persona?)
~ -deep dive into intent chaining
    UPDATE: flask-ask does have delegate() and elicit_slot() functionality which seems to be related to what I want, but I couldn't quite figure out how to use it do what I want (no docs :( ). Instead, started overhauling this process in new `chain` branch. Renamed searchIntent intent from reply -> delegate and have that point to enqueued funcs. Made Ask object handle this enqueue/deque process. Still need to test this more.

2/28/22 mon
-----------
~ -test new chain workflow
    -selecting a user that exists
    -selecting a user that does NOT exist and NOT downloading them
    -selecting a user that does NOT exist and downloading them
    UPDATE: made some major changes and saw some positive signs but haven't thoroughly validated that any of the above cases work post-changes.
X -auto avoid duplicate calls from queue (realized alexa often matches intents without help of queue/delegate, so enqueueing intents was problematic)
-rm remnants of old chaining system (followup_func, check for other '_' prefixed funcs, etc.)
    -consider if I should remove/update prev_func state attr? Maybe it should store func name instead of intent name since we're now often calling non-intent funcs.
-look for other intent chains that need to be implemented and document below
    -implement the following intent chains:

3/1/22 tues
-----------
X -test new chain workflow
    X -selecting a user that exists
    X -selecting a user that does NOT exist and NOT downloading them
    X -selecting a user that does NOT exist and downloading them
X -rm remnants of old chaining system (followup_func, check for other '_' prefixed funcs, etc.)
    X -consider if I should remove/update prev_func state attr? Maybe it should store func name instead of intent name since we're now often calling non-intent funcs.
        UPDATE: fine for now, not causing any problems. Maybe revisit later if I find a good reason to store more steps.
X -try sending transcript w/ hardcoded email (need break from debugging permissions issues)
    X -debug formatting, etc.
    UPDATE: Had wrong email in alexa config. Once I fixed that and updated htools.quickmail to allow Paths for attach files, no problems with email or formatting.
-look for other intent chains that need to be implemented and document below
    -implement the following intent chains:
~ -debug issues where changing settings wasn't working
    UPDATE: 1 issue was that MaxLength had wrong slot name (should be Number, not maxLength). Another was that "change model" sample utterances didn't include the exact order I was saying and I guess they're pretty rigid, so I added a couple more variants. Also changed to debug mode using htools deco to print when any func is called, bc I'm still finding it hard to follow flow sometimes.

3/2/22 wed
----------
X -look into ask.logger log file (is it just continually growing longer and longer? Maybe reset.)
    UPDATE: reset each time we instantiate Ask so it's just one session's worth of logs.
X -update send_transcript() to leave file in alexa/conversations by default rather than deleting.
X -maybe update deco to print queue/state etc. on function end as well. Sometimes confusing not having this info.
X -move some constants to config
X -clean up imports a bit
X -fix casing bug when generating a person
    UPDATE: prob should update wiki func itself to handle this, but for now just passed in a titlecased name instead to avoid updating lib.
X -adjust slots strategy to only use "resolutions" as a backup (saw some cases where resolutions worked worse than "value")
X -rm some old files in alexa dir from abandoned appraoches
    UPDATE: just moved to trash in case I need to recover later.
-look for other intent chains that need to be implemented and document below
    -implement the following intent chains:
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/3/22 thurs
------------
X -investigate: possible to get alexa to maintain casing/punctuation?
    X -if no, consider running punctuate on transcriptions before sending to gpt3. All lowercase may change results (more casual vibe?).
        UPDATE: no native support. I copied my punctuate_transcription prompt to punctuate_alexa and lowercased the first letter of each transcription, then added this to reply(). Seems to be working as expected so far.
X -tweak logging to be a bit easier to grok.
    UPDATE: some intent calls are nested and it was hard to tell which ON END log statement corresponded to which intent. Updated on_end call to print prev intent as well.
X -look into flask-ask reprompt (does this exist and where can I apply it if so?)
    UPDATE: added reprompts to all question() uses that seemed appropriate. Still need to test this in UI. Also updated some of the settings reading options to auto redirect to choose_person if the user's not in the midst of a conversation already.
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/4/22 fri
----------
X -add intent to change settings to auto-punctuate or not
    UPDATE: realized adding separate enable/disable endpoints was easier.
X -convenience func to maybe prompt user to choose_person depending on if in active conv
X -fix bug in _reply logger (can't pass multiple strings unlike print)
X -fix bug w/ func queue not being cleared on launch
-add support for banana.dev version of gptj 
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/5/22 sat
----------
~ -look into email access issues (api returns 403 Forbidden status)
    ~ -maybe need to add card/audio asking for permission in the skill itself?
    UPDATE: got personal email working via manual access and got consent card to display, but no haven't figured out how to process response yet. Included more notes in app.py (search tmp_email_me; eventually prob need to delete or change this, both in code and in UI. Just used a dummy intent triggered by "email me" for testing purposes.)
X -update get_user_email func to get name too
-add support for banana.dev version of gptj 
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/6/22 sun
----------
X -consider refactoring to make it cleaner to reset all state (queue, attrs, query kwargs, etc.). Periodically finding new things that aren't reset when I update code and flask reloads.
X -look for ways to integrate user name into skill
    X -any time Lou speaks, e.g. welcome message?
    X -maybe edit transcript to replace '\n\nMe: ' with '\n\n{name}: '?
        UPDATE: better to change ConversationPersona itself to use name in prompts. That way, generated responses can actually refer to you by name.
-allow fuzzy match for choosePerson, or at least catch last names (e.g. einstein -> albert einstein; felt like a questionably useful feature initially but even just for dev purposes it would be nice to not have to type out the full name each time)

3/7/22 mon
----------
~ -look into user email/name access issues (worked on sat but on sunday, had to provide access in alexa app again (i.e. it revoked access at some point?). Might be related to consent card, i.e. user has to provide consent for every new conv?)
    UPDATE: permissions were revoked again.
    X -add privacy policy (necessary for skills that request name/email)
        UPDATE: Also filled in some other details like skill description and sample utterances.
X -allow fuzzy match for choosePerson, or at least catch last names (e.g. einstein -> albert einstein; felt like a questionably useful feature initially but even just for dev purposes it would be nice to not have to type out the full name each time)
    UPDATE: for simplicity, just check for str equality w/ last name. Don't want any crazy assumptions being made and choosing the wrong person.

3/8/22 tues
-----------
X -add small skill icon (128x128 px) to alexa Distribution tab
    X -add large skill icon (512x512 px) to alexa Distribution tab
    X -check that icon shows up in iphone app
X -maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.
    UPDATE: mostl converted some print statements to log statements with the help of a slightly hacky getattr-inspired function that works with nested attributes and globals.
X -see if we can move IntentCallback and CustomAsk to utils (bit tricky w/ reliance on session?)
X -update end_chat flow to not ask about sending transcript if no email has been provided

3/9/22 wed
----------
X -should conv prompt config be updated to use "\n\nMe:" instead of "Me:"? More robust if we replace w/ name. Would need to update ConversationManager as well, probably.
~ -update query_gpt3 to support goose.ai models (e.g. neo 20b)
    UPDATE: wrote gooseai_backend contextmanager func and started openai_backend contextmanager class. The latter still needs some work and might end up replacing the former.
X -sign up for banana.dev and store api key
-add support for banana.dev version of gptj (see ipy in tab 1)

3/10/22 thurs
-------------
X -investigate issue where gpt3 (davinci model ðŸ˜±) seems to be used by alexa when I thought I set it to use gptj mock func. (See openai costs tab.)
    UPDATE: failed to set mock=True, both in alexa and gui, for gpt-j usage. Updated query_gpt3 to automatically do that when mock_func is not none. Still concerned that func's interface might need to change dramatically.
X -flesh out openai_backend support
    X -test openai_backend class in ipython
        X -switch from openai to gooseai
        X -switch from gooseai to openai
        X -persist mode works
        X -no persist mode works
    X -can this replace my gooseai_backend contextmanager func?
    UPDATE: major rewrite -> BackendHandler class. Deleted old contextmanager func.
-figure out best interface to use gooseai (just use query_gpt3 w/ diff backend? Kind of annoying that I have to pass mock_func in some cases, but in this I change the backend.)
    -look into docs on api_backend and api_key. Maybe all my mock_funcs could use this? Would be nice to avoid passing in mock_func.
    -make sure we can still track (via kwargs or other attr, perhaps) what backend is being used. Don't want this to be assumed.
-[BLOCKED: confirm possible redesign of backend switching and/or mock funcs] make dist and upload to pypi w/ new query_gpt3 changes?
-add support for banana.dev version of gptj (see ipy in tab 1)

3/11/22 fri
-----------
~ -easy option: write docs for...
    X -IntentCallback
    -look in alexa/utils and alexa/app for others
-figure out best interface to use gooseai (just use query_gpt3 w/ diff backend? Kind of annoying that I have to pass mock_func in some cases, but in this version I change the backend. Maybe can adjust BackendHandler and/or other query_gpt_{x} funcs to handle everything automatically.)
    ~ -look into docs on api_backend and api_key. Maybe all my mock_funcs could use this? Would be nice to avoid passing in mock_func.
        UPDATE: didn't find any formal docs besides a mention of some azure endpoints - nothing with huggingface.
    -make sure we can still track (via kwargs or other attr, perhaps) what backend is being used. Don't want this to be assumed.
    -[BLOCKED: confirm possible redesign of backend switching and/or mock funcs] make dist and upload to pypi w/ new query_gpt3 changes
-add support for banana.dev version of gptj (see ipy in tab 1)
X -add uses of _maybe_choose_person to other settings funcs
X -add str(int) versions of model_i values for changeModel (observed some errors where model change failed or wrong intent was chosen because transcription used "2" instead of 2).
X -debug new unexpected uses of gpt3 vs. gptj
    UPDATE: realized when I change code, flask refreshes some things but doesn't call launch() automatically. So app state does not fully refresh. Set debug=False since refresh func doesn't work before app.run() is called and that call is blocking.
X -make changeModel set mock_func=None when changing engine_i to an int (realized if prev was gpt-j, changing to engine_i=0 wasn't removing that and so we just queried gptj and igonred the engine_i arg)

3/12/22 sat
-----------
~ -figure out best interface to use gooseai (just use query_gpt3 w/ diff backend? Kind of annoying that I have to pass mock_func in some cases, but in this version I change the backend. Maybe can adjust BackendHandler and/or other query_gpt_{x} funcs to handle everything automatically. Maybe need more separation between the concept of a backend (e.g. openai, gooseai, banana, huggingface) and model (e.g. openai-davinci, gptj-6b, gooseai-6b, huggingface-6b))
    UPDATE: yes, keep backend and model separate. Maybe can adjust neo function to accept engine_i too (with gpt-j-6b, they technically have 4 solid options now too).
    X -create new changeBackend intent in console
    X -create changeBackend endpoint
    ~ -test
        UPDATE: initial attempt failed to recognize endpoint but can't remember if I built model since most recent changes. Launched build and can test tomorrow.
    X -new backendselector functionality:
        X -get current api name
        X -get model name from user-specified engine_i and current api name
-easier option: look for docs in app.py, utils.py

3/13/22 sun
-----------
~ -test changeBackend endpoint in console
    UPDATE: mixed results. Switching to openai works more reliably than gooseai, and typing works more reliably than speaking. Speaking gooseai never worked - always just defaulted to SearchIntent. Decided to write my own routing logic as a fallback in case alexa doesn't recognize certain intents.
X -write function to convert json editor -> fuzzy dict mapping utterance to intent. (handle all combos of sample slot values for all sample utterances for each intent)
    UPDATE: saved in data/alexa dir.
-update query_gpt_neo function in jabberwocky.openai_utils
    -support 6b param model
    -see if we can adjust interface to use engine_i rather than size
-gui updates?
    -decide if that's within the scope of this project. Could always pin jabberwocky version for gui.
    -if yes to prev point, adjust gui accordingly (cur offer different radio buttons for each neo size; could instead have 1 radio and have it actually use the engine_i slider)
    -add gooseai backend option to gui?

3/14/22 mon
-----------
X -write infer_intent func to guess based on fuzzy dict simlarity results
    X -update delegate to use it when appropriate
X -write CustomAsk method to get func (not just name) from intent name
X -move prev_intent update to occur post pre-func logging but before post-func logic
~ -continue testing changeBackend in console
    UPDATE: some success w/ inferring intent, but need to figure out how to handle slot vals. Could try to extract them using regex and pass to funcs directly, or extract and then mutate slots object, or pass in whole str and use some sort of partial_token_set_ratio or scorer that supports partial matches, or always raise error.
    -for diff utterances, see what matching intents are printed out by fuzzykeydict
    -see what common similarity scores are
    -determine a good way to map from similar intents to an actual choice (i.e. do all need to match? Do we just care that the top one is over some similarity threshold, and if so what? Do we take some weighted score, e.g. if top match is 80% changeBackend but #2 and #3 are both 79% changeModel?)
        -what other logic (if any) do we need to implement to make this work? In some cases we may want to trust the queued function, in some maybe not? Might also be able to hardcode some logic, i.e. "startswith(Lou)".
    -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -try some other intents and assess routing logic quality.
    -maybe assess whether n=3 is a good choice for similar keys

3/15/22 tues
------------
~ -Consider how to handle slot vals when calling an inferred function from delegates(). Could try to extract them using regex and pass to funcs directly, or extract and then mutate slots object, or pass in whole str and use some sort of partial_token_set_ratio or scorer that supports partial matches, or always raise error.
    UPDATE: decided to try manually extracting slots - atm, there are only 5 intents w/ slots and most seem pretty feasible to extract. Wrote draft function for names (choosePerson), numbers (maxLength, temperature), and backend (e.g. gooseai).
-continue testing changeBackend in console
    -for diff utterances, see what matching intents are printed out by fuzzykeydict
    -see what common similarity scores are
    -determine a good way to map from similar intents to an actual choice (i.e. do all need to match? Do we just care that the top one is over some similarity threshold, and if so what? Do we take some weighted score, e.g. if top match is 80% changeBackend but #2 and #3 are both 79% changeModel?)
        -what other logic (if any) do we need to implement to make this work? In some cases we may want to trust the queued function, in some maybe not? Might also be able to hardcode some logic, i.e. "startswith(Lou)".
    -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -try some other intents and assess routing logic quality.
    -maybe assess whether n=3 is a good choice for similar keys

3/16/22 wed
-----------
X -decide if that's within the scope of this project. Could always pin jabberwocky version for gui.
    UPDATE: no, just pin version of jabberwocky for gui requirements.txt and make a separate one for alexa.
X -update project files to account for decision to pin gui to diff version of jabberwocky
    X -make 2 requirements files
    X -add make install_dev cmd
    X -update readme accordingly
-update query_gpt_neo function in jabberwocky.openai_utils
    X -support 6b param model
    X -see if we can adjust interface to use engine_i rather than size
    ~ -test how performance (speed, reliability) compares to free gpt-j api. Trying to decide if one or both can be deprecated. (Would make settings easier to handle and generally more intuitive if "model" slot was always a number.)
        UPDATE: gptj api is down atm. Neo function appears to work, though slowly - haven't timed yet since gptj is down.
~ -look into adapting openai Completion api to use neo and vicgalle as backends
    UPDATE: I'm sure we *could* do this but after a bit of digging, it seems sufficiently complicated that I I'm not sure it's the easiest way. Might be easier to have backend provide a query() method that inserts mock_func into query_gpt3 kwargs and use that in place of query_gpt3? But that may not work with convmanager/promptmanager. Could update those, of course.
_ -write slot extraction func for chooseModel (see bottom of alexa/utils.py for other sample slot extraction funcs)
    UPDATE: plan is to make huggingface and vicgalle backends rather than models, so model can always be a number. No need for a new parsing function.
    -consider interface. Should this extraction occur w/in each func separately, or create 1 SlotExtractor that does this in delegates() after inferring a function?
    -implement chosen method
-continue testing changeBackend in console
    -for diff utterances, see what matching intents are printed out by fuzzykeydict
    -see what common similarity scores are
    -determine a good way to map from similar intents to an actual choice (i.e. do all need to match? Do we just care that the top one is over some similarity threshold, and if so what? Do we take some weighted score, e.g. if top match is 80% changeBackend but #2 and #3 are both 79% changeModel?)
        -what other logic (if any) do we need to implement to make this work? In some cases we may want to trust the queued function, in some maybe not? Might also be able to hardcode some logic, i.e. "startswith(Lou)".
    -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -try some other intents and assess routing logic quality.
    -maybe assess whether n=3 is a good choice for similar keys

3/17/22 thurs
-------------
-updates to reflect recharacterization of neo and j as backends rather than models
    X -in console, rm J and Neo as model options
    X -in console, add j and neo as backend options
        UPDATE: renamed J to Vic since it specifically refers to the vicgalle api (I assumed this was a name Vic Galle but I don't actually know. Not important though.).
    X -update saved fuzzy dict from new skill json (see func in alexa/utils)
    ~ -update app.py to account for changes
X -add slot func option to intent decorator and update intent funcs accordingly
-consider how to support j and neo as backends in BackendSelector
    -implement

3/18/22 fri
-----------
X -consider if any intents should be removed from fuzzy dict of inferrable intents (e.g. Yes, No, ones without slots?)
    _ -implement
    UPDATE: Yes and No are already not included. Other intents without slots could still be worth inferring so leave those in.
X -update slot processing code for each intent in fuzzy dict (don't worry about composing slot funcs yet, just update signatures)
    X -changeModel
    X -readContacts
    _ -emailMe
        UPDATE: intend to rm this eventually anyway.
    X -changeTemperature
    X -changeMaxLength
    X -readSettings
    X -enableAutoPunctuation
    X -disableAutoPunctuation
    X -endChat
    X -changeBackend
X -consider how to handle intents w/ multiple slots (maybe need to compose funcs like get_num, get_scope)
    UPDATE: change slot_func to slot_funcs which maps each slot name to a function.
    X -implement
    UPDATE: overhauled whole slot_func interface. Specify using type annotations, then make ask.intent auto-construct the slot parsing func based on that. Haven't tested yet outside of toy examples in ipython.
-work on new get_backend func (jump off of alexa/utils version) to handle new vals (see fuzzy dict "fd" in ipython sess below this pane)
     -consider approaches: __contains__, fuzzywuzzy, fuzzyregex or spacy matcher, gpt infer
     -implement
-test each inferrable intent in console (e.g. type something like "Lou, change backend to Open I" or speak "Lou change backend to goose AI" (hard for it to recognize))

3/19/22 sat
-----------
~ -work on new get_backend func (jump off of alexa/utils version) to handle new vals (see fuzzy dict "fd" in ipython sess below this pane)
     ~ -consider approaches: __contains__, fuzzywuzzy, fuzzyregex or spacy matcher, gpt infer
     ~ -implement
    UPDATE: tried a ton of stuff combining str similarity, phonetic similarity, gpt prompting, etc. Still haven't quite gotten something satisfactory.
X -update valid backend slot utts
    X -rename vic -> hobby (real word, not a name)
    X -regenerate fuzzy dict
-work on get_scope function
-test each inferrable intent in console (e.g. type something like "Lou, change backend to Open I" or speak "Lou change backend to goose AI" (hard for it to recognize))
    -test ability to extract slots in each case (above task refers to our ability to infer intents, which is a distinct step)

3/20/22 sun
-----------
-adjust changeBackend intent func - slot utts allow both with and without spaces. Need to account for that.
~ -investigate phonetic similarity->str sim approach in ipython below. Why all score=86? Maybe need diff str sim method.
    ~ -go through ideas in misc.txt until I find a get_backend I'm happy with
    ~ -look into Adrienne idea re formnat values
        UPDATE: lots of missing values in table I found compared to eng_to_ipa encoding. Also couldn't quickly find good way to encode consonant similarity.
    ~ -try patternomatic spacy package
        UPDATE: lots of code issues, seems like not compatible w/ current version of spacy.
    UPDATE: fairly promising method is to store slot vals used for each utt and then just use those from closest utt. Avoids lots of extraction hassle, but might not have really solved the poor transcription issue. That might be better solved with gpt3 - perhaps asking it to correct spellings will work better.

3/21/22 mon
-----------
X -update infer_intent to provide slots in weighted case (see TODO)
_ -prototype new gpt prompt asking to correct spelling rather than correct+extract all in 1
    UPDATE: let's hold off on this until I see more evidence that it's actually needed. Maybe new method will work already.
X -write deprecated deco and mark slot extraction funcs as such
    UPDATE: put this in htools rather than jabberwocky.
X -adjust changeBackend intent func - slot utts allow both with and without spaces. Need to account for that.
    UPDATE: changed BackendSelector to lowercase input and remove spaces. Think that should be sufficient.

3/22/22 tues
------------
~ -consider how to support hobby and huggingface as backends in BackendSelector
    X -implement
    UPDATE: wrote something that I think should work but needs much more testing.
-continue testing changeBackend in console
    -is the intent getting recognized?
    -if not, what intents (if any) are getting inferred?

3/23/22 wed
-----------
~ -test backendselector more in jupyter (not ipython, want to see docstring)
    X -do base and key change when we expect them to?
    X -does query() show docstring as expected?
        UPDATE: had to add modified fastai decorator but now it works.
    ~ -try actual queries w/ all functions wrapped in debug. Make sure mock funcs are getting called when appropriate.
        UPDATE: Seems to work but both gptj and huggingface eleuther apis are down or deprecated - unclear.
X -renew goose.ai api key
-continue testing changeBackend in console
    -is the intent getting recognized?
    -if not, what intents (if any) are getting inferred?

3/24/22 thurs
-------------
X -rename query neo to match backend name
X -maybe rename backendselector class? No longer just a backend selector exactly.
X -make ls() (from nb) a method of backendselector?
X -add gptbackend method to get current mock_func
X -make other extra simple mock funcs supported by backendselector? With 2 apis down, there's no free option for testing atm. (Note: make sure you don't break query_gpt3 if so, since mock_funcs currently edit the object loaded by our regular mock response).
X -update convManager to use new backend.query api
X -update promptManager to use new backend.query api
    UPDATE: updated query and kwargs methods, then briefly kwargs tested in jupyter. Seems all good. But I then went back and made gpt.query a classmethod so need to test this again.

3/25/22 fri
-----------
X -test convManager and promptManager in jupyter after latest change to gpt.query (see last bullet point yesterday)
    X -convManager.kwargs
    X -promptManager.kwargs
    X -convManager.query
    X -promptManager.query
X -continue testing changeBackend in console
    X -is the intent getting recognized?
    X -if not, what intents (if any) are getting inferred?
    UPDATE: intent inference seemed to work very well initially (magically, no bugs!) but at end of session realized problem w/ weighted inference strategy.
~ -test each inferrable intent in console (e.g. type something like "Lou, change backend to Open I" or speak "Lou change backend to goose AI" (hard for it to recognize))
    _-test ability to extract slots in each case (above task refers to our ability to infer intents, which is a distinct step)
    _ -for diff utterances, see what matching intents are printed out by fuzzykeydict
    ~ -see what common similarity scores are
    ~ -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -maybe assess whether n_keys should change in fuzzy_dict.similar() call
-easier(?) task: add support for banana.dev version of gptj (see ipy in tab 1)
-fun task: adjust Ask to track how many calls deep we are for a single user turn and adjust logging indentation accordingly. Make it visually easier to see when an intent call is really done vs. when it was just a nested call inside another intent.

2/26/22 sat
-----------
[NOTE: currently have gooseai enabled for both punctuation and replies. Hobby backend still down and huggingface backend seems too slow to do 2 tasks (punct + reply) in time for alexa.]

X -add "repeat" option to dialog model in UI
    X -generate new utt2meta.pkl
X -update readSettings to include defaults (i.e. if user hasn't explicitly set Model, this intent won't read it)
    X -update
    UPDATE: adjusted settings object to store these by default.
    X -test in console
X -add makefile commands for run_alexa and ngrok (realized I forgot ngrok cmd after restarting computer)
-add a "back to your conversation" msg for when changing settings mid conv
    X -implement
    -test in console
X -fix weighted strategy in infer_intent (e.g. if all 5 closest matches are changeModel but the closest match is .4, we wouldn't want to delegate to that, but we do atm)
    X -implement
    X -test
~ -troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    ~ -see if we can increase timeout
        UPDATE: sounds like default timeout is 10sec for skill to provide a response (incidentally, user has 8sec to provide a response) and these values are not configurable. However, it may be possible to hack a workaround using progressive responses, but this would not be ideal for every response. I.e. if we know a longer completion is coming, we could probably have alexa say "hmm, let me think about that" and then follow up with the real response. But we don't want alexa to say that every turn.
~ -adjust Ask to track how many calls deep we are for a single user turn and adjust logging indentation accordingly. Make it visually easier to see when an intent call is really done vs. when it was just a nested call inside another intent.
    X -implement
    ~ -test
        UPDATE: didn't see any indentation yet but I didn't check thoroughly if the sequence of actions/intents called for it. Do this tomorrow.
        

3/27/22 sun
-----------
X -test call stack logging more thoroughly in console (does it indent messages when it should?)
    UPDATE: ended up taking different approach since indenting was not affecting other logged statments - wrote custom formatter to address this. Also updated on_end to show which intent just ended since it wasn't before. Non intent-callback-induced log statements are indented more than intent-callback-induced log statements but I think that's okay and some of those (e.g. slots, the most common and annoying) will likel be removed anyway at some point.
-investigate potential issue where ' Me:' is getting attached to end of gpt responses. Unsure if weaker model is skipping the double newline and therefore not being caught by the stop phrase, or if conv.me set op isn't working right, or if gooseai includes the stop phrase(s) in the response.
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    -check if openai supports this too or just gooseai
    -implement
-update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    -check which backends support this
    -implement

3/28/22 mon
-----------
X -move logging to query_gpt3 or GPTBackend.query (rather than in conv/prompt managers only)?
    _ -first investigate if this is already done natively - vaguely recall seeing something in package code suggesting it was
    UPDATE: whoops, forgot to check for native solution. But already finished this so I guess it's fine.
~ -investigate potential issue where ' Me:' is getting attached to end of gpt responses. Unsure if weaker model is skipping the double newline and therefore not being caught by the stop phrase, or if conv.me set op isn't working right, or if gooseai includes the stop phrase(s) in the response.
    UPDATE: tentatively confirmed that gooseai does not truncate before stop word. Started refactoring this functionality out of huggingface mock func but found new issue re partial stop phrases. Documented in nb 12 for tomorrow.
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    -check if openai supports this too or just gooseai
    -implement
-update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    -check which backends support this
    -implement

3/29/22 tues
------------
X -add more params to query_gpt3 and update documentation
~ -finish stop phrase truncation function in jupyter
    X -support truncating on partial stop phrases when completion ended with reason "length"
    ~ -check which backends provide stopping 'reason' (affects which we can rm partial stop words for)
        UPDATE: openai and gooseai do. Haven't confirmed others yet but I doubt they do.
    -refactor functionality out of huggingface mock func
    -figure out how to do this for gooseai (does it need a mockfunc that just ONLY does this? Or should we put this GPTBackend.query and skip it depending on the current backend?)
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    -check if openai supports this too or just gooseai
    -implement
-update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    ~ -check which backends support this
        UPDATE: openai does w/ param n, same as gooseai. Still need to check HF and Hobby.
    -implement

3/30/22 wed
----------~
~ -update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    X -check which backends support this
        UPDATE: gptj still broken but from docs, looks like it does NOT support this natively. 3 others do though.
    -implement
~ -update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    X -check if HF and Hobby (J) backends implement this (openai and gooseai do w/ param n)
        UPDATE: gptj still broken but from docs, looks like it does NOT support this natively. 3 others do (wip hf func includes this, need to remap name from n->num_return_sequences).
    -implement
-stop phrase truncation function
    -refactor functionality out of huggingface mock func
    -figure out how to do this for gooseai (does it need a mockfunc that just ONLY does this? Or should we put this GPTBackend.query and skip it depending on the current backend?)
    UPDATE: realized I need a better understanding of all the diff api response objects first (e.g. which offer finish_reason, what do responses look like when n_completions>1 and/or when n_prompts>1?). These factors will influece how to refactor. 
-allow (or force?) query_gpt3 to make mock functions overwriting other parts of mocked response besides text, e.g. finish_reason and logprobs. (Currently only text is changed.)
X -allow GPTBackend mock_func and engine methods to specify a backend besides current
    UPDATE: realized this was a problem when calling mock funcs directly rather than through backend.query. Now we're sure hf mock func uses right engine based on i.
X -add gptbackend.backends() method for convenience

3/31/22 thurs
-------------
X -add support for banana.dev version of gptj
    UPDATE: will need additional work to support stop phrases, etc. but all the funcs will undergo changes when that happens.
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 AND n_completions > 1
    ~ -consider desired interface. What should be returned when np=1 and nc=1, np>1 and nc>1, np>1 and nc=1, etc.
        UPDATE: wrote some sample usage code in nb12 markdown cell. Still undecided. Considering larger rewrite - the whole mock_func scheme seems kind of annoying. Might be easier to have GPTBackend just choose the right query_func and forget about the "passing a mock func to query_gpt3" paradigm. Also used mark decorator to label which funcs require manual stop phrase truncation - I think that will help refactor that step out.
    -examine responses from various and consider how best to map from these to desired interface
    -implement
~ -stop phrase truncation function
    UPDATE: started noodling on what this could look like. See end of GPTBackend.query method, which uses attr from mark() decorator to do exra truncation if necessary.
    -refactor functionality out of huggingface mock func
    -figure out how to do this for gooseai (does it need a mockfunc that just ONLY does this? Or should we put this GPTBackend.query and skip it depending on the current backend?)

4/1/22 fri
----------
X -add banana backend in aws console
    X -re-generate json and save (remember to save both raw json and generated')
-continue considering query redesign 
    X -settle on removing mock_func construct or not (try to think: is there really a good reason for this?). If yes:
        X -decide what each func should return
            UPDATE: decided to let each function return either (str, dict-like) OR (list[str], list[dict-like]). This allows for my most common use case so far (n_prompts=1, n_completions=1) to continue having a convenient interface w/out extra indexing, while also supporting a straightforward interface for n_completions > 1. Tentatively decided to handle n_prompts > 1 and do that for all funcs using threading/multiprocessing (these completions should be indepedent so no NEED to use the api built-in implementation). Plan is to have a separate GPTBackend.query_multi method where we return a list of tuples. This way we can unpack each completion as (text, resp) rather than the weird zip stuff that forcing this to fit my current interface would entail.
        ~ -start implementing
            UPDATE: tentatively updated each mock_func in nb, but still some cleaning up to do.
        -make sure redesign doesn't break ConvManager or PromptManager too badly (I'm okay with updating these - the api will clearly change too much to be compatible w/ the gui anyway - but make sure stuff like the hooked generator doesn't TOTALLY break in hard to solve ways.) Might want to change how we get kwargs(), i.e. the default query function could change and we should use the active one.
        -add check in GPTBackend.query for if we need to manually trunc stop words

4/2/22 sat
----------
~ -gpt query redesign
    X -finish implementing each mock_func in nb
    X -port to lib
    -update docs for various funcs
    ~ -polish draft of backend query method so it:
        ~ -does the warnings that used to be in query_gpt3
            UPDATE: added some, prob need more.
        X -handles the warning/error if user tries to pass in a list of prompts
        X -does post-query stripping (see gpt3 query func in pycharm, which I removed from current nb version)
        X -does post-query stopword truncation when appropriate
        X -test changes so far
            UPDATE: good so far. Still need to test stream mode.
        -update convmanager (careful w/ hookedgenerator)
            -maybe change kwargs() depending on active backend?
        -update promptmanager 
            -maybe change kwargs() depending on active backend?
X -update truncate_at_first_stop to make full trunc and partial trunc both optional
    X -test

4/3/22 sun
----------
~ -gpt query redesign
    -update docs for various funcs
    ~ -polish draft of backend query method so it:
        -add more warnings that used to be in query_gpt3
        X -write Thread that returns value
            UPDATE: may need to adjust this to ensure order is correct. Also, logging may not be threadsafe? Unsure.
            X -check if I log results with logger or htools.save
                X -check if the used method is threadsafe and/or multiproc-safe
                X -check if the alternative is threadsafe and/or multiproc-safe
                UPDATE: I use htools.save, which is not thread-safe. Logging is, though we couldn't use jq anymore to view pretty results, which is annoying. Can use threads if I use a lock - need to look into these more.
            -update to return in order (may require adjustments depending on bullets above)
        ~ -flesh out streaming mode
            X -think more about desired interface
            ~ -test stream mode
            X -write generic stream func for backends that don't support it
                UPDATE: currently supports np=nc=1 mode and np=1,nc>1. Unsure about others yet.
            X -check how backends w/ streaming support handle np>1 situations if at all
                X -check goose docs
                X -check openai docs
                UPDATE: both seem to support it, though looks like I'll need to try it to confirm structure of response.
                X -run func calls if necessary
                    UPDATE: first tried w/ nc > 1 (maybe unintentional? I forget.). Gooseai seemed to work but openai responses seemed to jumble the tokens without an obvious way to reconstruct each completion. If that's true, 2 of the 3 options with np>1 or nc>1 would be impossible to implement, so I decided to just make it simple and say stream mode is only supported when np=nc=1.
                    UPDATE 2: above is not true after all. They provide res['choices'][0]['index'] which seems to point to which completion the new token belongs to. Gooseai provides this too, they just also return completions in order (e.g. all 0s, then all 1s, etc.).
              X -update backend.query to use new mock stream func
              ~ -write new mock stream func to handle when query return val is (list, list) rather than (str, dict)
                    UPDATE: need to check if it needs to handle empty lists differently and figure out how to integrate into query.backend, but the basic functionality seems to be there.
        -update convmanager (careful w/ hookedgenerator)
            -maybe change kwargs() depending on active backend?
        -update promptmanager 
            -maybe change kwargs() depending on active backend?
        -think about moving hardcoded params to backend.query and leaving most new funcs to accept mostly kwargs?
        -general cleanup of mock_funcs, backend methods, etc. - very messy atm
X -give gpt_repeat func ability to return n>1 completions

4/4/22 mon
----------
~ -gpt query redesign
    X -test if new stream_multi_response in nb handles empty lists okay (or needs to)
    X -integrate into backend.query (replace stream_response? Or select 1 depending on returned type?)
        UPDATE: repeat, huggingface, and banana all work with stream mode now. Confirmed stream mode works w/ banana, repeat (both nc=1 and nc>1), and huggingface (both nc=1 and nc>1).
X -update returnable thread to return query responses in order (or write diff way to do this)
    UPDATE: wrote something to do this if we pass in a dict but turns out it works even without that! Guess join() blocks and ensures we return in order.
    ~ -start implementing query_batch for np > 1 (use ReturningThread).
        UPDATE: So far only tested this on funcs w/ no native streaming mode. (np>1, nc=1, stream=False), (np>1, nc>1, stream=False), and (np>1, nc=1, stream=True) seem good so far. Still need to test (np>1, nc>1, stream=True) - or do we? Didn't I decide that had no forseeable use case?
-update backend.query logging to file to work w/ threads (or whatever method I use in above bullet point)
X -email openai about api key exposure

4/5/22 tues
-----------
X -generate new openai key
X -generate new gooseai key
X -adjust gpt.ls() to not expose api key
X -write some sort of script to check for api key exposure before pushing/committing to git
_ -fix requirements-dev missing jabberwocky version
    UPDATE: realized this is intended. requirements.txt DOES already pin the version.
-qpt query redesign
    -query_batch
        -update backend.query logging to file to work w/ threads (or whatever method I use in above bullet point)
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/6/22 wed
----------
X -add line in readme about using make hooks
    X -add line mentioning diff versions for gui and alexa? Or could wait til I have scripts to handle alexa dev.
-qpt query redesign
    -query_batch
        -update backend.query logging to file to work w/ threads (or whatever method I use in above bullet point)
            X -write JsonlinesFormatter
            X -write JsonlinesLogger
            UPDATE: decided writing custom logger might be good way to go since logging (IIRC) natively supports multithreading. Could have used lock approach but let's try this first. Also like the idea of keeping all calls rather than overwriting each time.
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/7/22 thurs
------------
X -update gptbackend to log using jsonlines logger instead of htools.save
    UPDATE: had to update JsonlinesFormatter to allow logfile name switch, but after that it seems to work well.
-qpt query redesign
    -query_batch
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/8/22 fri
----------
X -easy: clean up all the old messy query funcs in openai_utils.py (mostly deleting)
-qpt query redesign
    ~ -query_batch
        X -in nb13, test if new logging functionality works with multi-threaded approach
            UPDATE: found bug where logging to file fails silently if file DNE. Added `touch` func and check if logfile exists before logging. Also had to add a threading Lock here, which probably means I could switch back to htools.save if I wanted since GPTBackend has a lock assigned now anyway. Would have to update htools to be compatible w/ jsonlines which I think would be good to support generally, and I think this would remove a lot of the icky logic around recreating JsonlinesFormatter every query. Downside: if I want to print kwargs to stdout, that would now need to be a separate step from logging to file. I think this would mostly be useful if I change logic to fully resolve kwargs w/ current query func - right now it just logs the user-specified params.
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/9/22 sat
----------
[NOTE: most up to date version of GPTBackend is in nb13 cell]
_ -update htools.save to support jsonlines format
    UPDATE: looked into options for implementing this and it's not super straightforward with the current interface. Could add jsonlines lib but I'd like to cut down on deps if anything.
-qpt query redesign
    -query_batch
        _ -consider switching to use htools.save in log_query_kwargs? See discussion in yesterday's daily notes.
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    ~ -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        UPDATE: think maybe can just return the native response (note: can't yield from since that makes python treat the method as a generator and all responses are lazily evaluated, even in stream=False mode).
        ~ -implement
            UPDATE: very simple for reasons described above and need to test more, but I think this should work.
        -build default support for nc>1 for backends that don't provide it
            UPDATE: solution is to make all of those backend query funcs return (str, dict) and not provide n in params. Then use new threaded_starmap function to make nc calls. Tentatively seems to work, though I'm surprised - query_batch also uses threads and I read threads within threads are not possible. UPDATE 2: actually might have spoken too soon. Investigate tomorrow.
X -fix new threading bug after deleting files
    UPDATE: realized I needed to move lock to include a bit more logic. Might just want to lock the whole method if another bug pops up in the future, but seems ok for now.

4/10/22 sun
-----------
-query_batch
    X -investigate: does np>1 and nc>1 actually work for funcs w/out native support? Unclear. See cell 129 in nb13.
        UPDATE: unable to reproduce error. Seems fine.
    ~ -adjust interface so returned val has same format for backends w/ native support for np or nc > 1 and those without (realized gooseai and maybe openai charge by the request too, so it's more efficient if we can use the native functionality when possible instead of always using multithreaded approach).
        UPDATE: WIP. Adjusted query_batch to return (texts, fulls) instead of old behavior so we now match openai/gooseai. Still need better testing.
    ~ -add prompt_index (decided rather than overriding openai/gooseai index to create prompt_index and completion_index, just keep index (which is effectively "overall_index") and add prompt_index.)
        UPDATE: seems okay for some cases but still very buggy. See query(str, n=1).
    -write tests in new clean nb to figure out which backends/kwargs combos work (so many variants, need to do programmatically)
        -start with all free backends to root out simplest bugs
        -then do paid backends
            X -write script to save sample response from all relevant combinations of params
                UPDATE: did this for gooseai. Shouldn't need to for openai since it's nearly identical (only known difference I recall is that in streaming mode, openai may shuffle completions together, which I account for elsewhere).
    -figure out how to integrate into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
        -implement

4/11/22 mon
-----------
-query_batch
    X -start new clean nb and use pickled responses in data/misc instead of adhoc
    _ -try passing each pickled response through postprocess_gpt_response
    _ -try passing each of those ^ outputs through postprocess_response
        UPDATE: wrote new query_gpt_mock func that makes it easier to get fake openai/gooseai responses.
    -add prompt_index to all gooseai/openai responses
    ~ -write tests in new clean nb to figure out which backends/kwargs combos work (so many variants, need to do programmatically)
        X -start with all free backends to root out simplest bugs
        -then do paid backends using pickled responses
    X -figure out how to integrate into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
        ~ -implement
        UPDATE: query() calls _query_batch under the hood and _query_batch calls query n times (str prompt each time instead of list).
X -fix bug in stream_response
    UPDATE: itertools.cycle was returning the same dict repeatedly. We want it to return a new dict at each step.
X -kwarg_grid func to iterate over all relevant kwarg combos

4/12/22 tues
------------
~ -look into issue where logfile is being generated in notebooks dir
    UPDATE: could not reproduce.
X -add prompt_index when stream=True
    X -figure out what the issue is
        UPDATE: prompt_index was only assigned in query method, but we return early when stream=True. Needed to add logic to stream_multi_response().
    X -implement
~ -add support for backends w/ builtin batch queries? (Recall: at least w/ gooseai, it's cheaper to make 1 query w/ 100 prompts than 100 queries w/ 1 prompt each)
    UPDATE: fiddled around a bit and it seems like maybe query should work anyway? Probably easier to diagnose by just trying it.
~ -[BLOCKED: need to figure out if the above bullet is necessary first. Not sure if we can proceed with this yet.] test_backend('mock') and examine results
    UPDATE: initially failed on first set of kwargs so I switched kwarg grid func to do them in an easier order. Bug turned out to be I was checking if n the number rather than 'n' the param name was in query signature. Now they all execute BUT doesn't work correctly when np > 1. Seems to repeat completions - I sense it's related to the fact that query_batch is still being used on it. So we're making 2 calls to query_mock w/ n=1 rather than 1 call to it with n=2, which is why we get the results for the n=1 response. Fix this tomorrow.

4/13/22 wed
-----------
X -update query to only use query_batch if the backend doesn't natively support list prompts (or update query_batch to handle that itself)
~ -make sure prompt_index is correct when stream=True
    X -works for backend does NOT support stream natively case
        UPDATE: added assert in nb test_backend() func to check this. Repeat and banana pass.
    X -works for backend DOES support stream natively case
        UPDATE: wrote new stream_openai_generator func. All tests pass.
    -test huggingface backend
        UPDATE: tried to run but unsurprisingly got 400 error. Maybe if I manually insert wait times or alternate between engine_i values it would work? Try again later.
    -test gptj backend (if back up)
        X -check if back up
        -[BLOCKED: api down] test
-clean up query/query_batch so prompt_index isn't set in so many places (currently 3? end of query, end of query_batch, stream_mluti_response)

4/14/22 thurs
-------------
X -notebooks/data/logs is back. Want to stop this from happening.
    X -Try to reproduce error and identify cause
    X -fix
    UPDATE: because I made jsonlogger a class attribute, the path was resolved before I ran cd_root, i.e. on import. Added C.root var in config. Used this to update load_prompt func and PromptManager so paths are a little less fragile (arguably).
X -investigate what happens when 1 returningthread fails. Think it might not raise the error to the calling context.
    X -try w/ toy example (no query gpt)
    UPDATE: simply returns None in the corresponding list index. Switched thread_starmap to use PropagatingThread by default, though ReturningThread is still an option.
X -think about moving hardcoded params to backend.query and leaving most new funcs to accept mostly kwargs?
    UPDATE: No. Wrote explanation in query_gpt3 docstring.
X -general cleanup of mock_funcs, backend methods, etc. - very messy atm
    X -better document what these funcs need to return
    UPDATE: wrote pretty thorough section on how to implement a new backend func in the module docstring.
-clean up query/query_batch so prompt_index isn't set in so many places (currently 3? end of query, end of query_batch, stream_mluti_response)
_ -check if there are any more warnings from query_gpt3 that need to be moved to gpt.query()

4/15/22 fri
-----------
X -clean up query/query_batch so prompt_index isn't set in so many places (currently 3? end of query, end of query_batch, stream_mluti_response)
    UPDATE: technically it was 4 before and I got it down to 3 (removed query_batch one). Also refactored stream_response (formerly stream_multi_response) to include stream_openai_generator (the 4th case I previously forgot) so it's sort of like 2 places.
-repercussions of gpt.query redesign
    -update convmanager (careful w/ hookedgenerator)
        -maybe change kwargs() depending on active backend?
-repercussions of gpt.query redesign
    -update convmanager (careful w/ hookedgenerator)
        -maybe change kwargs() depending on active backend?
    -update promptmanager 
        -maybe change kwargs() depending on active backend?
X -fix docs in stream_openai_generator explaining how calculation works

4/16/22 sat
-----------
X -clean up misc todos in alexa/app.py (rm old commented out code, etc.)
    X -update app to use banana.dev by default for now
X -update engine names in config
    X -add banana.dev engine names
    X -replace hobby empty strings with engine names
X -document alexa.utils.slot function
X -document a couple CustomAsk methods
-make logging use fully resolved kwargs?
-handle error when wiki person generation fails
-repercussions of gpt.query redesign
    -update convmanager (careful w/ hookedgenerator)
        -maybe change kwargs() method depending on active backend?
    -update promptmanager 
        -maybe change kwargs() method depending on active backend?

4/17/22 sun
-----------
X -add datetime to log meta
X -make logging use fully resolved kwargs?
    UPDATE: investigated this and decided this is no different - individual query_func defaults are basically all overridden by gpt.query defaults.
X -update convmanager and promptmanager kwargs()
    UPDATE: rm refs to mock_func, use GPTBackend.query for bound args. Also added keep=True to with_signature deco so kwargs that are not in query_gpt3 are accepted in bound_args().
-handle error when wiki person generation fails
X -repercussions of gpt.query redesign
    X -update convmanager (careful w/ hookedgenerator)
        _ -maybe change kwargs() method depending on active backend?
    X -update promptmanager 
        _ -maybe change kwargs() method depending on active backend?
~ -test convmanager
    UPDATE: after some fixes to handle new list outputs, seems to work. One remaining issue: confirmed my suspicion that stream mode does not respect my stop_words.

4/18/22 mon
-----------
-test promptmanager
-handle error when wiki person generation fails
-make stream mode respect stop words


Backlog
-------
-[BLOCKED: api down] test gptj backend using nb14's test_backend func
-[BLOCKED: possible rate limiting? 400 client error again] test huggingface backend
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary
-adjust prompt files to be stored in a single file instead of 2? Tried to do this initially but I recall running into problems with newlines and/or quotes. I think some other people have managed to do this so might be able to use those repos as guidance. This would be quite an involved process though:
    -port all prompts to new format
    -update load_prompt func
    -test that everything still works
-support custom persona (speak bio yourself)
-resolve 2 utterance conflicts (changePerson apparently soaking up 2 unintended commands in Hush and Oh No)
-update query_gpt3 to support codex, embeddings, etc. (not necessary here, probably, but good to do in general)
-get consent_card working on use_email endpoint (then refactor to have this automatically done on skill launch?)
-update end of conv to erase conv-level settings
    ~ -consider whether settings resolution order is right/desirable (i.e. should global level take priority or should conv level?)
        UPDATE: thought a bit and this is quite a complex task. Still not at a solution yet but my sense is that priority should be more about sequence (most recent = highest priority) and scope should just be thought of as "when do we undo this setting change action"? Need to hold off on other changes until I figure out desired system.
    -update change of person to change person-level settings
    -maybe update settings.clear() to allow clearing a single (or finite number) of state levels.

Easy Backlog
------------
-look for things to document

Long term backlog (i.e. things I decided are outside the current scope)
-----------------------------------------------------------------------
-update gui to support newer jabberwocky version
    -adjust gui accordingly (cur offer different radio buttons for each neo size; could instead have 1 radio and have it actually use the engine_i slider)
    -add gooseai backend option

