Context: been fiddling around with this project again lately but I'm not quite ready to restart daily contributions. Documenting where I'm at here for now so it's easier to pick up when I come back.

11/15/21 mon
------------
Left off
-talk() now prints responses with pretty formatted persona name.

Next Steps
-adjust query_kwargs so that we don't pass the wrong ones to gptj or gpt-neo mock funcs (maybe something like drop keys in query_kwargs that aren't in the func signature? Might not work if they take kwargs).
-write new mock funcs to support:
    -codex
    -new open source gpt-like model on modelhub (name is something about "sci" IIRC)
-live typing effect?
-consider a "choose from 1 of k responses" mode. Would need to think about how this would work w/ live typing and how it would work w/ conv manager (usually query() updates the conv history).
-maybe replace save/quit options w/ prompt_toolkit menus?

Context: picking up on this again for part 2 (Alexa skill).

1/31/22 mon
-----------
X -write premortem

2/2/22 tues
-----------
X -look through sample alexa conversation skeleton repo
    X -decide whether to use that template or "start from scratch"
        UPDATE: start by trying template.
X -create alexa skill in AWS UI
X -change invocation name
    UPDATE: needed to be 2 words and I wanted it to be short. Jabberwocky -> Voice Chat.
~ -start adding intents
    ~ -start adding slots
        UPDATE: wrote list of possible slots. added Person slot and Model slot.
    UPDATE: wrote list of possible intents in misc.txt. Added choosePersona intent.
-figure out how code will be structured locally (will alexa reference a separate repo if I use the conversation template? Will a single file basically be sufficient? Maybe an `alexa` subdir which can contain whatever I need it to?)
-follow UI instructions to determine next steps

2/3/22 wed
----------
~ -read up on dialog delegations strategy (do I want this?)
    _ -add to choosePersona if necessary
        UPDATE: alexa auto-determines next thing to ask? Most of my interactions should require minimal scaffolding so I don't think we need this.
~ -read up on intent confirmation (do I want this?)
    _ -add to choosePersona if necessary
        UPDATE: unnecessary at least for now. Consider adding later for Model or settings or choosePersona, maybe.
X -create more slots (see misc.txt)
X -create more intents (see misc.txt)
~ -see backlog
    X -fix incorrect usage of intents/slots 
        UPDATE: intents should use a var name and then you set the var type in the section below. Previously I was using the var type in the intent itself. I.e. "call {amazon.Person}" -> "call {Person}", where Person has type amazon.Person
    ~ -start testing interactions
        UPDATE: lots of failures. I realized my code has been filled in with their conv template but I need to make a lot of changes.

2/4/22 thurs
-------------
~ -look through code tab to see how expected interactions look
    X -try out in Test tab
        UPDATE: still having trouble getting this to work. Found docs for ask_sdk and decided to try to follow those locally.
~ -explore possibility of local ngrok
    UPDATE: not sure if this is compatible with lamda style app code. Table this for now and return to amazon hosted endpoint.
X -add code locally
    UPDATE: created new alexa dir.
-start following ask_sdk tutorial
    X -write launch handler
    ~ -write choose model handler
    ~ -write choose person handler
-update code to use my new intents
    -regular conversation
    -choose person
    -change max length
    -change temperature

2/4/22 fri
----------
-continue following ask_sdk tutorial
    X -finish choose person handler
        X -figure out best way to make conv manager available globally
            UPDATE: initially wrote wrapper class that delegates to an attr where is stores the manager, then realized it didn't seem to be doing anything useful. Just load the manager as a global var in the global scope. Can adjust strategy later if needed.
        UPDATE: tentatively done, though will likely need some troubleshooting/extra functionality/logging/error handling once I start testing.
    ~ -finish choose model handler
        UPDATE: tentatively mostly done, though need to figure out how to handle changing model to gptj or other non-int version. I think a better strategy than updating kwargs may be to store some session-level kwargs (I believe handler_input has some kind of session object that may be appropriate for this), then always pass that in to the query method.
X -write convenience func to access slots
X -write convenience func to speak/respond
~ -clean up template file a bit (delete unused classes)
_ -confirm stackoverflow-suggested method of extracting slots works (unclear - pycharm code completion stopped finding it)
    UPDATE: prob need to wait until I figure out how to run this thing. Template has a helper method that does something similar so that's an option too.

2/5/22 sat
----------
[DON'T LET THIS GET PUSHED BACK TOO FAR - don't want to end up having to rewrite a ton of code if something is incompatible.]
~ -figure out how to get this running on lambda (if possible: reliance on local files might be a problem)
    X -how hard would it be to convert this to something we can run with ngrok? Is lambda_function.py still compatible or is it totally different locally?
        UPDATE: decided to try rewriting with flask-ask. We're still early and it seems like dev/testing will be far easier (hopefully avoid slow build time for every change).
X -write new template app with flask-ask
X -try running flask app w/ ngrok
    UPDATE: accessible via generic url.
X -rebuild model 
    ~ -see if I can access flask-ask app through aws console
        UPDATE: no errors but still references console code rather than flask ask local code.
-look into handler_input session object
    -try using this to update kwargs (e.g. model, max len, etc.)
-more intent functions/endpoints:
    X -set temperature
    ~ -set max length
        UPDATE: added skeleton func.

2/6/22 sun
----------
~ -troubleshoot model build (no errors but still references console code rather than flask ask local code)
    ~ -try new flask-ask version in Alexa console (Test tab)
        UPDATE: spent most of time troubleshooting w/ minimal progress. Seems like the test console knows about the right URL (I think) but the api call is failing so it falls back to some generic message. Sample flask-ask code from aws tutorial has the same problem - possible that flask-ask is just no longer compatible 😬. When using test console, I see failed requests show up in app (not to any specific endpoint, looks like?).
-more endpoints:
    ~ -set max_length endpoint
        UPDATE: still need error handling though.
    -conversation end (add extra question offering to email transcript to user?)
    -generic conv response endpoint
X -fix logger (wasn't printing to stdout)
    UPDATE: unsure if fix was related to changing logging level or just restarting app, but it works now.
X -read up on flask-ask session vs. context (readthedocs.io tab)
    X -update it to store default _kwargs attr from conv manager
    X -update chooseModel to use this
    X -update chooseMaxLength to use this
    X -update chooseTemperature to use this

2/7/22 mon
----------
X -add error handling to max_length endpoint
-add more endpoints:
    ~ -conversation end (add extra question offering to email transcript to user?)
        UPDATE: started writing but it's a bit tricky with optional saving.
    X -generic conv response endpoint
    X -debugging intent (repeat user response back)
X -first draft of func to save conv (want it emailed rather than local)
-brainstorm: what else could possibly cause these alexa issues? How to troubleshoot and eventually fix?
-see if there are places I should add reprompt (chained method after question)

2/8/22 tues
-----------
_ -new endpoint for user to set email
    _ -add intent in console
    UPDATE: think I found a way to do this via amazon API rather than through voice.
    X -function to get user email using Amazon API
        UPDATE: untested though because it requires Alexa context object to run, which means I need to get the test tab working to see if it works.
-update other endpoints to check if should_exit
-rename `exit` function to avoid clobbering builtin
-brainstorm: what else could possibly cause these alexa issues? How to troubleshoot and eventually fix?
-see if there are places I should add reprompt (chained method after question)

2/9/22 wed
----------
X -rename `exit` function to avoid clobbering builtin
~ -see if we can easily update htools.quickmail to include text attachment
    UPDATE: can't fully test yet bc of freedom app, but seems to send without error at least. Though worryingly, text never arrives to phone when I tried that using {phonenumber}@vtext.com.
X -make new jabberwocky email for sending transcripts
    X -add info to htools creds file
    X -enable unsafe app access
    X -update email_user func to send transcript as attachment
-see if there are places I should add reprompt (chained method after question)

2/10/22 thurs
-------------
X -check hmamin2 email to see if quickmail works as expected (did emails arrive? did image attachment work? Did text attachment work? What is attached text file named?)
    UPDATE: email worked, sms didn't. Text just got appended to body though, not a real attachment.
    -update to allow multiple attachments
    X -investigate lack of sms showing up in text mode
        UPDATE: do not use leading 1 in phone number email.
    X -look for solution to lack of image attachments in text
        UPDATE: you need to use a different phone number email domain than vtex.com for mms, but it looks like it may not be supported anymore.
    X -update docs to reflect new signature
    -[BLOCKED: check emailed results first to see if attachment worked. Consider adding option to attach vs. embed.] bump version and upload to pypi
~ -look in alexa dev forum for how to direct 1 intent to another (prob need to use session to set temporary key)
    UPDATE: sdk does support "intent chaining". Unclear if flask-ask does, either natively or via compatibility w/ sdk.
    -update other endpoints to check if should_exit

2/11/22 fri
-----------
X -check hmamin2 email to see if quickmail works as expected (did emails arrive? did image attachment work? Did text attachment work? What is attached text file named?)
    _ -fix as needed
    X -[BLOCKED: check emailed results first to see if attachment worked. Consider adding option to attach vs. embed.] bump version and upload to pypi
    UPDATE: emails arrived, image attachment worked, no diff between image attach vs. inline, text attachment worked, text file is named same as source file as intended.
X -update curl to have verbose/non-verbose option
~ -read more of intent chaining tutorial
    UPDATE: not worth getting too far into this yet - wanted to make sure flask-ask provided some way to do this first.
    X -look for how to implement this w/ flask-ask
        UPDATE: can import delegate just like question or statement, but still figuring out how to use it.
    -update other endpoints to check if should_exit
X -update intent decorator to always store prev intent in session
~ -figure out how to test flask-ask locally
    UPDATE: still no progress. Think I need to figure this out before going any further - hard enough to build a hello world app without any mechanism for testing, but impossible for a complex skill w/ intent chaining.
X -brainstorm: what else could possibly cause these http/alexa issues? How to troubleshoot and eventually fix?
    UPDATE: some thoughts below
    -wrong AWS account?
    -start totally new skill in console and try again
    -pare app down to most bare-bones version and see if I can get it working in console (comment out new endpoints that might not have intents defined for them. Also might get rid of some boilerplate, like the "what's your fav color?" intro which I don't know the source of)
    -flask-ask is deprecated (sort of doubtful, since found a tutorial from not that long ago still using it)
    -alexa dev console is broken (sounds like it was at some point, but that was years ago - surely it's not still broken. Could create new skill w/ old console maybe? Forget if that's an option.)

2/12/22 sat
-----------
X -fill in some docstrings in app.py
~ -work through troubleshooting ideas 1 by 1
    ~ -wrong AWS account?
        UPDATE: think this is correct (name Harrison and initial H; couldn't figure out how to easily confirm email but I'm pretty sure it's the right one)
    ~ -pare app down to most bare-bones version and see if I can get it working in console (comment out new endpoints that might not have intents defined for them. Also might get rid of some boilerplate, like the "what's your fav color?" intro which I don't know the source of)
        UPDATE: got launch intent working on dummy app! Still haven't gotten other intents working though.
    _ -flask-ask is deprecated (sort of doubtful, since found a tutorial from not that long ago still using it)
        UPDATE: seems doubtful since bare bones app shows signs of working. Possible question/statement json is no longer compatible, I suppose.
    X -start totally new skill in console and try again
        UPDATE: combined this with bare bones app strategy.
    _ -alexa dev console is broken (sounds like it was at some point, but that was years ago - surely it's not still broken. Could create new skill w/ old console maybe? Forget if that's an option.)
        UPDATE: seems unlikely given positive signs with bare bones app.

2/13/22 sun
-----------
~ -continue trying to get dummy app to work
    X -add HelloWorld endpoint
        UPDATE: This works! 
    ~ -debug YesIntent endpoint (first expected response after launch. Error seems to be with recognizing the right intent, not with the response, based on terminal logs.)
        UPDATE: Still no success. Sincce HelloWorld works, my guess is YesIntent sample words are too short/generic ("yes", "sure"). Maybe AnswerIntent fails for the same reason.
~ -look at jabberwocky launch endpoint and see if I can find any differences that would cause it to fail when dummy app launch endpoint works (see misc.txt for promising idea)
    [IF theory of name clobbering is correct]
    X -rename Voice Chat to something less likely to have duplicates
        UPDATE: trying "Quick Chat". Tested this in dev console and it doesn't seem to be a built-in, unlike Voice Chat.
    X -rebuild model
    X -test launch intent for jabberwocky
        UPDATE: still failing. Think we may need to change template. See misc.txt.
    X -Try non-intent response to see if fallback handler works on jabberwocky (maybe don't even try to actively start app? Could try both ways.)
        UPDATE: no, just no response or built in default of some kind.
    X -If that doesn't work, switch to make skill debugger console the main one and deprecate/delete jabberwocky in console. Choice of vanilla skill rather than conv template might fix things.
        UPDATE: Created new skill since I can't figure out how to rename skill (can change name in invocation but not how it shows up in console, it seems)
-test new skill intents
    X -launch
        UPDATE: Yes! First working endpoint for jabberwocky. Needed to use name "Quick Chat" as alexa interprets "Voice Chat" as some kind of existing functionality which I haven't enabled, apparently.
    ~ -fallback intent
        UPDATE: not working yet. It did in dummy app so need to figure out what the difference is here.
X -map out conv flow model
    UPDATE: Drew out basic flowchart of interactions. See notebook.

2/14/22 mon
-----------
~ -troubleshoot fallbackIntent in new skill (worked in dummy app, so why not here?)
    UPDATE: my intent deco was implemented wrong. After fixing it, the fallback endpoint does work as far as reading the right response. However, while logging inside the fallback endpoint wokrs, logging inside deco does not. Unsure if this means the deco is somehow going unused (seems unlikely given recent behavior) or if this is a matter of logging output being hidden. Tried using diff logger than the default app one but this didn't resolve the issue. Tried checking prev_intent and it's not being updated either (supposed to be done in deco). However, if I manually update it in the func itself, it works.
X -add endpoint to list out all personas

2/15/22 tues
------------
X -add sys.exit in various stages of intent deco to try to get to the bottom of whether it's being called
    UPDATE: it wasn't. ask.intent deco is quite unusual and overriding it is confusing. Took a different approach.
X -write new ask subclass and redefine intent deco
    X -write custom IntentCallback
    UPDATE: ask.intent wrapper is odd, appears to never actually be called but it must be sometime. Must do some weird registration magic or something on the ask object. Managed to get my prev_intent tracking working by writing custom callback and having custom ask class wrap the input function when it's passed in (rather than explicitly defining on start/end behavior in decorator, define it via callback methods).

2/16/22 wed
-----------
X -add intents to console for new skill (jabberwocky-voice-chat, not jabberwocky)
X -add slots to console for new skill
X -test app in test console and see what doesn't work (sure there will be something)
    UPDATE: looks like it can send a request to choosePerson and changeTemperature endpoints but they receive no arguments (e.g. slots aren't working, or at least not how I expected them to).
    X -write next steps based on observed errors
        UPDATE: see tomorrow's to do list.
X -docs for custom ask (in particular: note to self not to delete seemingly useless wrapper inside intent deco. Flask-ask version uses that so I don't know what else is reliant on it)
-start updating endpoints in py file based on observed errors (see above)

2/17/22 thurs
-------------
-fix missing slots issue with choosePerson
    _ -replace actual logic w/ debugging/logging to figure out what (if any) arg(s) func is receiving
        UPDATE: looked at existing logs and error indicatded no arg of any kind was received.
    X -update parsing to get correct object
        UPDATE: found blog post implying custom slot might have to be extracted from request manually. Wrote new code to do that and it works for choosePerson (successfully selected an existing persona!).
    X -reintroduce actual logic and test
        UPDATE: new persona doesn't work yet (relies on YesIntent, which I added now, but much more new logic is needed).
~ -start updating endpoints in py file based on observed errors (see above)
    ~ -changeModel
        X -rename chooseModel -> changeModel (forgot I chose diff name in console for this skill)
        X -fix ModelType slot (4 -> 0)
        UPDATE: added some str handling to chooseModel endpoint. Model is currently rebuilding in console.
    -changeTemp
        UPDATE: brief testing reminded me of issue where alexa thinks I'm trying to change temperature of an actual device. Will need to consider options for how to handle this.
    -changeMaxLen

2/18/22 fri
-----------
~ -continue updating endpoints in py file based on observed errors (see above)
    -changeModel
        X -check that build succeeded
        X -try out zero, one, two, three
        _ -try out j
        _ -try out neo
        UPDATE: might want to rewrite j/neo query funcs w/ availability of new open source models (and possible deprecation of old gpt-j api? Unclear.) Do this later.
    -changeTemp
        ~ -consider how to deal with issue noted in prev day's notes (could rename to Temp/something else, or maybe there's a way to override default commands when in a custom skill)
            UPDATE: tentatively seems okay now - I think maybe before I was saying this after returning a Statement rather than a Question so we had exited the skill?
        ~ -figure out how to convert str to num
            UPDATE: considered requiring an int in [0, 99] and hardcoding list of strings. Was curious though and decided to try gpt way - got this working reasonably well but only with a disappointingly big model (smaller ones prob could do well with more prompt tuning but I didn't quite get there yet). Added prompt text and config files and added API call to app script. However, still having some trouble parsing decimals.
    -changeMaxLen
    -reply

2/19/22 sat
-----------
-continue fleshing out endpoint logic
    ~ -changeTemp
        X -consider options for improving number parsing and choose one (i.e. require int from 0-99 and divide answer by 100, or update slots to expect "point {number}".)
            X -make necessary changes in console and/or script
            _ -update gpt word2number prompt files accordingly (e.g. maybe no decimals needed after all, could just be word2int)
                UPDATE: unused now so not important. For general purpose future use, better to leave it more broadly capable.
        UPDATE: changed to use hard-coded dict with some fuzzy matching. Couldn't test yet since I accidentally blocked dev console w/ Freedom.
    -changeMaxLen
    -reply
~ -planning re maintaining settings
    UPDATE: initially started as just trying to alias session.attributes with something shorter since it's used so much. Surprisingly trick. Ended up causing me to think about settings and realizing we really want 3 diff levels (see misc.txt notes from today). Started building new class to maintain this.

2/20/22 sun
-----------
X -continue fleshing out endpoint logic
    X -changeTemp
        X -test new changes in console.
            UPDATE: updating temperature works for valid integers. Non-number word rightfully raised an error which was handled gracefully.
    X -changeMaxLen
        X -look more if there's a better way to do this with alexa (surely yes, but seems like no???)
        _ -add gpt3 parsing (too many ints to hardcode cleanly)
            UPDATE: Turns out it does this automatically after all. Previous failures might have been due to flask ask argument issue. Tested this on both valid and invalid values.
X -update settings intents to accept optional slot for scope (global/person/conversation)
    UPDATE: still need to fully make use of this in app.py.
X -Settings class
    X -think more about desired interface (important - need to know what I'm trying to implement)
    X -continue fleshing out attr access methods
    X -write resolve() method
    X -update app.py accordingly
X -start building query_gpt_j variant using banana.dev backend (prob smallish version, but good to have another free option in case random git one gets deprecated)
    UPDATE: need to add support for 'stop' param, maybe better arg validation to ensure no nnaming issues around topK vs top_k, etc.

2/21/22 mon
-----------
X -consider: should Settings include kwargs only or also things like prev_intent/should_end/should_save?
    X -update app.py and/or utils.py accordingly
    X -document Settings
    UPDATE: include both in Settings. Make kwargs accessible via bracket notation and general settings accessible via dot notation. Also try to cut down on number of general (non model query) settings - rely more on prev_intent and yes/no. Might have to change yes/no intent endpoints to not update prev_intent though.
X -test new changes in test console
-flesh out `reply` endpoint

2/22/22 tues
------------
-debug/fix: choose person intent seems to soak up any reply containing a name (or at least sometimes). Can try:
    X -adjusting sample utterances for choosePerson intent in console and re-test
        UPDATE: found "Call {person}" no longer is clobbered by default behavior. Guessing I previously exited the skill session w/out realizing it before trying to trigger that intent.
    ~ -add some logic using prev_intent to infer when a reply has mistakenly been identified as choosePerson
        UPDATE: added check in choose_person if conv is active but this still chops off the first word of the response  we can't recover the full text.
    X -Add new `reply` endpoint w/ AMAZON.SearchQuery slot (this is supposed to soak up the whole text response)
        UPDATE: had to use leading space to avoid errors.
-try refactoring out base functionality of endpoint funcs (bc flask_ask not passing args to intent funcs, we need to get slots inside func, which makes it hard for one intent to call another)
    UPDATE: Ended up being unnecessary so far but might have to reintroduce at some point. Another option is to make endpoint accept an optional arg and then use it if provided and extract slot if not.

2/23/22 wed
-----------
X -try updating reply intent to match what I made in console
    X -test in console
    _ -update/rm fallback intent in code
        _ -and rm in console if necessary
        UPDATE: decided keep for now. Might need an extra layer of fallback beyond SearchQuery.
    UPDATE: works, though identified possible bug on second query (I also called changeModel in between). Conv didn't think a conv was in progress - does conv object get recreated after each skill call? Need to investigate. Also found possible bug where sessionstate str appeared empty in console even though mock_func should have been set.
X -update ask deco to log settings state on each intent call
-consider yes/no intent logic: need to prevent them from updating prevIntent? Consider tracking a longer intent history list rather than just prev.
    -make updates accordingly
-see if we can move IntentCallback and CustomAsk to utils (bit tricky w/ reliance on session?)
X -make use of Scope var in settings intents
    UPDATE: also added `default` option to slots func to make this work better when user doesn't specify level.

2/24/22 thurs
-------------
X -investigate issue where second reply causes error due to "conv not in progress". (see yesterday's notes documenting this error and the one below)
    UPDATE: rewrote slots func a bit (I think I was doing several things wrong - list(values())[0] logic was not safe, changeTemp func passed wrong slot name to slots for Temperature). Seems to work now.
~ -investigate bug where Settings obj __str__ appears empty in logged message
    UPDATE: could not reproduce. Keep an eye out for this.
X -add readContacts intent in console
    X -test in test console
-update end of conv to erase conv-level settings
    -update change of person to change person-level settings
    -maybe update settings.clear() to allow clearing a single (or finite number) of state levels.

2/25/22 fri
-----------
X -add readSettings intent to console
    X -add code endpoint
    X -launch rebuild
    X -test in console
X -add endChat endpoint
    UPDATE: also had idea to prefix settings commands with "Alexa" to distinguish them from replies. But when testing, alexa did not recognize these utterances (just fell back to reply intent). Need to work on this more.
-update end of conv to erase conv-level settings
    ~ -consider whether settings resolution order is right/desirable (i.e. should global level take priority or should conv level?)
        UPDATE: thought a bit and this is quite a complex task. Still not at a solution yet but my sense is that priority should be more about sequence (most recent = highest priority) and scope should just be thought of as "when do we undo this setting change action"? Need to hold off on other changes until I figure out desired system.
    -update change of person to change person-level settings
    -maybe update settings.clear() to allow clearing a single (or finite number) of state levels.
~ -start prototyping prompt routing system (a way to implement intent chaining, basically)
    UPDATE: some progress here. Plan is to have an endpoint func's follow up func by named identically except with a leading underscore. Could become problematic if we need multiple followups though. Started testing on end_chat but ran into utterance problems (see above).
-tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)
    
2/26/22 sat
-----------
[LEAVE EMAIL UNBLOCKED - testing email sending]
X -try adjusting end_chat utterances since curr ones aren't getting recognized
    X -maybe adjust other settings intent sample utterances to be prefixed by something distinctive. Looks like 'alexa' might not have been the best choice.
        UPDATE: noticed my attempt to trigger end_chat was getting sent to SearchQuery WITHOUT the leading "Alexa", adding to my theory that this word is treated differently. Decided on the name "Lou" for the skill "assistant" (one syllable, inspired by Lewis Carroll, gender neutral) and prefixed sample utterances for all settings-related commands with it.
~ -continue testing end_chat intent + prompt chaining
    ~ -troubleshoot transcript sending as needed
        UPDATE: initially was getting 401 status code due to 'Bearer: {token}' instead of 'Bearer {token}'. Fixing this now gives me a 403 status code (also added permission for user first name and email in console, and gave my personal permission in iphone app and alexa website). Still not working though.
X -look into EndSession more
    _ -When is it called? I.e. is it triggered by "stop" or something else?
    X -What should it return? I think this is correct based on an official tutorial but should confirm.
    UPDATE: every example I've seen handles this identically, so I think there's no need to look into this any more.

2/27/22 sun
-----------
-try sending transcript w/ hardcoded email (need break from debugging permissions issues)
    -debug formatting, etc.
-look into email access issues (api returns 403 Forbidden status)
    -maybe need to add card/audio asking for permission in the skill itself?
X -write choose_person followup func (do you want to download new persona?)
~ -deep dive into intent chaining
    UPDATE: flask-ask does have delegate() and elicit_slot() functionality which seems to be related to what I want, but I couldn't quite figure out how to use it do what I want (no docs :( ). Instead, started overhauling this process in new `chain` branch. Renamed searchIntent intent from reply -> delegate and have that point to enqueued funcs. Made Ask object handle this enqueue/deque process. Still need to test this more.

2/28/22 mon
-----------
~ -test new chain workflow
    -selecting a user that exists
    -selecting a user that does NOT exist and NOT downloading them
    -selecting a user that does NOT exist and downloading them
    UPDATE: made some major changes and saw some positive signs but haven't thoroughly validated that any of the above cases work post-changes.
X -auto avoid duplicate calls from queue (realized alexa often matches intents without help of queue/delegate, so enqueueing intents was problematic)
-rm remnants of old chaining system (followup_func, check for other '_' prefixed funcs, etc.)
    -consider if I should remove/update prev_func state attr? Maybe it should store func name instead of intent name since we're now often calling non-intent funcs.
-look for other intent chains that need to be implemented and document below
    -implement the following intent chains:

3/1/22 tues
-----------
X -test new chain workflow
    X -selecting a user that exists
    X -selecting a user that does NOT exist and NOT downloading them
    X -selecting a user that does NOT exist and downloading them
X -rm remnants of old chaining system (followup_func, check for other '_' prefixed funcs, etc.)
    X -consider if I should remove/update prev_func state attr? Maybe it should store func name instead of intent name since we're now often calling non-intent funcs.
        UPDATE: fine for now, not causing any problems. Maybe revisit later if I find a good reason to store more steps.
X -try sending transcript w/ hardcoded email (need break from debugging permissions issues)
    X -debug formatting, etc.
    UPDATE: Had wrong email in alexa config. Once I fixed that and updated htools.quickmail to allow Paths for attach files, no problems with email or formatting.
-look for other intent chains that need to be implemented and document below
    -implement the following intent chains:
~ -debug issues where changing settings wasn't working
    UPDATE: 1 issue was that MaxLength had wrong slot name (should be Number, not maxLength). Another was that "change model" sample utterances didn't include the exact order I was saying and I guess they're pretty rigid, so I added a couple more variants. Also changed to debug mode using htools deco to print when any func is called, bc I'm still finding it hard to follow flow sometimes.

3/2/22 wed
----------
X -look into ask.logger log file (is it just continually growing longer and longer? Maybe reset.)
    UPDATE: reset each time we instantiate Ask so it's just one session's worth of logs.
X -update send_transcript() to leave file in alexa/conversations by default rather than deleting.
X -maybe update deco to print queue/state etc. on function end as well. Sometimes confusing not having this info.
X -move some constants to config
X -clean up imports a bit
X -fix casing bug when generating a person
    UPDATE: prob should update wiki func itself to handle this, but for now just passed in a titlecased name instead to avoid updating lib.
X -adjust slots strategy to only use "resolutions" as a backup (saw some cases where resolutions worked worse than "value")
X -rm some old files in alexa dir from abandoned appraoches
    UPDATE: just moved to trash in case I need to recover later.
-look for other intent chains that need to be implemented and document below
    -implement the following intent chains:
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/3/22 thurs
------------
X -investigate: possible to get alexa to maintain casing/punctuation?
    X -if no, consider running punctuate on transcriptions before sending to gpt3. All lowercase may change results (more casual vibe?).
        UPDATE: no native support. I copied my punctuate_transcription prompt to punctuate_alexa and lowercased the first letter of each transcription, then added this to reply(). Seems to be working as expected so far.
X -tweak logging to be a bit easier to grok.
    UPDATE: some intent calls are nested and it was hard to tell which ON END log statement corresponded to which intent. Updated on_end call to print prev intent as well.
X -look into flask-ask reprompt (does this exist and where can I apply it if so?)
    UPDATE: added reprompts to all question() uses that seemed appropriate. Still need to test this in UI. Also updated some of the settings reading options to auto redirect to choose_person if the user's not in the midst of a conversation already.
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/4/22 fri
----------
X -add intent to change settings to auto-punctuate or not
    UPDATE: realized adding separate enable/disable endpoints was easier.
X -convenience func to maybe prompt user to choose_person depending on if in active conv
X -fix bug in _reply logger (can't pass multiple strings unlike print)
X -fix bug w/ func queue not being cleared on launch
-add support for banana.dev version of gptj 
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/5/22 sat
----------
~ -look into email access issues (api returns 403 Forbidden status)
    ~ -maybe need to add card/audio asking for permission in the skill itself?
    UPDATE: got personal email working via manual access and got consent card to display, but no haven't figured out how to process response yet. Included more notes in app.py (search tmp_email_me; eventually prob need to delete or change this, both in code and in UI. Just used a dummy intent triggered by "email me" for testing purposes.)
X -update get_user_email func to get name too
-add support for banana.dev version of gptj 
-maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.

3/6/22 sun
----------
X -consider refactoring to make it cleaner to reset all state (queue, attrs, query kwargs, etc.). Periodically finding new things that aren't reset when I update code and flask reloads.
X -look for ways to integrate user name into skill
    X -any time Lou speaks, e.g. welcome message?
    X -maybe edit transcript to replace '\n\nMe: ' with '\n\n{name}: '?
        UPDATE: better to change ConversationPersona itself to use name in prompts. That way, generated responses can actually refer to you by name.
-allow fuzzy match for choosePerson, or at least catch last names (e.g. einstein -> albert einstein; felt like a questionably useful feature initially but even just for dev purposes it would be nice to not have to type out the full name each time)

3/7/22 mon
----------
~ -look into user email/name access issues (worked on sat but on sunday, had to provide access in alexa app again (i.e. it revoked access at some point?). Might be related to consent card, i.e. user has to provide consent for every new conv?)
    UPDATE: permissions were revoked again.
    X -add privacy policy (necessary for skills that request name/email)
        UPDATE: Also filled in some other details like skill description and sample utterances.
X -allow fuzzy match for choosePerson, or at least catch last names (e.g. einstein -> albert einstein; felt like a questionably useful feature initially but even just for dev purposes it would be nice to not have to type out the full name each time)
    UPDATE: for simplicity, just check for str equality w/ last name. Don't want any crazy assumptions being made and choosing the wrong person.

3/8/22 tues
-----------
X -add small skill icon (128x128 px) to alexa Distribution tab
    X -add large skill icon (512x512 px) to alexa Distribution tab
    X -check that icon shows up in iphone app
X -maybe tweak slots() interface so we can get more informative error messages (right now all errors are read back to user as empty strings)? Or just change print statement to log before returning final value.
    UPDATE: mostl converted some print statements to log statements with the help of a slightly hacky getattr-inspired function that works with nested attributes and globals.
X -see if we can move IntentCallback and CustomAsk to utils (bit tricky w/ reliance on session?)
X -update end_chat flow to not ask about sending transcript if no email has been provided

3/9/22 wed
----------
X -should conv prompt config be updated to use "\n\nMe:" instead of "Me:"? More robust if we replace w/ name. Would need to update ConversationManager as well, probably.
~ -update query_gpt3 to support goose.ai models (e.g. neo 20b)
    UPDATE: wrote gooseai_backend contextmanager func and started openai_backend contextmanager class. The latter still needs some work and might end up replacing the former.
X -sign up for banana.dev and store api key
-add support for banana.dev version of gptj (see ipy in tab 1)

3/10/22 thurs
-------------
X -investigate issue where gpt3 (davinci model 😱) seems to be used by alexa when I thought I set it to use gptj mock func. (See openai costs tab.)
    UPDATE: failed to set mock=True, both in alexa and gui, for gpt-j usage. Updated query_gpt3 to automatically do that when mock_func is not none. Still concerned that func's interface might need to change dramatically.
X -flesh out openai_backend support
    X -test openai_backend class in ipython
        X -switch from openai to gooseai
        X -switch from gooseai to openai
        X -persist mode works
        X -no persist mode works
    X -can this replace my gooseai_backend contextmanager func?
    UPDATE: major rewrite -> BackendHandler class. Deleted old contextmanager func.
-figure out best interface to use gooseai (just use query_gpt3 w/ diff backend? Kind of annoying that I have to pass mock_func in some cases, but in this I change the backend.)
    -look into docs on api_backend and api_key. Maybe all my mock_funcs could use this? Would be nice to avoid passing in mock_func.
    -make sure we can still track (via kwargs or other attr, perhaps) what backend is being used. Don't want this to be assumed.
-[BLOCKED: confirm possible redesign of backend switching and/or mock funcs] make dist and upload to pypi w/ new query_gpt3 changes?
-add support for banana.dev version of gptj (see ipy in tab 1)

3/11/22 fri
-----------
~ -easy option: write docs for...
    X -IntentCallback
    -look in alexa/utils and alexa/app for others
-figure out best interface to use gooseai (just use query_gpt3 w/ diff backend? Kind of annoying that I have to pass mock_func in some cases, but in this version I change the backend. Maybe can adjust BackendHandler and/or other query_gpt_{x} funcs to handle everything automatically.)
    ~ -look into docs on api_backend and api_key. Maybe all my mock_funcs could use this? Would be nice to avoid passing in mock_func.
        UPDATE: didn't find any formal docs besides a mention of some azure endpoints - nothing with huggingface.
    -make sure we can still track (via kwargs or other attr, perhaps) what backend is being used. Don't want this to be assumed.
    -[BLOCKED: confirm possible redesign of backend switching and/or mock funcs] make dist and upload to pypi w/ new query_gpt3 changes
-add support for banana.dev version of gptj (see ipy in tab 1)
X -add uses of _maybe_choose_person to other settings funcs
X -add str(int) versions of model_i values for changeModel (observed some errors where model change failed or wrong intent was chosen because transcription used "2" instead of 2).
X -debug new unexpected uses of gpt3 vs. gptj
    UPDATE: realized when I change code, flask refreshes some things but doesn't call launch() automatically. So app state does not fully refresh. Set debug=False since refresh func doesn't work before app.run() is called and that call is blocking.
X -make changeModel set mock_func=None when changing engine_i to an int (realized if prev was gpt-j, changing to engine_i=0 wasn't removing that and so we just queried gptj and igonred the engine_i arg)

3/12/22 sat
-----------
~ -figure out best interface to use gooseai (just use query_gpt3 w/ diff backend? Kind of annoying that I have to pass mock_func in some cases, but in this version I change the backend. Maybe can adjust BackendHandler and/or other query_gpt_{x} funcs to handle everything automatically. Maybe need more separation between the concept of a backend (e.g. openai, gooseai, banana, huggingface) and model (e.g. openai-davinci, gptj-6b, gooseai-6b, huggingface-6b))
    UPDATE: yes, keep backend and model separate. Maybe can adjust neo function to accept engine_i too (with gpt-j-6b, they technically have 4 solid options now too).
    X -create new changeBackend intent in console
    X -create changeBackend endpoint
    ~ -test
        UPDATE: initial attempt failed to recognize endpoint but can't remember if I built model since most recent changes. Launched build and can test tomorrow.
    X -new backendselector functionality:
        X -get current api name
        X -get model name from user-specified engine_i and current api name
-easier option: look for docs in app.py, utils.py

3/13/22 sun
-----------
~ -test changeBackend endpoint in console
    UPDATE: mixed results. Switching to openai works more reliably than gooseai, and typing works more reliably than speaking. Speaking gooseai never worked - always just defaulted to SearchIntent. Decided to write my own routing logic as a fallback in case alexa doesn't recognize certain intents.
X -write function to convert json editor -> fuzzy dict mapping utterance to intent. (handle all combos of sample slot values for all sample utterances for each intent)
    UPDATE: saved in data/alexa dir.
-update query_gpt_neo function in jabberwocky.openai_utils
    -support 6b param model
    -see if we can adjust interface to use engine_i rather than size
-gui updates?
    -decide if that's within the scope of this project. Could always pin jabberwocky version for gui.
    -if yes to prev point, adjust gui accordingly (cur offer different radio buttons for each neo size; could instead have 1 radio and have it actually use the engine_i slider)
    -add gooseai backend option to gui?

3/14/22 mon
-----------
X -write infer_intent func to guess based on fuzzy dict simlarity results
    X -update delegate to use it when appropriate
X -write CustomAsk method to get func (not just name) from intent name
X -move prev_intent update to occur post pre-func logging but before post-func logic
~ -continue testing changeBackend in console
    UPDATE: some success w/ inferring intent, but need to figure out how to handle slot vals. Could try to extract them using regex and pass to funcs directly, or extract and then mutate slots object, or pass in whole str and use some sort of partial_token_set_ratio or scorer that supports partial matches, or always raise error.
    -for diff utterances, see what matching intents are printed out by fuzzykeydict
    -see what common similarity scores are
    -determine a good way to map from similar intents to an actual choice (i.e. do all need to match? Do we just care that the top one is over some similarity threshold, and if so what? Do we take some weighted score, e.g. if top match is 80% changeBackend but #2 and #3 are both 79% changeModel?)
        -what other logic (if any) do we need to implement to make this work? In some cases we may want to trust the queued function, in some maybe not? Might also be able to hardcode some logic, i.e. "startswith(Lou)".
    -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -try some other intents and assess routing logic quality.
    -maybe assess whether n=3 is a good choice for similar keys

3/15/22 tues
------------
~ -Consider how to handle slot vals when calling an inferred function from delegates(). Could try to extract them using regex and pass to funcs directly, or extract and then mutate slots object, or pass in whole str and use some sort of partial_token_set_ratio or scorer that supports partial matches, or always raise error.
    UPDATE: decided to try manually extracting slots - atm, there are only 5 intents w/ slots and most seem pretty feasible to extract. Wrote draft function for names (choosePerson), numbers (maxLength, temperature), and backend (e.g. gooseai).
-continue testing changeBackend in console
    -for diff utterances, see what matching intents are printed out by fuzzykeydict
    -see what common similarity scores are
    -determine a good way to map from similar intents to an actual choice (i.e. do all need to match? Do we just care that the top one is over some similarity threshold, and if so what? Do we take some weighted score, e.g. if top match is 80% changeBackend but #2 and #3 are both 79% changeModel?)
        -what other logic (if any) do we need to implement to make this work? In some cases we may want to trust the queued function, in some maybe not? Might also be able to hardcode some logic, i.e. "startswith(Lou)".
    -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -try some other intents and assess routing logic quality.
    -maybe assess whether n=3 is a good choice for similar keys

3/16/22 wed
-----------
X -decide if that's within the scope of this project. Could always pin jabberwocky version for gui.
    UPDATE: no, just pin version of jabberwocky for gui requirements.txt and make a separate one for alexa.
X -update project files to account for decision to pin gui to diff version of jabberwocky
    X -make 2 requirements files
    X -add make install_dev cmd
    X -update readme accordingly
-update query_gpt_neo function in jabberwocky.openai_utils
    X -support 6b param model
    X -see if we can adjust interface to use engine_i rather than size
    ~ -test how performance (speed, reliability) compares to free gpt-j api. Trying to decide if one or both can be deprecated. (Would make settings easier to handle and generally more intuitive if "model" slot was always a number.)
        UPDATE: gptj api is down atm. Neo function appears to work, though slowly - haven't timed yet since gptj is down.
~ -look into adapting openai Completion api to use neo and vicgalle as backends
    UPDATE: I'm sure we *could* do this but after a bit of digging, it seems sufficiently complicated that I I'm not sure it's the easiest way. Might be easier to have backend provide a query() method that inserts mock_func into query_gpt3 kwargs and use that in place of query_gpt3? But that may not work with convmanager/promptmanager. Could update those, of course.
_ -write slot extraction func for chooseModel (see bottom of alexa/utils.py for other sample slot extraction funcs)
    UPDATE: plan is to make huggingface and vicgalle backends rather than models, so model can always be a number. No need for a new parsing function.
    -consider interface. Should this extraction occur w/in each func separately, or create 1 SlotExtractor that does this in delegates() after inferring a function?
    -implement chosen method
-continue testing changeBackend in console
    -for diff utterances, see what matching intents are printed out by fuzzykeydict
    -see what common similarity scores are
    -determine a good way to map from similar intents to an actual choice (i.e. do all need to match? Do we just care that the top one is over some similarity threshold, and if so what? Do we take some weighted score, e.g. if top match is 80% changeBackend but #2 and #3 are both 79% changeModel?)
        -what other logic (if any) do we need to implement to make this work? In some cases we may want to trust the queued function, in some maybe not? Might also be able to hardcode some logic, i.e. "startswith(Lou)".
    -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -try some other intents and assess routing logic quality.
    -maybe assess whether n=3 is a good choice for similar keys

3/17/22 thurs
-------------
-updates to reflect recharacterization of neo and j as backends rather than models
    X -in console, rm J and Neo as model options
    X -in console, add j and neo as backend options
        UPDATE: renamed J to Vic since it specifically refers to the vicgalle api (I assumed this was a name Vic Galle but I don't actually know. Not important though.).
    X -update saved fuzzy dict from new skill json (see func in alexa/utils)
    ~ -update app.py to account for changes
X -add slot func option to intent decorator and update intent funcs accordingly
-consider how to support j and neo as backends in BackendSelector
    -implement

3/18/22 fri
-----------
X -consider if any intents should be removed from fuzzy dict of inferrable intents (e.g. Yes, No, ones without slots?)
    _ -implement
    UPDATE: Yes and No are already not included. Other intents without slots could still be worth inferring so leave those in.
X -update slot processing code for each intent in fuzzy dict (don't worry about composing slot funcs yet, just update signatures)
    X -changeModel
    X -readContacts
    _ -emailMe
        UPDATE: intend to rm this eventually anyway.
    X -changeTemperature
    X -changeMaxLength
    X -readSettings
    X -enableAutoPunctuation
    X -disableAutoPunctuation
    X -endChat
    X -changeBackend
X -consider how to handle intents w/ multiple slots (maybe need to compose funcs like get_num, get_scope)
    UPDATE: change slot_func to slot_funcs which maps each slot name to a function.
    X -implement
    UPDATE: overhauled whole slot_func interface. Specify using type annotations, then make ask.intent auto-construct the slot parsing func based on that. Haven't tested yet outside of toy examples in ipython.
-work on new get_backend func (jump off of alexa/utils version) to handle new vals (see fuzzy dict "fd" in ipython sess below this pane)
     -consider approaches: __contains__, fuzzywuzzy, fuzzyregex or spacy matcher, gpt infer
     -implement
-test each inferrable intent in console (e.g. type something like "Lou, change backend to Open I" or speak "Lou change backend to goose AI" (hard for it to recognize))

3/19/22 sat
-----------
~ -work on new get_backend func (jump off of alexa/utils version) to handle new vals (see fuzzy dict "fd" in ipython sess below this pane)
     ~ -consider approaches: __contains__, fuzzywuzzy, fuzzyregex or spacy matcher, gpt infer
     ~ -implement
    UPDATE: tried a ton of stuff combining str similarity, phonetic similarity, gpt prompting, etc. Still haven't quite gotten something satisfactory.
X -update valid backend slot utts
    X -rename vic -> hobby (real word, not a name)
    X -regenerate fuzzy dict
-work on get_scope function
-test each inferrable intent in console (e.g. type something like "Lou, change backend to Open I" or speak "Lou change backend to goose AI" (hard for it to recognize))
    -test ability to extract slots in each case (above task refers to our ability to infer intents, which is a distinct step)

3/20/22 sun
-----------
-adjust changeBackend intent func - slot utts allow both with and without spaces. Need to account for that.
~ -investigate phonetic similarity->str sim approach in ipython below. Why all score=86? Maybe need diff str sim method.
    ~ -go through ideas in misc.txt until I find a get_backend I'm happy with
    ~ -look into Adrienne idea re formnat values
        UPDATE: lots of missing values in table I found compared to eng_to_ipa encoding. Also couldn't quickly find good way to encode consonant similarity.
    ~ -try patternomatic spacy package
        UPDATE: lots of code issues, seems like not compatible w/ current version of spacy.
    UPDATE: fairly promising method is to store slot vals used for each utt and then just use those from closest utt. Avoids lots of extraction hassle, but might not have really solved the poor transcription issue. That might be better solved with gpt3 - perhaps asking it to correct spellings will work better.

3/21/22 mon
-----------
X -update infer_intent to provide slots in weighted case (see TODO)
_ -prototype new gpt prompt asking to correct spelling rather than correct+extract all in 1
    UPDATE: let's hold off on this until I see more evidence that it's actually needed. Maybe new method will work already.
X -write deprecated deco and mark slot extraction funcs as such
    UPDATE: put this in htools rather than jabberwocky.
X -adjust changeBackend intent func - slot utts allow both with and without spaces. Need to account for that.
    UPDATE: changed BackendSelector to lowercase input and remove spaces. Think that should be sufficient.

3/22/22 tues
------------
~ -consider how to support hobby and huggingface as backends in BackendSelector
    X -implement
    UPDATE: wrote something that I think should work but needs much more testing.
-continue testing changeBackend in console
    -is the intent getting recognized?
    -if not, what intents (if any) are getting inferred?

3/23/22 wed
-----------
~ -test backendselector more in jupyter (not ipython, want to see docstring)
    X -do base and key change when we expect them to?
    X -does query() show docstring as expected?
        UPDATE: had to add modified fastai decorator but now it works.
    ~ -try actual queries w/ all functions wrapped in debug. Make sure mock funcs are getting called when appropriate.
        UPDATE: Seems to work but both gptj and huggingface eleuther apis are down or deprecated - unclear.
X -renew goose.ai api key
-continue testing changeBackend in console
    -is the intent getting recognized?
    -if not, what intents (if any) are getting inferred?

3/24/22 thurs
-------------
X -rename query neo to match backend name
X -maybe rename backendselector class? No longer just a backend selector exactly.
X -make ls() (from nb) a method of backendselector?
X -add gptbackend method to get current mock_func
X -make other extra simple mock funcs supported by backendselector? With 2 apis down, there's no free option for testing atm. (Note: make sure you don't break query_gpt3 if so, since mock_funcs currently edit the object loaded by our regular mock response).
X -update convManager to use new backend.query api
X -update promptManager to use new backend.query api
    UPDATE: updated query and kwargs methods, then briefly kwargs tested in jupyter. Seems all good. But I then went back and made gpt.query a classmethod so need to test this again.

3/25/22 fri
-----------
X -test convManager and promptManager in jupyter after latest change to gpt.query (see last bullet point yesterday)
    X -convManager.kwargs
    X -promptManager.kwargs
    X -convManager.query
    X -promptManager.query
X -continue testing changeBackend in console
    X -is the intent getting recognized?
    X -if not, what intents (if any) are getting inferred?
    UPDATE: intent inference seemed to work very well initially (magically, no bugs!) but at end of session realized problem w/ weighted inference strategy.
~ -test each inferrable intent in console (e.g. type something like "Lou, change backend to Open I" or speak "Lou change backend to goose AI" (hard for it to recognize))
    _-test ability to extract slots in each case (above task refers to our ability to infer intents, which is a distinct step)
    _ -for diff utterances, see what matching intents are printed out by fuzzykeydict
    ~ -see what common similarity scores are
    ~ -briefly examine fuzzykeydict examples and try to think of what common bugs could appear (e.g. very short sample utterances might be problematic? check what shortest ones are)
    -maybe assess whether n_keys should change in fuzzy_dict.similar() call
-easier(?) task: add support for banana.dev version of gptj (see ipy in tab 1)
-fun task: adjust Ask to track how many calls deep we are for a single user turn and adjust logging indentation accordingly. Make it visually easier to see when an intent call is really done vs. when it was just a nested call inside another intent.

2/26/22 sat
-----------
[NOTE: currently have gooseai enabled for both punctuation and replies. Hobby backend still down and huggingface backend seems too slow to do 2 tasks (punct + reply) in time for alexa.]

X -add "repeat" option to dialog model in UI
    X -generate new utt2meta.pkl
X -update readSettings to include defaults (i.e. if user hasn't explicitly set Model, this intent won't read it)
    X -update
    UPDATE: adjusted settings object to store these by default.
    X -test in console
X -add makefile commands for run_alexa and ngrok (realized I forgot ngrok cmd after restarting computer)
-add a "back to your conversation" msg for when changing settings mid conv
    X -implement
    -test in console
X -fix weighted strategy in infer_intent (e.g. if all 5 closest matches are changeModel but the closest match is .4, we wouldn't want to delegate to that, but we do atm)
    X -implement
    X -test
~ -troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    ~ -see if we can increase timeout
        UPDATE: sounds like default timeout is 10sec for skill to provide a response (incidentally, user has 8sec to provide a response) and these values are not configurable. However, it may be possible to hack a workaround using progressive responses, but this would not be ideal for every response. I.e. if we know a longer completion is coming, we could probably have alexa say "hmm, let me think about that" and then follow up with the real response. But we don't want alexa to say that every turn.
~ -adjust Ask to track how many calls deep we are for a single user turn and adjust logging indentation accordingly. Make it visually easier to see when an intent call is really done vs. when it was just a nested call inside another intent.
    X -implement
    ~ -test
        UPDATE: didn't see any indentation yet but I didn't check thoroughly if the sequence of actions/intents called for it. Do this tomorrow.
        

3/27/22 sun
-----------
X -test call stack logging more thoroughly in console (does it indent messages when it should?)
    UPDATE: ended up taking different approach since indenting was not affecting other logged statments - wrote custom formatter to address this. Also updated on_end to show which intent just ended since it wasn't before. Non intent-callback-induced log statements are indented more than intent-callback-induced log statements but I think that's okay and some of those (e.g. slots, the most common and annoying) will likel be removed anyway at some point.
-investigate potential issue where ' Me:' is getting attached to end of gpt responses. Unsure if weaker model is skipping the double newline and therefore not being caught by the stop phrase, or if conv.me set op isn't working right, or if gooseai includes the stop phrase(s) in the response.
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    -check if openai supports this too or just gooseai
    -implement
-update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    -check which backends support this
    -implement

3/28/22 mon
-----------
X -move logging to query_gpt3 or GPTBackend.query (rather than in conv/prompt managers only)?
    _ -first investigate if this is already done natively - vaguely recall seeing something in package code suggesting it was
    UPDATE: whoops, forgot to check for native solution. But already finished this so I guess it's fine.
~ -investigate potential issue where ' Me:' is getting attached to end of gpt responses. Unsure if weaker model is skipping the double newline and therefore not being caught by the stop phrase, or if conv.me set op isn't working right, or if gooseai includes the stop phrase(s) in the response.
    UPDATE: tentatively confirmed that gooseai does not truncate before stop word. Started refactoring this functionality out of huggingface mock func but found new issue re partial stop phrases. Documented in nb 12 for tomorrow.
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    -check if openai supports this too or just gooseai
    -implement
-update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    -check which backends support this
    -implement

3/29/22 tues
------------
X -add more params to query_gpt3 and update documentation
~ -finish stop phrase truncation function in jupyter
    X -support truncating on partial stop phrases when completion ended with reason "length"
    ~ -check which backends provide stopping 'reason' (affects which we can rm partial stop words for)
        UPDATE: openai and gooseai do. Haven't confirmed others yet but I doubt they do.
    -refactor functionality out of huggingface mock func
    -figure out how to do this for gooseai (does it need a mockfunc that just ONLY does this? Or should we put this GPTBackend.query and skip it depending on the current backend?)
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    -check if openai supports this too or just gooseai
    -implement
-update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    ~ -check which backends support this
        UPDATE: openai does w/ param n, same as gooseai. Still need to check HF and Hobby.
    -implement

3/30/22 wed
----------~
~ -update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 (will help w/ my mostly unrelated perplexity project)
    X -check which backends support this
        UPDATE: gptj still broken but from docs, looks like it does NOT support this natively. 3 others do though.
    -implement
~ -update query_gpt3 (and accompanying methods) to support n_completions > 1 (for backends that support this). (This might be useful for this project, but would certainly be useful for many other gpt use cases and I don't want to break my api yet again. Good reason to wait on pushing this version to pypi.)
    X -check if HF and Hobby (J) backends implement this (openai and gooseai do w/ param n)
        UPDATE: gptj still broken but from docs, looks like it does NOT support this natively. 3 others do (wip hf func includes this, need to remap name from n->num_return_sequences).
    -implement
-stop phrase truncation function
    -refactor functionality out of huggingface mock func
    -figure out how to do this for gooseai (does it need a mockfunc that just ONLY does this? Or should we put this GPTBackend.query and skip it depending on the current backend?)
    UPDATE: realized I need a better understanding of all the diff api response objects first (e.g. which offer finish_reason, what do responses look like when n_completions>1 and/or when n_prompts>1?). These factors will influece how to refactor. 
-allow (or force?) query_gpt3 to make mock functions overwriting other parts of mocked response besides text, e.g. finish_reason and logprobs. (Currently only text is changed.)
X -allow GPTBackend mock_func and engine methods to specify a backend besides current
    UPDATE: realized this was a problem when calling mock funcs directly rather than through backend.query. Now we're sure hf mock func uses right engine based on i.
X -add gptbackend.backends() method for convenience

3/31/22 thurs
-------------
X -add support for banana.dev version of gptj
    UPDATE: will need additional work to support stop phrases, etc. but all the funcs will undergo changes when that happens.
-update query_gpt3 (and accompanying methods) to work w/ n_prompts > 1 AND n_completions > 1
    ~ -consider desired interface. What should be returned when np=1 and nc=1, np>1 and nc>1, np>1 and nc=1, etc.
        UPDATE: wrote some sample usage code in nb12 markdown cell. Still undecided. Considering larger rewrite - the whole mock_func scheme seems kind of annoying. Might be easier to have GPTBackend just choose the right query_func and forget about the "passing a mock func to query_gpt3" paradigm. Also used mark decorator to label which funcs require manual stop phrase truncation - I think that will help refactor that step out.
    -examine responses from various and consider how best to map from these to desired interface
    -implement
~ -stop phrase truncation function
    UPDATE: started noodling on what this could look like. See end of GPTBackend.query method, which uses attr from mark() decorator to do exra truncation if necessary.
    -refactor functionality out of huggingface mock func
    -figure out how to do this for gooseai (does it need a mockfunc that just ONLY does this? Or should we put this GPTBackend.query and skip it depending on the current backend?)

4/1/22 fri
----------
X -add banana backend in aws console
    X -re-generate json and save (remember to save both raw json and generated')
-continue considering query redesign 
    X -settle on removing mock_func construct or not (try to think: is there really a good reason for this?). If yes:
        X -decide what each func should return
            UPDATE: decided to let each function return either (str, dict-like) OR (list[str], list[dict-like]). This allows for my most common use case so far (n_prompts=1, n_completions=1) to continue having a convenient interface w/out extra indexing, while also supporting a straightforward interface for n_completions > 1. Tentatively decided to handle n_prompts > 1 and do that for all funcs using threading/multiprocessing (these completions should be indepedent so no NEED to use the api built-in implementation). Plan is to have a separate GPTBackend.query_multi method where we return a list of tuples. This way we can unpack each completion as (text, resp) rather than the weird zip stuff that forcing this to fit my current interface would entail.
        ~ -start implementing
            UPDATE: tentatively updated each mock_func in nb, but still some cleaning up to do.
        -make sure redesign doesn't break ConvManager or PromptManager too badly (I'm okay with updating these - the api will clearly change too much to be compatible w/ the gui anyway - but make sure stuff like the hooked generator doesn't TOTALLY break in hard to solve ways.) Might want to change how we get kwargs(), i.e. the default query function could change and we should use the active one.
        -add check in GPTBackend.query for if we need to manually trunc stop words

4/2/22 sat
----------
~ -gpt query redesign
    X -finish implementing each mock_func in nb
    X -port to lib
    -update docs for various funcs
    ~ -polish draft of backend query method so it:
        ~ -does the warnings that used to be in query_gpt3
            UPDATE: added some, prob need more.
        X -handles the warning/error if user tries to pass in a list of prompts
        X -does post-query stripping (see gpt3 query func in pycharm, which I removed from current nb version)
        X -does post-query stopword truncation when appropriate
        X -test changes so far
            UPDATE: good so far. Still need to test stream mode.
        -update convmanager (careful w/ hookedgenerator)
            -maybe change kwargs() depending on active backend?
        -update promptmanager 
            -maybe change kwargs() depending on active backend?
X -update truncate_at_first_stop to make full trunc and partial trunc both optional
    X -test

4/3/22 sun
----------
~ -gpt query redesign
    -update docs for various funcs
    ~ -polish draft of backend query method so it:
        -add more warnings that used to be in query_gpt3
        X -write Thread that returns value
            UPDATE: may need to adjust this to ensure order is correct. Also, logging may not be threadsafe? Unsure.
            X -check if I log results with logger or htools.save
                X -check if the used method is threadsafe and/or multiproc-safe
                X -check if the alternative is threadsafe and/or multiproc-safe
                UPDATE: I use htools.save, which is not thread-safe. Logging is, though we couldn't use jq anymore to view pretty results, which is annoying. Can use threads if I use a lock - need to look into these more.
            -update to return in order (may require adjustments depending on bullets above)
        ~ -flesh out streaming mode
            X -think more about desired interface
            ~ -test stream mode
            X -write generic stream func for backends that don't support it
                UPDATE: currently supports np=nc=1 mode and np=1,nc>1. Unsure about others yet.
            X -check how backends w/ streaming support handle np>1 situations if at all
                X -check goose docs
                X -check openai docs
                UPDATE: both seem to support it, though looks like I'll need to try it to confirm structure of response.
                X -run func calls if necessary
                    UPDATE: first tried w/ nc > 1 (maybe unintentional? I forget.). Gooseai seemed to work but openai responses seemed to jumble the tokens without an obvious way to reconstruct each completion. If that's true, 2 of the 3 options with np>1 or nc>1 would be impossible to implement, so I decided to just make it simple and say stream mode is only supported when np=nc=1.
                    UPDATE 2: above is not true after all. They provide res['choices'][0]['index'] which seems to point to which completion the new token belongs to. Gooseai provides this too, they just also return completions in order (e.g. all 0s, then all 1s, etc.).
              X -update backend.query to use new mock stream func
              ~ -write new mock stream func to handle when query return val is (list, list) rather than (str, dict)
                    UPDATE: need to check if it needs to handle empty lists differently and figure out how to integrate into query.backend, but the basic functionality seems to be there.
        -update convmanager (careful w/ hookedgenerator)
            -maybe change kwargs() depending on active backend?
        -update promptmanager 
            -maybe change kwargs() depending on active backend?
        -think about moving hardcoded params to backend.query and leaving most new funcs to accept mostly kwargs?
        -general cleanup of mock_funcs, backend methods, etc. - very messy atm
X -give gpt_repeat func ability to return n>1 completions

4/4/22 mon
----------
~ -gpt query redesign
    X -test if new stream_multi_response in nb handles empty lists okay (or needs to)
    X -integrate into backend.query (replace stream_response? Or select 1 depending on returned type?)
        UPDATE: repeat, huggingface, and banana all work with stream mode now. Confirmed stream mode works w/ banana, repeat (both nc=1 and nc>1), and huggingface (both nc=1 and nc>1).
X -update returnable thread to return query responses in order (or write diff way to do this)
    UPDATE: wrote something to do this if we pass in a dict but turns out it works even without that! Guess join() blocks and ensures we return in order.
    ~ -start implementing query_batch for np > 1 (use ReturningThread).
        UPDATE: So far only tested this on funcs w/ no native streaming mode. (np>1, nc=1, stream=False), (np>1, nc>1, stream=False), and (np>1, nc=1, stream=True) seem good so far. Still need to test (np>1, nc>1, stream=True) - or do we? Didn't I decide that had no forseeable use case?
-update backend.query logging to file to work w/ threads (or whatever method I use in above bullet point)
X -email openai about api key exposure

4/5/22 tues
-----------
X -generate new openai key
X -generate new gooseai key
X -adjust gpt.ls() to not expose api key
X -write some sort of script to check for api key exposure before pushing/committing to git
_ -fix requirements-dev missing jabberwocky version
    UPDATE: realized this is intended. requirements.txt DOES already pin the version.
-qpt query redesign
    -query_batch
        -update backend.query logging to file to work w/ threads (or whatever method I use in above bullet point)
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/6/22 wed
----------
X -add line in readme about using make hooks
    X -add line mentioning diff versions for gui and alexa? Or could wait til I have scripts to handle alexa dev.
-qpt query redesign
    -query_batch
        -update backend.query logging to file to work w/ threads (or whatever method I use in above bullet point)
            X -write JsonlinesFormatter
            X -write JsonlinesLogger
            UPDATE: decided writing custom logger might be good way to go since logging (IIRC) natively supports multithreading. Could have used lock approach but let's try this first. Also like the idea of keeping all calls rather than overwriting each time.
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/7/22 thurs
------------
X -update gptbackend to log using jsonlines logger instead of htools.save
    UPDATE: had to update JsonlinesFormatter to allow logfile name switch, but after that it seems to work well.
-qpt query redesign
    -query_batch
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/8/22 fri
----------
X -easy: clean up all the old messy query funcs in openai_utils.py (mostly deleting)
-qpt query redesign
    ~ -query_batch
        X -in nb13, test if new logging functionality works with multi-threaded approach
            UPDATE: found bug where logging to file fails silently if file DNE. Added `touch` func and check if logfile exists before logging. Also had to add a threading Lock here, which probably means I could switch back to htools.save if I wanted since GPTBackend has a lock assigned now anyway. Would have to update htools to be compatible w/ jsonlines which I think would be good to support generally, and I think this would remove a lot of the icky logic around recreating JsonlinesFormatter every query. Downside: if I want to print kwargs to stdout, that would now need to be a separate step from logging to file. I think this would mostly be useful if I change logic to fully resolve kwargs w/ current query func - right now it just logs the user-specified params.
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        -implement

4/9/22 sat
----------
[NOTE: most up to date version of GPTBackend is in nb13 cell]
_ -update htools.save to support jsonlines format
    UPDATE: looked into options for implementing this and it's not super straightforward with the current interface. Could add jsonlines lib but I'd like to cut down on deps if anything.
-qpt query redesign
    -query_batch
        _ -consider switching to use htools.save in log_query_kwargs? See discussion in yesterday's daily notes.
        -figure out how to integrate new query_batch func into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
    ~ -figure out how to support stream mode w/ np=1 and nc>1 for backends w/ builtin streaming (think this may be achieved w/ native behavior + slightly tweaked version of stream_multi_response)
        UPDATE: think maybe can just return the native response (note: can't yield from since that makes python treat the method as a generator and all responses are lazily evaluated, even in stream=False mode).
        ~ -implement
            UPDATE: very simple for reasons described above and need to test more, but I think this should work.
        -build default support for nc>1 for backends that don't provide it
            UPDATE: solution is to make all of those backend query funcs return (str, dict) and not provide n in params. Then use new threaded_starmap function to make nc calls. Tentatively seems to work, though I'm surprised - query_batch also uses threads and I read threads within threads are not possible. UPDATE 2: actually might have spoken too soon. Investigate tomorrow.
X -fix new threading bug after deleting files
    UPDATE: realized I needed to move lock to include a bit more logic. Might just want to lock the whole method if another bug pops up in the future, but seems ok for now.

4/10/22 sun
-----------
-query_batch
    X -investigate: does np>1 and nc>1 actually work for funcs w/out native support? Unclear. See cell 129 in nb13.
        UPDATE: unable to reproduce error. Seems fine.
    ~ -adjust interface so returned val has same format for backends w/ native support for np or nc > 1 and those without (realized gooseai and maybe openai charge by the request too, so it's more efficient if we can use the native functionality when possible instead of always using multithreaded approach).
        UPDATE: WIP. Adjusted query_batch to return (texts, fulls) instead of old behavior so we now match openai/gooseai. Still need better testing.
    ~ -add prompt_index (decided rather than overriding openai/gooseai index to create prompt_index and completion_index, just keep index (which is effectively "overall_index") and add prompt_index.)
        UPDATE: seems okay for some cases but still very buggy. See query(str, n=1).
    -write tests in new clean nb to figure out which backends/kwargs combos work (so many variants, need to do programmatically)
        -start with all free backends to root out simplest bugs
        -then do paid backends
            X -write script to save sample response from all relevant combinations of params
                UPDATE: did this for gooseai. Shouldn't need to for openai since it's nearly identical (only known difference I recall is that in streaming mode, openai may shuffle completions together, which I account for elsewhere).
    -figure out how to integrate into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
        -implement

4/11/22 mon
-----------
-query_batch
    X -start new clean nb and use pickled responses in data/misc instead of adhoc
    _ -try passing each pickled response through postprocess_gpt_response
    _ -try passing each of those ^ outputs through postprocess_response
        UPDATE: wrote new query_gpt_mock func that makes it easier to get fake openai/gooseai responses.
    -add prompt_index to all gooseai/openai responses
    ~ -write tests in new clean nb to figure out which backends/kwargs combos work (so many variants, need to do programmatically)
        X -start with all free backends to root out simplest bugs
        -then do paid backends using pickled responses
    X -figure out how to integrate into GPTBackend (should it actually have a public interface or just call it under the hood inside query() depending on the args?)
        ~ -implement
        UPDATE: query() calls _query_batch under the hood and _query_batch calls query n times (str prompt each time instead of list).
X -fix bug in stream_response
    UPDATE: itertools.cycle was returning the same dict repeatedly. We want it to return a new dict at each step.
X -kwarg_grid func to iterate over all relevant kwarg combos

4/12/22 tues
------------
~ -look into issue where logfile is being generated in notebooks dir
    UPDATE: could not reproduce.
X -add prompt_index when stream=True
    X -figure out what the issue is
        UPDATE: prompt_index was only assigned in query method, but we return early when stream=True. Needed to add logic to stream_multi_response().
    X -implement
~ -add support for backends w/ builtin batch queries? (Recall: at least w/ gooseai, it's cheaper to make 1 query w/ 100 prompts than 100 queries w/ 1 prompt each)
    UPDATE: fiddled around a bit and it seems like maybe query should work anyway? Probably easier to diagnose by just trying it.
~ -[BLOCKED: need to figure out if the above bullet is necessary first. Not sure if we can proceed with this yet.] test_backend('mock') and examine results
    UPDATE: initially failed on first set of kwargs so I switched kwarg grid func to do them in an easier order. Bug turned out to be I was checking if n the number rather than 'n' the param name was in query signature. Now they all execute BUT doesn't work correctly when np > 1. Seems to repeat completions - I sense it's related to the fact that query_batch is still being used on it. So we're making 2 calls to query_mock w/ n=1 rather than 1 call to it with n=2, which is why we get the results for the n=1 response. Fix this tomorrow.

4/13/22 wed
-----------
X -update query to only use query_batch if the backend doesn't natively support list prompts (or update query_batch to handle that itself)
~ -make sure prompt_index is correct when stream=True
    X -works for backend does NOT support stream natively case
        UPDATE: added assert in nb test_backend() func to check this. Repeat and banana pass.
    X -works for backend DOES support stream natively case
        UPDATE: wrote new stream_openai_generator func. All tests pass.
    -test huggingface backend
        UPDATE: tried to run but unsurprisingly got 400 error. Maybe if I manually insert wait times or alternate between engine_i values it would work? Try again later.
    -test gptj backend (if back up)
        X -check if back up
        -[BLOCKED: api down] test
-clean up query/query_batch so prompt_index isn't set in so many places (currently 3? end of query, end of query_batch, stream_mluti_response)

4/14/22 thurs
-------------
X -notebooks/data/logs is back. Want to stop this from happening.
    X -Try to reproduce error and identify cause
    X -fix
    UPDATE: because I made jsonlogger a class attribute, the path was resolved before I ran cd_root, i.e. on import. Added C.root var in config. Used this to update load_prompt func and PromptManager so paths are a little less fragile (arguably).
X -investigate what happens when 1 returningthread fails. Think it might not raise the error to the calling context.
    X -try w/ toy example (no query gpt)
    UPDATE: simply returns None in the corresponding list index. Switched thread_starmap to use PropagatingThread by default, though ReturningThread is still an option.
X -think about moving hardcoded params to backend.query and leaving most new funcs to accept mostly kwargs?
    UPDATE: No. Wrote explanation in query_gpt3 docstring.
X -general cleanup of mock_funcs, backend methods, etc. - very messy atm
    X -better document what these funcs need to return
    UPDATE: wrote pretty thorough section on how to implement a new backend func in the module docstring.
-clean up query/query_batch so prompt_index isn't set in so many places (currently 3? end of query, end of query_batch, stream_mluti_response)
_ -check if there are any more warnings from query_gpt3 that need to be moved to gpt.query()

4/15/22 fri
-----------
X -clean up query/query_batch so prompt_index isn't set in so many places (currently 3? end of query, end of query_batch, stream_mluti_response)
    UPDATE: technically it was 4 before and I got it down to 3 (removed query_batch one). Also refactored stream_response (formerly stream_multi_response) to include stream_openai_generator (the 4th case I previously forgot) so it's sort of like 2 places.
-repercussions of gpt.query redesign
    -update convmanager (careful w/ hookedgenerator)
        -maybe change kwargs() depending on active backend?
-repercussions of gpt.query redesign
    -update convmanager (careful w/ hookedgenerator)
        -maybe change kwargs() depending on active backend?
    -update promptmanager 
        -maybe change kwargs() depending on active backend?
X -fix docs in stream_openai_generator explaining how calculation works

4/16/22 sat
-----------
X -clean up misc todos in alexa/app.py (rm old commented out code, etc.)
    X -update app to use banana.dev by default for now
X -update engine names in config
    X -add banana.dev engine names
    X -replace hobby empty strings with engine names
X -document alexa.utils.slot function
X -document a couple CustomAsk methods
-make logging use fully resolved kwargs?
-handle error when wiki person generation fails
-repercussions of gpt.query redesign
    -update convmanager (careful w/ hookedgenerator)
        -maybe change kwargs() method depending on active backend?
    -update promptmanager 
        -maybe change kwargs() method depending on active backend?

4/17/22 sun
-----------
X -add datetime to log meta
X -make logging use fully resolved kwargs?
    UPDATE: investigated this and decided this is no different - individual query_func defaults are basically all overridden by gpt.query defaults.
X -update convmanager and promptmanager kwargs()
    UPDATE: rm refs to mock_func, use GPTBackend.query for bound args. Also added keep=True to with_signature deco so kwargs that are not in query_gpt3 are accepted in bound_args().
-handle error when wiki person generation fails
X -repercussions of gpt.query redesign
    X -update convmanager (careful w/ hookedgenerator)
        _ -maybe change kwargs() method depending on active backend?
    X -update promptmanager 
        _ -maybe change kwargs() method depending on active backend?
~ -test convmanager
    UPDATE: after some fixes to handle new list outputs, seems to work. One remaining issue: confirmed my suspicion that stream mode does not respect my stop_words.

4/18/22 mon
-----------
X -test promptmanager
    UPDATE: seems fine.
~ -handle error when wiki person generation fails
    UPDATE: the error actually does seem to be handled. The problem is if we try to add a new contact and fail, and then try to add another new contact that ALSO does not exist. If we add an existing contact or a new contact that DOES exist, we seem to be okay. Possible fix: I updated _generate_person to use _maybe_choose_persona(), which it was previously missing. Haven't tested this yet.
X -shorten conversation default max_tokens a bit
    UPDATE: dropped this down from 250 to 100 in prompt config - unsure if this will really help or not. I was annoyed at responses getting cut off and I suppose this might actually make it worse - there doesn't seem to be any ability to ask for gpt to actually generate a shorter response, we can just interrupt it sooner.
X -fix bug where choose_person no longer checks response kwarg
    UPDATE: this was that case in delegates where I forgot what it was doing for a while. I think any func that gets called with func_push needs to accept either kwargs or a "response" arg and I'd removed it from choose_person when building out the intent inference system.
-make stream mode respect stop words

4/19/22 tues
------------
X -test possible solution to wiki person generation (recall that trying to generate 2 people without wiki pages in a row was causing an error, but I THINK I fixed this).
    UPDATE: confirmed this works. Also updated queueing interface to push kwargs directly into the same queue - this way, we don't have to worry about forgetting to reset state.kwargs (previously, I was seeing that the generate_person attempt after a failed generation used the same name as the previous call because I wasn't resetting kwargs). Removed state.kwargs entirely.
X -generate new single file prompts from my current 2 file setup
    UPDATE: func to do so is in ipython - might want to save that somewhere. Still need to figure out what to do with old prompt dirs (keep around somewhere for legacy gui) and update load_prompt to use new cfg files.
-make stream mode respect stop words

4/20/22 wed
-----------
X -port yaml generation func to lib/scripts somewhere
X -consider best way to store new prompts (maybe should have a separate ~/prompts dir and repo entirely?)
    UPDATE: eventually, a ~/.jabberwocky or ~/.prompts would probably be a good idea, but for now let's keep everything in 1 repo. Old format prompts are now in data/legacy_prompts, new single-file prompts are in data/prompts.
X -mv either old or new prompts to a new dir (i.e. 1 dir should hold only legacy 2-file prompts, another should hold only new format 1-file prompts)
X -update load_prompt to use new single-file format
    UPDATE: supports both formats (and infers v1 vs v2 format automatically). Also supports different prompt dirs in case I decide to move this eventually.
-resolve 2 utterance conflicts (changePerson apparently soaking up 2 unintended commands in Hush and Oh No)

4/21/22 thurs
-------------
_ -rewrite engine mappings to map from i to "nearest" match? I.e. engine_i=3 for huggingface is currently more like engine_i=1 (or something lower than 3, anyway) for openai. Or maybe support passing in either a name OR a number?
	UPDATE: decided I need a better understanding of the available engines and api endpoints before settling on a method.
~ -read up on newly released engines, maybe take some notes
	UPDATE: took notes in misc.txt. Still have more reading to do on classification, maybe others.
~ -function to upload files to openai (used for search, q&a, classification)
	UPDATE: needs some tweaks to support passing in metadata, labels.
X -write random_str func and add to htools
	UPDATE: haven't uploaded to pypi yet though. This was a used in my upload_openai_files func.
X -skim post on gpt instruct
	UPDATE: sounds like you don't need to use it that differently, the results should just be a bit better for instruct-style prompts.
-resolve 2 utterance conflicts (changePerson apparently soaking up 2 unintended commands in Hush and Oh No)
-update jabberwocky version and upload to pypi
-Harder option: make stream mode respect stop words

4/22/22 fri
-----------
X -finish upload_openai_files func
	X -support labels for clf mode
	X -support metadata for all modes
~ -finish reading clf docs re using logprobs
	UPDATE: read a bit more but I'm finding the classification endpoint kind of puzzling, and I don't have a pressing need for it. Cut this short.
~ -finish reading any other docs that seem relevant in near term
~ -revisit question of how to specify engines in prompt config files (note: some tasks require multiple engines 😱. This may require some changes. I guess we could have a diff format for completion prompts vs. search prompts, for instance.)
	UPDATE: openai is up to 50 models now and will probably continue to grow (though it's basically still 4 for the instruct use case), so specifying engines as ints may be hard to maintain. Also: looked at gooseai pricing again since that's the only one besides openai that is fairly reliable (over, say, 80% uptime) and has multiple engines. And idk if they changed prices or I'm screwing up the math somehow but it seems like it is often more expensive (sometimes 10x!?) than openai. Maybe the answer is just say forget all the other backends and just use openai. But anyway, I think it's clear we should be passing in engine rather than engine_i, perhaps with some extra logic to handle a few special ints.
X -mark some old todos as "NOFIX"

4/23/22 sat
-----------
X -easy: write docs (including updating returns section) for gpt._query_batch method
X -rename np vars in openai_utils to np_ to avoid collision w/ numpy
~ -look into codex "invalid url" error (see ipython pane below)
	UPDATES: curl works, python doesn't.
	Not just codex, now now openai engines work w/ python. Maybe due to updating pip package? Temporarily upped billing limit to try other models.
	Restarted kernel and gpt query works again w/ ada. BUT after I import openai explicitly, that fails too. That must be a clue.
	Tried uninstalling, reinstalling, opened new tmux pane. Still same error.
	Tried deleting 'openai' object and then importing jabberwocky. This does work!?
	If I re-import openai after that, gpt.query still works. But openai.completion with codex does not.
	If I import openai FROM jabberwocky openai_utils, codex query still fails. But gpt.query works. And openai.Completion works w/ engine ada!
	Conclusion: maybe it is codex-specific then?
-resolve 2 utterance conflicts (changePerson apparently soaking up 2 unintended commands in Hush and Oh No)

4/24/22 sun
-----------
X -continue investigating inability to query any openai engine (invalid URL error) after importing openai explicitly
	X -try uninstall and reinstall old version from lib/requirements.txt
	UPDATE: Could not reproduce. Everything seems to work now though /shrug.
X -resolve 2 utterance conflicts (changePerson apparently soaking up 2 unintended commands in Hush and Oh No)
	UPDATE: two collisions are "oh no" and "hush", which are both mistakenly mapped to changePerson rather than built in intents (StopIntent and NoIntent.) Couldn't figure out a godo way to stop this in the UI but I updated choosePerson endpoint to handle it (either use as a response when a conv is in progress or reroute to choosing a person if not).
X -fix prompt loading bug in PromptManager (caused by switching from prompt dirs to single files)
X -write some thoughts on dropping partial sentences due to max length
	UPDATE: see bullets in backlog.

4/25/22 mon
-----------
~ -make stream mode respect stop words
	X -look into pickled stream response format - is it really 1 token per item? How often are common words split up?
		UPDATE: yeah, 1 token.
	X -use gpt tokenizer to see if I can identify a more reliable single token stop word, consistent for all prompts (i.e. let us put some STOP_TOKEN in every prompt and not have to dynamically update that param)
		UPDATE: 1 token prompt is risky in case we accidentally generate it. Uncommon tokens aren't ideal either - mostly random things like "[space]informants". "<|endoftext|>" might work though...I wonder if gooseai would know to omit that. Probably not.
	~ -start prototyping function
		UPDATE: see stream_with_stop func in nb15. Works decently for np=1, nc=1, 1 hardcoded stopword case. May be a bit tricky to generalize to all cases and integrate into existing codebase.
X -fix bug in openai stream mode
	UPDATE: turned out I haven't been passing stream=True to postprocess_gpt_response func in query_gpt3. No clue how those pickled responses work.
X -fix bug #2 in query_gpt3
	UPDATE: turns out zipping a generator with itself was too good to be true. It ends up doing something weird, as documented in the comment above my fix.
X -easier option: email gooseai asking about claims of being cheaper than openai
-try to reduce amount of repetition in gpt responses
    -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    -try tweaking prompt a bit.
    -try with better model (maybe this task just really needs davinci?)
    -try one of the "instruct" models
-more thoroughly troubleshoot "no response" errors in reply (think this is due to huggingface being very slow and gptj being broken, but something to watch out for)
    -re-enable HF punct and try to time (programmatically or watching clock, whichever's easier)
    -if that works, re-enable HF reply as well and time
    -fix if necessary

4/26/22 tues
------------
-stream_with_stop func
	X -update nb15 stream_with_stop func to use <|endoftext|> as stopword (this conveniently gets tokenized as 1 token. While I suspect gooseai will still return the token itself, it should actually stop it from generating more tokens, saving us money.).
	-determine if it needs to support np>1 and/or nc>1
	-determine how to integrate into gpt.query
	-determine how/whether to support more stopwords - can we feasibly update prompts to make every task use the same stopword?
		UPDATE: most few shot prompts would support this no problem. Zero shot prompts probably won't, though they should lean less heavily on stop words anywa.
~ -write func to compute gooseai cost vs. openai cost, given a prompt of known length and an output of max_tokens length (realized gooseai's claim is still plausible given that they don't charge for input tokens, but it's not intuitive in a given situation which backend will be cheaper)
	UPDATE: decided it would help to do this before deciding how much time/effort to devote into stop word removal in stream mode, since that really only helps gooseai. Wrote basic function that doesn't yet support passing in an engine and doesn't make an explicit recommendation of the better choice, but it does return a df with engines/backends sorted by cost.
X -rm use of conversation_formatter func (no longer ideal to use that prompt this way) from openai_utils promptmanager etc.

4/27/22 wed
-----------
-cost estimator func
	~ -support openai names like ada AND 'text-ada-001'
	~ -support passing in engine_i int
		UPDATE: tried a few different approaches and ended up writing a new EngineMap class to try to make this simpler. Almost satisfied but still a bit puzzled as to how to handle names like 'code-ada-001'. Deal with this tomorrow.
	-add sugestion of which backend/engine to use
	-come up with some helpful visualizations/metrics to begin to understand results
		-plot heatmap/3d scatter (something w/ meshgrid, probably) w/ input len and output len as x and y axes and price as color/z dim (prob need to pick single engine "size" to compare? Maybe 1 plot for each "pair".)
		-similar to above bullet but something heatmap-like where each box is 1 of 2 colors, just shows which backend was cheaper? Have to think about how to implement.
-update config backend engine map to align w/ openai idx (i.e. huggingface now has '' for i=2 and 3); need to update other files to reflect this

4/28/22 thurs
-------------
X -finish EngineMap
	X -write down desired input-output pairs (given an engine name and backend, what should output be? Focus on cases like 'code-ada-001'.
	X -based on input-output pairs, implement changes (latest version is in pychar, not jupyter)
X -integrate EngineMap into rest of openai_utils and alexa and anywhere else in project that refs it (recall now some engine() callscould return None)
-cost estimator func
	-integrate EngineMap
	-support openai names like ada AND 'text-ada-001'
	-support passing in engine_i int
	-add sugestion of which backend/engine to use

4/29/22 fri
-----------
X -cost estimator func
	X -integrate EngineMap into estimate_cost func for engine resolution
	X -support option for output name to be base only (e.g. 'ada' rather than 'text-ada'001')
        UPDATE: realized this only makes sense in openai backend case since gooseai names don't contain any of these. Added basify option for when backend is openai.
	X -add suggestion of which backend/engine to use
        UPDATE: now returns dict where everything corresponds to cheapest option, plus a 'full' key where value is df of all results.
X -cost visualizations
	X -come up with some helpful visualizations/metrics to begin to understand results
        X -plot heatmap/3d scatter (something w/ meshgrid, probably) w/ input len and output len as x and y axes and price as color/z dim (prob need to pick single engine "size" to compare? Maybe 1 plot for each "pair".)
		_ -similar to above bullet but something heatmap-like where each box is 1 of 2 colors, just shows which backend was cheaper? Have to think about how to implement.
        X -histogram of diffs between equivalent engines

4/30/22 sat
-----------
X -integrate estimate_cost func into EngineMap? (prices as class vars, func as a classmethod?)
-port iter_engine_names and iter_paid_engines
    -document
_ -consider porting visualization funcs? Maybe not.
    UPDATE: I did do some more plots and cost estimation for conv-specific prompts though. Seems like I start using much longer prompts (which is highly possible if I work on a NLCA style agent in the future), it's fine to stick with openai for the most part.
~ -add auto-backend select mode to gpt? (We usually don't know prompt length so we might need to tokenize, which could slow things down a bit. But actually gpt tokenizer isn't that slow, IIRC.)
    UPDATE: not yet, but did add a dev_mode option to print out estimated costs for now.

5/1/22 sun
-----------
X -port iter_engine_names and iter_paid_engines
    X -document
X -try out dev_mode a bit (debug, also build intuition)
~ -add auto-backend select mode to gpt? (We usually don't know prompt length so we might need to tokenize, which could slow things down a bit. But actually gpt tokenizer isn't that slow, IIRC.)
    UPDATE: a bit tricky. Converted gpt.query and query_batch to instance methods and made GPTBackend instance part of the module automatically so we have access to the contextmanager in query. Then started implementing optimized() contextmanager. Still needs work.
-continue stream_with_stop func
    -consider: is this still necessary given my findings that gooseai isn't really saving money in most cases?
	-determine if it needs to support np>1 and/or nc>1
	-determine how to integrate into gpt.query
	-how to deal w/ zero shot prompts that can't easily incorporate <|endoftext|>
	-test my <|endoftext|> stopword in gooseai ui - need to make sure this is even a valid strategy

5/2/22 mon
----------
X -finish optimized() contextmanager to auto-optimize query cost
    X -consider interface more: should this instead RETURN a ctx manager (either self(best_backend) or dummy()) and then run the rest of the query in that? Or should it actually allow us to enter the manager itself (as I have with the current implementation)?
        X -key consideration: how to avoid calling this on every query call in query_batch case? Previous integration into query method achieved this by setting optimize_cost=True, but with my current implementation that var is obscured inside the optimized() method so I don't think that works.
    X -briefly test out new functionality (2 changes: separate query/_query methods and optimize_cost option).
    UPDATE: separated functionality into _query and query methods, where the latter is the only one to possess an optimize_cost arg.
X -bug(ish) fix: replace all old cls refs w/ self in query since its no longer a classmethod
-alternate option: try to reduce amount of repetition in gpt responses
    -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    -try tweaking prompt a bit.
    -try with better model (maybe this task just really needs davinci?)
    -try one of the "instruct" models

5/3/22 tues
-----------
-continue stream_with_stop func
    ~ -consider: is this still necessary given my findings that gooseai isn't really saving money in most cases?
        UPDATE: marginally useful and strongly considered skipping, but this felt like an excuse to avoid having to get it working. Ended up writing StopWordStreamer class using trie that supports multiple stop words and seems to work pretty well. Still needs some work to integrate it into lib though.
	-determine if it needs to support np>1 and/or nc>1
	-determine how to integrate into gpt.query
	~ -how to deal w/ zero shot prompts that can't easily incorporate <|endoftext|>
        UPDATE: seems like we can't rely ONLY on endoftext as stopword. If we're really going to support stop words for streaming, we need to allow any stopwords.
	~ -test my <|endoftext|> stopword in gooseai ui - need to make sure this is even a valid strategy
        UPDATE: tried briefly and it seemed ok in the cases I tried, but in the openai forums an openai employee recommended against using this (David Shapiro was actually the asker lol). They said the instruct models should be pretty good at this already. But I suppose I'm thinking more about the gooseai case, which doesn't yet have instruct-tuned models.
-fun option: support custom persona (speak bio yourself) in alexa
-yet another alternate option: try to reduce amount of repetition in gpt responses
    -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    -try tweaking prompt a bit.
    -try with better model (maybe this task just really needs davinci?)
    -try one of the "instruct" models

5/5/22 wed
----------
-continue StopWordStreamer class
    X -does this need to implement its own logic about index/prompt_index/finish_reason? Or is it possible to sneak this into an existing function?
        UPDATE: we either need to add it here or integrate it into another func. This interface seems more promising tbh so I'm thinking we can try to add it here.
    X -determine if it needs to support np>1 and/or nc>1
        UPDATE: yes, just pay attention to index (NOT prompt_index - the decision to stop should affect a single completion, not all completions for a prompt). Threading is used when we have np > 1 but that's irrelevant here.
    X -need to make stream method accept a (text, full) tuple?
        UPDATE: yes, we'd be streaming from a generator of (text, full) tuples.
    X -update finish_reason
    X -accept (text, full) tuple
    X -add support for nc > 1
    -integrate into gpt.query (replace/build into one of the stream funcs?)

5/5/22 thurs
------------
-StopWordStreamer
    X -consider which interface I like: create streamer once and reset each time, or create new streamer for each completion. Depends where I envision making/storing this obj - if it's in GPT (which can use any prompt), we may want to create new obj each time since stop words are diff for each prompt.
        UPDATE: decided to revert to pattern of new object for each query, since stopwords can vary not just by prompt but also by query (see extra_kwargs).
    X -fix bug: doesn't work w/ multiple stopwords of diff lengths
        UPDATE: quite a difficult bug but I think I got it. Updated htools.TrieNode to track its own depth AND allow passing a new TrieClass to htools.Trie. Then make the streaming method return an int instead of bool, telling us how many tokens the found stop word was.
    ~ -integrate into gpt.query (replace/build into one of the stream funcs?)
        UPDATE: I think the easiest/safest thing to do would be keep this just for the gooseai case. Useless and potentially buggy for openai, and the rest of stream_response does a bunch of weird logic to assign indices which gooseai doesn't need. Just keep it as a subcase of real_stream I think. Also: I'm realizing I could potentially add this as an intermediate step before calling stream_openai_response, i.e. yield from stream_openai_response(gen=streamer.stream(gen)) but it's unclear if that would be good. Have to consider.
X -write wrapper func to avoid directly dealing w/ stopwordstreamer
    UPDATE: written but not yet ported or documented.
-fun option: support custom persona (speak bio yourself) in alexa

5/6/22 fri
----------
~ -finish stopword streaming functionality
    X -consider: should index handling occur here? Or reuse stream_openai's functionality (see yesterday's notes on integration).
        UPDATE: set index in _stream_gooseai_generator(). Sufficiently different from stream_openai that I don't want to rely on that - cleaner to just create new stream gen for each class of backend behavior.
    X -support np>1 and nc>1 scenario
        UPDATE: realized streamer can only act on 1 prompt unless we even more deeply nest self.q/self.done/etc. Updated stream_gooseai to create np streamers and call them a step at a time. This required refactoring stream() into something that can be called step by step. Also fixed bug with full['index']: need to modulo nc when there are multiple prompts, even though each streamer only deals with 1, because of how openai assigns indices.
    -port stopword streamer cls
    -port stream_gooseai func
    -add as option to utils.stream_response
    -update call in GPT.query
    -update warning messages in GPT.query? Now some stream modes support stop words.
    -maybe refactor rest of stream_response into stream_static() and make stream_response just delegate to diff generators (which it will already mostly be doing)? Could also potentially make this FAR simpler since these inputs aren't actually generators.
-integrate into gpt.query (replace/build into one of the stream funcs?)
-fun option: support custom persona (speak bio yourself) in alexa

5/7/22 sat
----------
X -finish support for stopword filtering in streaming mode
    X -port stopword streamer cls
        X -document
        X -add constructor for from_trie
        X -add constructor for from_stopwords
    X -port stream_gooseai func
    X -add as option to utils.stream_response
    X -update call in GPT.query
    X -update warning messages in GPT.query? Now some stream modes support stop words.
    X -maybe refactor rest of stream_response into stream_static() and make stream_response just delegate to diff generators (which it will already mostly be doing)? Could also potentially make this FAR simpler since these inputs aren't actually generators.
    UPDATE: implemented as planned but untested - prob a bunch of small bugs to iron out. Do that tomorrow.
X -give gptbackend a tokenizer attr to pass to _stream_gooseai_generator
-fun option: support custom persona (speak bio yourself) in alexa

5/8/22 sun
----------
X -refactor method call in gpt.query so call only occurs once instead of repeating with _query and _query_batch.
~ -finish stopword streamer integration
    X -confirm: is the stop I pass to stream_response from gpt._query() fully resolved? Should it be? 
        UPDATE: no, but query funcs don't have default stop values so there's no difference.
    X -confirm: what happens if I pass empty list to StopWordStreamer?
        X -what about if I pass None? There could plausibly be no stopwords, but I guess gpt._query() call uses empty list as backup val.
        UPDATE: added check to error out if we try to create a streamer with no stopwords - no point. Just hardcoded if/else in stream gooseai generator. Considered making a dummy mode for stopword streamer for when no stopwords are present but it seemed kind of pointless.
    ~ -test new stopword streaming functionality for each backend
        -openai
        -gooseai
        X -hf
        X -hobby
        X -mock
        X -repeat
X -rewrite pretty_tokenize to avoid bug with special characters
    X -update stream_gooseai gen to be compatible
    X -update gpt.query
    X -update Stopwordstreamer
    X -remove tokenizer loading from gpt
X -try removing jupyter files from github language calculations (may need to wait for github to reindex it before changes show up)
-maybe apply static stopword truncation func to each text response in stream_fake_generator. We actually do have the full strings upfront in this case so that should be very doable.


5/9/22 mon
-----------
X -add subwords option to gpt.query (or somewhere?) so users can choose between streaming words and subwords for static backends.
_ -update static stopword truncation func to use ByteLevel() like in pretty_tokenize? (looks like HF, for one, might actually try to truncate before the stopword but it only removes the last 1 token, which doesn't do what we want if stopword is multiple tokens - at least that's what it looks like).
    UPDATE: After some testing, I think my partial_trunc option already takes care of this satisfactory degree.
X -fix bug in stopwordstreamer (wasn't truncating as expected on a new sample case)
    UPDATE: Added strip_spaces option to deal with this - turned out the issue was I specified a stopword with a trailing space (to try to minimize the chance of unintentional truncation) but spaces are typically assigned to the next token. A more thorough solution might be to completely rewrite streamer trie to be character level but I think this solution works well enough, especially since specifying stopwords with trailing spaces seems difficult with yaml file.)
X -maybe apply static stopword truncation func to each text response in stream_fake_generator. We actually do have the full strings upfront in this case so that should be very doable.
-test new stopword streaming functionality for paid backends
    -openai
    -gooseai
-test backends on other cases
    -np > 1
    -nc > 1
    -nc > 1 AND np > 1

5/10/22 tues
------------
X -consider how to avoid ending a reply mid-sentence? (Ideally api would do this automatically but it doesn't seem to)
	X -option 1: update postprocessing to check for cutoff sentences and leave off the end bit. Maybe simple rule, maybe nltk/spacy - depends what I want to use this for. Some completions may be valid when not ending in a period/full sentence, others may not be.
        UPDATE: decided to just base this off punctuation and disable it by default, along with some other limitations on when it's used (see misc.txt for today).
	~ -option 2: try prompting gpt3 to generate a specific number of sentences/words, and/or only generate full sentences. (Basically, making it "aware" of its own hyperparameters.)
        UPDATE: still like the idea of prompt mixins, but found this particular one didn't seem all that effective, at least with my initial phrasing/settings.
	_ -option 3: additional model (could be gpt) to trim off any partial sentences.
        UPDATE: probably effective but don't want to slow things down any further with additional queries - we're already pushing it time-wise.
-alternate option: try to reduce amount of repetition in gpt responses
    -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    -try tweaking prompt a bit.
    -try with better model (maybe this task just really needs davinci?)
~ -briefly try out my NL unit test idea and some other misc ideas in openai playround
    UPDATE: playground does not support the logit_bias arg, which I think is key here. Not perfect but there's some promise. Not sure if was just the prmopts I happened to try today but I was getting WAY better results with openai than gooseai.

5/11/22 wed
-----------
X -try a few short calls in aws console - got sidetracked a bit with library dev tasks so it's been a while, and I want to make sure everything still works
    X -assess repetition problems with different backends/engines
    UPDATE: also found and removed old bug where change_model changed nonexistant "model_i" param (now "engine"). Also updated error message to use "model" instead of "backend".
~ -try to reduce amount of repetition in gpt responses
    X -try with better model (maybe this task just really needs davinci?)
    X -try increasing repetition penalty (or presence penalty? Need to refresh my mind of details/best practices re values to use, etc.)
    _ -try tweaking prompt a bit.
    UPDATE: Was seeing a lot of repetition in alexa responses but less in console. Tried upping frequency_penalty anyway (NLCA book recommends .5-1, while openai playground default is 0. I was using 0.1 and moved it to 0.5 now.) Also experimented with bio cleanup prompt (sometimes my regex-based parsing doesn't work perfectly). Had some success in playground but it's also not perfect.
X -add new bio cleanup prompt
    UPDATE: "wiki_bio_cleanup". Also added a "social_hypotheses" prompt.
X -start prototyping func to support remote prompt loading (would be good if jabberwocky was less wedded to local files)

5/12/22 thurs
-------------
X -add support for remote prompts
    X -select desired interface (maybe one func where you can either pass a local prompt dir or a url fmt)
        UPDATE: load local by default, but if url is provided that takes priority.
    X -finish implementing (see load_remote_prompt draft in ipython below)
-incorporate new bio cleanup prompt into convmanager
~ -add support for "default" section in load_prompt (e.g. if we have a list prompt of n items, we might want to let the user adjust n but fallback to a default of 3)
	UPDATE: seems like best way to do this would be by writing a Prompt class that can store defaults and adding custom logic in the format method. Made a lot of progress on this but prob still have some work to do.
-add "docs" arg for prompt yaml files containing more details on what they do?
-review old notes and online sources re adding email integration. 

5/13/22 fri
-----------
~ -finish Prompt class (see data/tmp/prompt.py - maybe good to move to nb now. Getting annoying scrolling through ipython cells.)
	X -test adding default args in prompt
	X -add ability to store more kwargs (e.g. engine, max_tokens, temperature)
	-try incorporating into a prototype version of load_prompt
	UPDATE: for now, added classmethod to Prompt which loads yaml using load_prompt. This is a little weird so might end up changing it later.
	-if interface seems promising, maybe update prompt_manager/conv_manager etc. to use this instead of storing dicts
	X -potential extensions: specify unit tests (natural language or otherwise)
		UPDATE: added postprocessors and validators, 2 lists of functions to apply after querying.

5/14/22 sat
-----------
X -move tmp.py elsewhere and adjust prompt tmp.yaml accordingly
X -more updates to Prompt class (see data/tmp/prompt.py - maybe good to move to nb now. Getting annoying scrolling through ipython cells.)
	X -validate that if defaults are provided, they are a subset of the available fields
	X -allow passing in *args or **kwargs to resolve. Passing in a dict is a little clunky.
	X -refactor resolve() to use Signature and bind()
	-consider: should postprocessors and validators be called automatically after a query? (Either for all queries in GPT or in prompt/conv manager(s)?)
-if I like Prompt interface, update load_prompt/prompt_manager/conv_manager etc. to use this instead of storing dicts

5/15/22 sun
-----------
~ -more prompt class updates
	_ -consider: should postprocessors and validators be called automatically after a query? (Either for all queries in GPT or in prompt/conv manager(s)?)
		_ -should validator raise an error when broken or return false? Should it return true or the completion str when it passes?
	UPDATE: made some updates but there are still a lot of issues to work out which will take a while, and because there's no immediate use case, I made the call to postpone this. I'm getting sidetracked with all these openai library wrapper changes which are out of scope of the original alexa project plan.
_ -if I like Prompt interface, update load_prompt/prompt_manager/conv_manager etc. to use this instead of storing dicts
-alternate option: review old notes and online sources re adding email integration. 
	-get consent_card working on use_email endpoint (then refactor to have this automatically done on skill launch?)
X -move tmp.yaml to a new experimental prompts dir (don't want unsupported fields causing any surprise errors)

5/16/22 mon
-----------
[THEME: return focus to alexa and start to think about how to wrap up skill v1. Enough getting sidetracked on (admittedly interesting) openai wrapper stuff.]
~ -review old notes and online sources re adding email integration. 
	-get consent_card working on use_email endpoint (then refactor to have this automatically done on skill launch?)
	UPDATE: tried again but still get 403 error (basically unauthorized to access info) when trying to retrieve user email. Also tried voice consent again but still get that weird mystery "audio only" blip response and then the app exits. Enabling permissions on that amazon website seems to briefly work (sometimes?) but it gets reset very easily - maybe after a few minutes or when one session ends?
X -make default option in alexa conv to drop last fragment
X -fix bug where auto punct was performed regardless of setting
-alternate: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

6/17/22 tues
------------
X -find gmail alternative for sending emails (less secure app access option expires 5/30/22, and even without that it's proving difficult to disable 2FA for just the jabberwocky account)
	UPDATE: outlook seems to work fine.
~ -try adding account linking instead of user permissions method
	UPDATE: found good tutorial (link in misc.txt) but I don't think it ended up being used bc for some reason now the regular permissions are still working (recall in the past they would frequently get turned off by amazon for some reason).
~ -allow user to add api key (openai/gooseai etc.) (Note: maybe this would be an alternate way to get user name/email? Allow them to type it in in app?)
	UPDATE: did more digging on this and it sounds like there's not a good way to do this natively. Only options are to have the user read it 1 character at a time (very error prone, though maybe doable with some custom logic in the app) or create a site where user can enter it in.
-alternate: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/18/22 wed
-----------
~ -see if I can access polly.ai voices
	X -change speaker gender based on current persona
	UPDATE: currently just uses 1 pre-selected voice for each gender, but I just 3 voice options for gender. Need to extract/infer nationality for each speaker for this to work.
	-add emotion markup (start w/ hardcoded)
	-prototype prompt: gpt3 infers emotion tags based on response text
-revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/19/22 thurs
-------------
~ -polly.ai voices
	X -see if alexa lets us check which default voice is currently in use (thinking we could select 1 paid voice and 1 free one, i.e. use default alexa voice for women and polly for men or vice versa)
		UPDATE: couldn't find any way to get this info from alexa.
	~ -support both neural and standard modes in alexa.utils.voice() func
		~ -find how to specify neural vs. standard in markup
			UPDATE: found "engine" parama in polly python api. Haven't yet found if/how this is exposed to alexa.
		-implement
	-consider how to infer/extract nationality
		X -choose first method to try (e.g. ask gpt, Q&A model on wiki summary, google search)
		~ -implement
			UPDATE: started with HF QA model. Results look pretty solid from wiki summary. Need to clean up a bit to align w/ polly voices (map Greek to European, Texas to American, etc.) Also tried Q/A on whole wiki summary rather than my truncated version and it did work better for beyonce. Wiki does have a "Born" section - might be more reliable to use this, or a combination of the two.
	~ -try emotion markup (see if console lets us do this)
		UPDATE: only supported by native voices and only supports disappointed and excited, but it is cool. Spent some time trying various sentiment scorers/zero shot models/emotion classification models to see if we can detect these emotions quickly and accurately. Started evaluating on sms dataset. Also messed around a bit in polly console w/ pitch/prosody. See misc.txt for settings I liked for salli voice.
	X -see if we can find somewhere in console showing usage/charges (prob not, knowing amazon)
		UPDATE: Looks like no :(
-[BLOCKED: try manual markup first to get a sense of how this works] prototype emotion markup (Note: most gpt options will likely be too slow for alexa, but still could be cool to prototype for future use in ???)
	-have gpt3 insert tags directly (i.e. Edit rather than complete)
	-have gpt3 select from options (almost like classification. Maybe just use logit_bias.) then fill in template programmatically.
	~ -smaller model running locally (Q&A model, sentiment scorer, etc.). Limit to a much smaller subset of markup patterns.

5/20/22 fri
-----------
X -debug why two personas are loaded when only 1 was specified
	UPDATE: forgot that howard was a custom persona. Deleted him from that dir and removed the limitation for loading 1 person, since it takes ~.03seconds and only executes once on app start anyway.
-polly.ai voices
	X -check if polly response includes an "engine" param where we can specify neural or standard
		_ -if yes, add support for both
		UPDATE: all I could find was a message from 3-4 years ago from an amazon rep saying alexa did not support the engine param yet and only provided the standard voice. No clue if that's still true but I guess I'm stuck with whatever it is.
	X -continue work on extracting nationality from wiki (see nb17)
		X -fetch wiki page for each persona
			_ -might need to fix func since disambiguation doesn't seem to be working how I expected
		UPDATE: realized lib's builtin fallback method only fixed 1 error but introduced 12 new ones (for 27 people) compared to my method. Stick with mine.
		X -try computing QA on full summary
		X -try doing my html extraction technique
		_ -check if this q/a model is purely extractive and if so, try one that isn't
		X -postprocess results to be able to map to a polly voice (american/english/australian)
		UPDATE: finished prototyping func but need to figure out where to fit it into lib.
	-emotion classifier for polly emotion tags
		-review more emotion samples at various predicted scores (nb17). Try to get a sense of what decision threshold to use.
		-is this even worth it? Might only be <5% of responses. But maybe that will make things feel more realistic.
	-update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.

5/21/22 sat
-----------
~ -integration voice/nationality extraction
	X -port func to lib (maybe rm scores? Or just ignore them in wiki_data func or whatever calls it. Not bad to have for inforational purposes.)
	X -where should this go? Loading q/a model could be slow.
		UPDATE: made it loaded lazily but updating openai_utils (or modules it imports) is still super slow. That could be an autoreload issue though - my lazy loading seemed to remove the model load message so I think it's working.
	X -update various openai classes accordingly (wiki_data() is used in a few places)
	X -update alexa voice selection function
		X -test in console
			UPDATE: works, though the male australian voice sounds a lot more robotic than in the sample.
	_ -document how to adjust this if polly adds more voice options
		UPDATE: my implementation method actually makes this happen automatically. Adding new attrs to wiki_data() is another story, but I think there are ways around that. (I.e. if we want to add more attrs in the future, maybe just compute them externally to ConvManager).
-nb17
	-emotion classifier for polly emotion tags
		-review more emotion samples at various predicted scores (nb17). Try to get a sense of what decision threshold to use.
		-is this even worth it? Might only be <5% of responses. But maybe that will make things feel more realistic.
	-update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.

5/22/22 sun
-----------
X -troubleshoot why openai_utils is SO slow to reload now
	UPDATE: ~2 seconds seem to be external deps that have been here for a while, ~2 seconds was the addition of a transformers import, ~1s was everything else, and ~3 seconds seems to be from using autoreload in a big notebook (i.e. it's not present when importing the module fresh for the first time). Added default html parsing option for birthplace extraction and adjusted convmanager to handle passing in a qa pipe.
-nb17
	-emotion classifier for polly emotion tags
		X -review more emotion samples at various predicted scores (nb17). Try to get a sense of what decision threshold to use.
		UPDATE: decided this dataset is just not right for my use case. Found a better one in amazon common sense conversations and computed new scores on the validation set (1k samples; train would have been a bit slow at roughly 10x that). Still pinning down rules but results are looking better so far.
		~ -is this even worth it? Might only be <5% of responses. But maybe that will make things feel more realistic.
			UPDATE: with commonsense dataset, looks like it could be quite a bit higher. (Probably shouldn't go TOO much higher though - alexa cautions against over-using emotion tags).	
	-update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.
-makefile/cli command to generate new prompt skeleton
-makefile/cli command to generate new custom persona skeleton

5/23/22 mon
-----------
X -nb17
	X -review more samples from commonsense dataset to select thresholds/heuristics for excitement (closest option in classifier is "joy") (NOTE: the level=High rows look pretty good, but the level=Low are a bit weak. Maybe should up that threshold.)
		UPDATE: in my eyes this really seems to hinge on the exclamation point. Withouto that, require score >= .9. Now we have ~9% excited turns which seems more reasonable than the 14% I was getting at the start of this work session.
	X -follow a similar process to define thresholds/heuristics for "disappointed" (closest option in classifier is "sadness")
		UPDATE: about 6% excited, 6% disappointed, 88% neutral. Sounds reasonable enough?
-update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.

5/24/22 tues
------------
X -port emotion func to lib
	X -consider: should emotion pipeline go inside that or be separate? Maybe just one emotion_tagger func that maps from text -> tag.
X -update voice func to include all options (voice, pitch, emotion tags). Might need some sort of stack of open tags, sort of like leetcode brackets lol.
	UPDATE: works, though we seem to be forced to choose between emotion tags and custom polly voices. I favor the latter atm since emotion tags affect only a small % of responses and are not all that noticeable, compared to speaker gender/accent.

5/25/22 wed
-----------
X -troubleshoot why saying just last name in choose_person is not working
	UPDATE: I was lowercasing personas() but not the prompt slot name.
X -makefile/cli command to generate new prompt skeleton
	UPDATE: also tweaked query_gpt3 defaults as a result: top_p 1->.99.
X -makefile/cli command to generate new custom persona skeleton
	UPDATE: also changed from storing meta.json to meta.yaml to make it easier to create custom personas manually. Then generated all 31 personas over again (even with no qa pipe, this still takes ~3 min - slower than I remembered).
~ -prototype gpt Emotion Markup Language prompt
	UPDATE: not really necessary atm since polly doesn't provide many emotions and we wouldn't have time for another gpt query anyway, but this is pretty cool. GPT3 sort of made up its own markup language based on 1-2 examples! Technically it turns out emotion markup language is a real thing, but the syntax is different than what I specified and gpt matched my syntax, not the real language's (i.e. it's inferring syntax rather than regurgitating it). That being said, if I actually wanted to use this for something, the real EML syntax (or something closer to it) is probably better - I think that would allow us to parse it as xml or something like it, whereas my current syntax would be trickier to parse. Might be desirable to use logit_bias to promote a fixed set of tags but it depends on the use case (currently: None) and I don't think it natively supports multi-token biases so we'd either be out of luck or have to devise a hacky workaround. I am curious to see if we could use gpt to invent other more useful languages though.

5/26/22 thurs
-------------
X -easy option: add "doc" field to prompt
	X -update template
	X -update existing
	X -update load_prompt if necessary
	UPDATE: atm load_prompt just pops it off and does nothing with it. If I revisit my idea of a Prompt class, it could be useful to keep around but not pass to gpt.query().
-provide makefile command to update prompt dir readmes using new "doc" field
	X -figure out plan (bash? jabberwocky? htools?) for how to implement
		UPDATE: write py func in jabberwocky and call it from makefile inside `make readmes`.
	X -implement
		UPDATE: basically done, though slight tweak needed: '<|endoftext|>' is used in one prompt 'doc' and gets cut off, I think because it's displayed in a markdown table in the readme. Need to figure out some way to escape it.
-try tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/27/22 fri
-----------
X -try to escape text like '<|endoftext|>' so it can show up in a markdown table.
X -decide where to store last night's prompt prototype script; maybe copy to or ref in other dev nb so I can see both if I return to it at some point
X -try out other templating engines
	X -jinja2
	X -mako
	UPDATE: see nb16 for notes on this, as well as pasted code from new prompt class noodling.
	~ -watch textx walkthrough
	UPDATE: not convinced the level of completity:utilty is worth it here.
-consider how to deal with prompt versioning (i.e. should we explicitly version prompts inside the yaml file? Or just rely on github commits and dates from logs? Probably a good idea to include a version number and include it in meta field of logs.)
-try tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/28/22 sat
-----------
X -see if I can publish a WIP private skill for me only (so I can test it on echo rather than the test UI)
	UPDATE: already available on my echo, no need to publish actually.
X -generally try interacting with it on echo to find new bugs to fix
X -add fuzzy matching for mis-transcribed names
X -use correct name from wiki page in cases where I misspelled name (e.g. "apollo ohno")
X -regenerate ohno persona
~ -troubleshoot outlook email sending
	UPDATE: was unable to reproduce issue after initial failure. Keep an eye out for future issues.
-consider how to deal with prompt versioning (i.e. should we explicitly version prompts inside the yaml file? Or just rely on github commits and dates from logs? Probably a good idea to include a version number and include it in meta field of logs.)
~ -try tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
    UPDATE: not immediately clear if this makes a difference.
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/29/22 sun
-----------
X -get skill back up and running after mac os updates
    X -troubleshoot xcrun error when running 'make ngrok'
    X -see if we need to update IP address in alexa console
        UPDATE: yes. Seems we need to rebuild too for new endpoint to be used.
~ -add error handling for if email send fails
    UPDATE: seems to be already there. I recall seeing an error message in the console but I that may have just been logged by one of the intermediate functions. Nothing to do here atm.
X -add method to give a cleaner way to check if ConversationHandler is mid-conv
~ -look into outlook email server name in htools. Should I be using "smtp.office365.com" instead?
    UPDATE: seems like maybe they use slightly diff protocols (?) or something to send emails. Tried again with current setup and it executed w/out error so /shrug.
X -add "nobody"/"noone" to skill model in UI as another valid way to quit.
    X -figure out how to implement
        UPDATE: originally started to add new intent but I want to capture the whole utterance which is basically just delegate(). Decided to try integrating it into that first.
    X -implement
    X -test
~ -consider how to deal with prompt versioning (i.e. should we explicitly version prompts inside the yaml file? Or just rely on github commits and dates from logs? Probably a good idea to include a version number and include it in meta field of logs.)
    UPDATE: gave dvc a brief try but didn't seem particularly useful for my purposes. I think for now maybe I'll just add a version field to everything, log it, and in the future if I really need to dig into past versions I can dig up the relevant git commands.
-further experiments with tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 

5/30/22 mon
-----------
X -add Version as prompt field
    X -update existing prompts
    X -update prompt template
    X -update load_prompt() and/or gpt.query() logging to handle this
        ~ -consider: should we have a "meta" field in yaml file and make things like reminder/doc/version subfields of that? Maybe easier to handle in load_prompt/logging.
    UPDATE: keep things very simple for now since I might redesign prompt obj later. Just add a single int version field tracking the text portion of the prompt.
X -debug why logging doesn't seem to be working
    UPDATE: convmanager and promptmanager call GPT.query(log=self.log_path) rather than with log=True so they were using custom log paths. Changed them to use the GPT default path for now but still need to consider if that's desirable.
-consider how to deal with minor problems identified during last night's conv
    -500 error during granger conv (couldn't find in tmux history sun though)
    -can't change settings when asked "Who would you like to speak to?". Only when a conv is ongoing, it seems. Could have sworn this worked before.
    -issue of completions piling up after 1 missed response (i.e. I took too long to think/speak, it cut me off which was sort of disorienting and I missed my next turn too, and at some point kind of lost control of it - it was just returning completions faster than I could keep up. Good exercise for thinking on feet tbh.)
    -transcription quality was weaker than I'd hoped. Might be addressed by enabling auto-punct, though timeouts might be a risk. Responses were still surprisingly decent given how noisy the inputs were.
-further experiments with tweaking conversational prompt wording (e.g. "a transcript of a conversation" instead of "a conversation")
-Harder option: revisit question of settings scopes (global, conversation, persona, etc.). Is there a non-insanely-complicated solution that would still be somewhat useful? 
X -fix new generate_persona bug
    UPDATE: previously assumed each wiki page had >0 images. Added extra logic to account for the other case.

5/31/22 tues
------------
X -set male "other" voice to British and make "other" accept dicts as well as strings (male australian voice isn't so good after all)
X -explore possibility of displaying persona image in card (similar to what we did in GUI)
    X -what are requirements for small_image_url and large_image_url? Do they require exact sizes? If so, using the wiki img_url may not work.
        UPDATE: sizes seem to be recommended, not required. You can also pass in only 1 - it just may stretch/compress it depending on the available screen size.
    _ -try passing in a hardcoded url - does card display work at all?
        _ -can we easily make it persist throughout? Or do we need to do this on each response? (Extension idea: when generating a persona, use google image search + classifier to save pictures for a range of emotions. Then depending on the emotion pipeline's assessment of their response, display the proper image to accompany each reply.)
        UPDATE: sort of unclear if this is behaving as expected. It seems like I need to manually drag down my feed so it refreshes in order to see the card. Anyway, sending a new card for each reply would not be desirable - right now it just stays at the top so it effectively persists, or at least is closer to doing that than sending a new card each time.
    X -if promising, update ConvManager accordingly:
        X -keep img_url field when saving new persona
        _ -add dummy img_url field when one doesn't exist? I.e. for custom persona.
            UPDATE: no, just use get().
-logging updates
    -consider whether I want to always use GPT default daily log file or let Convmanager and/or Promptmanager maintain diff files
    -implement changes if necessary (search "log=True" to see usage in conv and prompt manager classes. 
    ~ -possible GPTBackend bug: we set default log file path as a class attr which probably means if we leave it running for days, we only create a file with the date we launched the program
        UPDATE: trying to auto-update log file name at midnight automatically. Still need to check if this works (tomorrow). Didn't use built in class bc it looks like it deletes the old file and it's unclear how threadsafe it is.
-consider how to deal with minor problems identified during last night's conv
    -500 error during granger conv (couldn't find in tmux history sun though)
    -can't change settings when asked "Who would you like to speak to?". Only when a conv is ongoing, it seems. Could have sworn this worked before.
    -issue of completions piling up after 1 missed response (i.e. I took too long to think/speak, it cut me off which was sort of disorienting and I missed my next turn too, and at some point kind of lost control of it - it was just returning completions faster than I could keep up. Good exercise for thinking on feet tbh.)
    -transcription quality was weaker than I'd hoped. Might be addressed by enabling auto-punct, though timeouts might be a risk. Responses were still surprisingly decent given how noisy the inputs were.
    -Seems like reprompt might not be working right. E.g. if they ask me to choose who to speak to on startup and I don't answer at first but then do on reprompt, it failed.
    -"Goodbye" during conversation raises notimplementederror (see misc.txt)

6/1/22 wed
----------
~ -logging updates
    ~ -check if new logging file updater works (i.e. if I make a query, does it now write to 6/1/22 file instead of 5/31/22?)
        UPDATE: saw in logs that update failed. Turned out I referenced wrong var name. Fixed now but need to wait til tomorrow to see if it all works.
    X -consider whether I want to always use GPT default daily log file or let Convmanager and/or Promptmanager maintain diff files
    X -implement changes if necessary (search "log=True" to see usage in conv and prompt manager classes. 
        UPDATE: just make everything use the same GPT log file. Simplicity...
-consider how to deal with minor problems identified during last night's conv
    -500 error during granger conv (couldn't find in tmux history sun though)
    -can't change settings when asked "Who would you like to speak to?". Only when a conv is ongoing, it seems. Could have sworn this worked before.
    X -issue of completions piling up after 1 missed response (i.e. I took too long to think/speak, it cut me off which was sort of disorienting and I missed my next turn too, and at some point kind of lost control of it - it was just returning completions faster than I could keep up. Good exercise for thinking on feet tbh.)
        UPDATE: added 1 reprompt which doubles the allowed thinking time. Seems to work fine and feels less rushed now.
    -transcription quality was weaker than I'd hoped. Might be addressed by enabling auto-punct, though timeouts might be a risk. Responses were still surprisingly decent given how noisy the inputs were.
    -Seems like reprompt might not be working right. E.g. if they ask me to choose who to speak to on startup and I don't answer at first but then do on reprompt, it failed.
    -"Goodbye" during conversation raises notimplementederror (see misc.txt)
X -support "no thank you" and "yes please"
    UPDATE: tested this and it already seems to work.

6/2/22 thurs
------------
~ -check if logger successfully created file for 6/2
    _ -try new query to make sure it gets written there (probably would be empty initially even if the creation did work overnight)
    UPDATE: needed to rm parentheses in Lock context manager. Try again tomorrow.
-consider how to deal with minor problems identified during recent user testing
    _ -500 error during granger conv (couldn't find in tmux history sun though)
        UPDATE: logs are gone, trash is empty, not in tmux history anymore either. /shrug
    -can't change settings when asked "Who would you like to speak to?". Only when a conv is ongoing, it seems. Could have sworn this worked before.
    -transcription quality was weaker than I'd hoped. Might be addressed by enabling auto-punct, though timeouts might be a risk. Responses were still surprisingly decent given how noisy the inputs were.
    ~ -Seems like reprompt might not be working right. E.g. if they ask me to choose who to speak to on startup and I don't answer at first but then do on reprompt, it failed.
        X -test all reprompts to see if they all fail or if it's specific to the case
            X -reprompt in launch
                X -person already in contacts
                X -new person
            X -reprompt in end_chat
            X -re-enable generate_person reprompt and test
        UPDATE: seems fine /shrug.
    X -"Goodbye" during conversation raises notimplementederror (see misc.txt)
        UPDATE: frustratingly, seems we're not allowed to override this, so you can't say "Goodbye" or "Goodybe {name}" at any point unless you want the session to end immediately. You can say "Bye" or "Bye {Name}" though.

6/3/22 fri
----------
~ -check if logger successfully created file for 6/2
    X -try new query to make sure it gets written there (probably would be empty initially even if the creation did work overnight)
    UPDATE: a new log file did appear at midnight, but the name was wrong. A correctly named file appeared a little after 9am today for some reason. Fixed bug that caused the filename to be wrong so I think we should be good now but we'll need to check again tomorrow. Worry about the mystery new log file later if it becomes an issue.
X -can't change settings when asked "Who would you like to speak to?". Only when a conv is ongoing, it seems. Could have sworn this worked before.
    X -try to reproduce error
    _ -try to fix
    UPDATE: could not reproduce error. I'm guessing I either used the wrong utterance last time or transcription failed so it just proceeded to the enqueued function (generate_persona). That happened one this time but that's not a problem with the dialog model or anything I can easily address.
-easier option: add more docs/help on how to invoke each intent
    -add spoken examples to docstrings
    _ -add "help" intent that reads out some sample commands (e.g. "To change your settings, you can say...")
        _ -create new intent in UI
        _ -download intent model and regenerate fuzzy dict
        _ -add endpoint in skill
        UPDATE: fiddled around on this for a while but I'm not sure there's a super compelling use case for this. Maybe if I find myself trying and failing to trigger certain intents a lot I'll go back and implement something like this.

6/4/22 sat
----------
~ -check if logger successfully created file for 6/2
    _ -try new query to make sure it gets written there (probably would be empty initially even if the creation did work overnight)
    UPDATE: new file was created at midnight (maybe <1 minute after) and queries from last night's conv were logged to it. Still had slight typo where new path contains extra period. Hopefully fixed now.
X -generate esther perel persona
    X -generate
    X -test that alexa is able to resolve it correctly
X -test if we can add multiple reprompts (or even infinite?)
    UPDATE: no, it ignores reprompts after the first one. (For the record, flask_ask does let us do question().reprompt().reprompt() etc. though.)
X -check price of last night's lex conv (sure it's not a problem, just want to confirm that my perception of pricing is roughly accurate. Also see how long it lasted.)
    UPDATE: slightly more than I thought but not too bad. $0.44 / 31 completions so ~1.4 cents per completion. Lasted ~12 minutes so 3.67 cents/min, or ~$2.20/hour. Granted, auto punctuation would add a little cost, but it would also slow down responses so cost/hour might not change all that much (and we probably can get away with a cheaper model).
~ -manually change some nationalities/voices? Or change default voice. e.g. lex shouldn't have a british accent
    UPDATE: for now just changed Hanson and Mosby. Harry Potter prompt should eventually get replaced anyway (currently is from wiki page for book series, not character) and undecided on what to do about default.
X -add a few new personas (Callard, Srinivasan, Altman, Sutskever)
X -better UX for changing multiple settings at once (might be difficult with my slot parsing strategy). As a fallback, could just try to shorten the "back to your conv" message.
    UPDATE: shortened message slightly to use "call" instead of "conversation" and replaced full name with first name only. It's already not very long so removing ~4-6 syllables seems like a solid reduction. I think this is good enough for now - truly supporting multiple setting changes at once seems like it would require a dramatic revamp.
    -consider how to approach
    -implement
~ -add multiple modes (conversation, transcript, podcast, brainstorm)
    ~ -some other mostly-unrelated prompt ideas: "An unpublished novel by {author} was recently discovered. Here is the first chapter:" (can tweak wording, but basically I'm thinking this might be a better way to get gpt to mimic authors than instructing it to do so directly). Also an error stack trace -> stack overflow solution (similar to google but hopefully finds the answer with a bit less sifting) and a python tutorial ("this is a {library} tutorial. This is how you {task}.").
    UPDATE: made new transcript prompt and a new "generalized" conversation prompt that lets you specify different types of interactions, e.g. debate/argument/negotiation/etc. The latter really would benefit from some jinja-esque templating though.
-consider ways to address this issue: transcription quality was weaker than I'd hoped. Might be addressed by enabling auto-punct, though timeouts might be a risk. Responses were still surprisingly decent given how noisy the inputs were.
    -try enabling punct and see if we get timeouts again
    -try enabling punct with openai backend and see if we still get timeouts
-easier option: add more docs/help on how to invoke each intent
    -add spoken examples to docstrings
X -fix email subject and body to use pretty name instead of snake case name

6/5/22 sun
----------
X -check if logger successfully created file for 6/2
    _ -try new query to make sure it gets written there (probably would be empty initially even if the creation did work overnight)
    UPDATE: new file with correct name got created an used. Still a little weird - another file also got created with the name '2022.06.05.2022.06.03' which looks like it uses the old buggy naming process with path.stem instead of path.suffix. Let's see if this happens again tonight.
~ -look into good way to record alexa skill instead of (presumably poor quality) iphone quality recording
    X -record short audio demo
    UPDATE: couldn't find any good answers in alexa forums. Just used iphone voice memo.
X -look into threading error/issues where trying to quit skill hangs (unsure if this is an issue with my app or computer in general; I recall seeing a similar phenomenon today when quitting ipython))
    UPDATE: set daemon=True for thread. Not sure if this is the ideal way but it does seem to work for my purposes.
X -add CLI flag to start in default mode
    X -test
-integrate generalized conv prompt into conv manager to support multiple modes (conversation, transcript, podcast, brainstorm)
-think more about how to deal with the eponymous character issue when generating personas. (e.g. try wiki_search(f'{name} character') for everyone, or run model first to infer if character is fictional)
-consider ways to address this issue: transcription quality was weaker than I'd hoped. Might be addressed by enabling auto-punct, though timeouts might be a risk. Responses were still surprisingly decent given how noisy the inputs were.
    -try enabling punct and see if we get timeouts again
    -try enabling punct with openai backend and see if we still get timeouts
-consider ways to backup conv or auto send email in case where conv quits unexpectedly (recall sat how one conv just didn't return a response in time or something and just ended suddenly with no option to save a transcript)
-easier option: add more docs/help on how to invoke each intent
    -add spoken examples to docstrings

6/6/22 mon
----------
~ -check if log file generation worked correctly
    X -did new file appear w/ 6/6/22 date, did it appear at midnight, do new queries get written to it?
    X -did incorrectly named new file (with date as suffix) also appear? If so, did anything get written to it?
        UPDATE: new incorrectly named file appeared at midnight ("2022.06.06.2022.06.05.2022.06"). No logs written there - not sure if I manually quit it but it was not running in the morning. Suspect it may have crashed it. No obvious bug found so I added some regular plaintext logging to try to pin down what is happening when.
X -when choosing who to talk to and name match is a moderate value (i.e. too low to auto-resolve to an existing persona but high enough that there's reason to think they might be who we're referring to), ask user for confirmation before asking about generating new person. (Motivation: some names are often mis-transcribed so it's hard to select them. Using just first name helps but that won't scale as well as n_personas increases and it's hard to remember which personas require that.)
-add more reprompts (maybe to _maybe_choose_person itself? If so, may need to remove some of the existing ones.)
-try reactivating auto-punct w/ openai
    -do we get timeouts like w/ banana?
    -is quality of completions noticeably different?
    -is wait time noticeably worse (qualitatively)?
~ -try revising punctuate_alexa prompt/hypers in playground using examples from recent longer convs, e.g. altman. IIRC I originally used much more minor transcription errors than seem to be common with alexa.
    UPDATE: old prompt isn't quite empowered enough (only fixes very minor mistakes) while new prompt is too empowered (dramatically changes the meaning sometimes). Still some tuning needed. Maybe put that off until seeing if timing even works (using the old prompt).
-integrate generalized conv prompt into conv manager to support multiple modes (conversation, transcript, podcast, brainstorm)
-think more about how to deal with the eponymous character issue when generating personas. (e.g. try wiki_search(f'{name} character') for everyone, or run model first to infer if character is fictional)
-consider ways to backup conv or auto send email in case where conv quits unexpectedly (recall sat how one conv just didn't return a response in time or something and just ended suddenly with no option to save a transcript)
-easier option: add more docs/help on how to invoke each intent
    -add spoken examples to docstrings

6/7/22 tues
-----------
X -check if log file generation worked correctly
    X -did new file appear w/ 6/7/22 date, did it appear at midnight, do new queries get written to it?
    X -did incorrectly named new file (with date as suffix) also appear? If so, did anything get written to it?
    UPDATE: new correctly-named file is there, new incorrectly-named file is not. Hooray!
X -try reactivating auto-punct w/ openai
    X -do we get timeouts like w/ banana?
        UPDATE: initially yes. Then tried again a few minutes later and it was fine.
    X -is quality of completions noticeably different?
        UPDATE: hard to say. Maybe not.
    X -is wait time noticeably worse (qualitatively)?
        UPDATE: maybe slightly? Seems to vary a bit - sometimes banana is quite fast but sometimes not.
X -benchmark banana backend. How fast and reliable is it?
    UPDATE: 10x slower than equivalent openai model.
_ -integrate generalized conv prompt into conv manager to support multiple modes (conversation, transcript, podcast, brainstorm)
    UPDATE: let's put this in the backlog for v3. Really should be trying to wrap up v2 soon.
-think more about how to deal with the eponymous character issue when generating personas. (e.g. try wiki_search(f'{name} character') for everyone, or run model first to infer if character is fictional)
-consider ways to backup conv or auto send email in case where conv quits unexpectedly (recall sat how one conv just didn't return a response in time or something and just ended suddenly with no option to save a transcript)
X -easier option: add utterance examples to docstrings

6/8/22 wed
----------
X -think more about how to deal with the eponymous character issue when generating personas. (e.g. try wiki_search(f'{name} character') for everyone, or run model first to infer if character is fictional)
    UPDATE: tried searching for both name and "{name} character" for all existing personas. The only case where the issue I pointed out occurs is harry potter, and I couldn't find a single other case where a. the issue occurs and b. it's a persona I'd want to generate (artemis fowl is the only other case where it occurs at all and that doesn't particularly interest me). Did briefly try out zero shot classification and it didn't seem great. Issue is sufficiently rare that I don't want to add another model call for every persona download so I think we'll just leave it as is.
X -consider ways to backup conv or auto send email in case where conv quits unexpectedly (recall sat how one conv just didn't return a response in time or something and just ended suddenly with no option to save a transcript)
    UPDATE: was able to send an email in end_session function if conv is active.
~ -allow convmanager to load an old conv
    -from_str
    -from_file
    UPDATE: WIP. 99% of the way there but need to work through a few spacing bugs.
-easy options: 
    -look for things to document
    -ack for todos
-consider getting rid of the whole prompt-level/conv-level/person-level distinction re settings
    -if yes, could remove some complexity from alexa.utils.Settings cls (though could also keep in case we re-enable this in future; not doing any harm in the meantime)
    -if yes, could rm some sample utterances from json model in UI
-re-build model and generate new fuzzy dict (wait til after above bullet in case I decide to rm some sample utts)

6/9/22 thurs
------------
X -allow convmanager to load an old conv
    X -reconstruct conv so that conv.full_conversation() == full_text in nb19 (currently some issues with newlines)
    X -from_str method
    ~ -from_file method
        UPDATE: support loading from file, just don't need a separate method. It's just 1 load() call and then we can pass the input into from_str().
X -create >= 1 new custom persona
    UPDATE: CM
    ~ -check that can load/use custom persona via echo
        UPDATE: tried but encountered new error (skill unresponsive for unknown reasons. Also extremely slow to start app but I feel like that should be unrelated.) Restarted ngrok to get new url (got "phishing reported" on old one) and updated in alexa console, but on rebuild I got "Error 500. An unexpected error occurred.". Try rebuilding tomorrow and see if anything changes (maybe a transient issue on their end?).
-easy options: 
    -look for things to document
    -ack for todos
-consider getting rid of the whole prompt-level/conv-level/person-level distinction re settings
    -if yes, could remove some complexity from alexa.utils.Settings cls (though could also keep in case we re-enable this in future; not doing any harm in the meantime)
    -if yes, could rm some sample utterances from json model in UI
-re-build model and generate new fuzzy dict (wait til after above bullet in case I decide to rm some sample utts)

6/10/22 fri
-----------
X -debug local deployment issues (i.e. skill no longer works on echo)
    X -try "build model" again in aws console (recall got 500 error thurs)
    X -confirm can start skill
    X -confirm can use custom persona
X -auto send email on "goodbye"
    UPDATE: recall we can't delegate() and it turns out we can't end_chat() either, but we can auto send the email. Just can't let user have a say.
~ -support continuing last conversation
    X -consider how this would work (new intent? just check automatically in choose_person?)
    UPDATE: the real value here seems like it would be in implementing a whole doc search/QA type system, or perhaps summarizing old convs and incorporating that into the new template. Right now we only look at the last couple turns anyway so continuing a conv doesn't actually mean anything after the first few turns. Also, I sometimes end a conversation bc the coherence starts to fall apart or transcription errors are piling up, so we don't really want to seed the next conv automatically with the last completion from the previous conv. Also, want to keep friction low so don't want to be asked every time if I want to continue or not, and adding a separate "continue conv" intent would signicantly complicate things since I use _maybe_choose_person so frequently. Very tempting feature but implementing this will will take a much larger effort and I'm already trying to wrap this up due to excessive scope creep.
    _ -create new intent in console if necessary
    _ -create new flask endpoint if necessary
    _ -test
-easy options: 
    -look for things to document
    ~ -ack for todos
    UPDATE: removed note about auto punct being off by default (think we're fine disabling that by default since it hasn't provided very convincing evidence of usefulness) and update EMO_PIPE creation to depend on a new argparse option --voice.
-consider getting rid of the whole prompt-level/conv-level/person-level distinction re settings
    -if yes, could remove some complexity from alexa.utils.Settings cls (though could also keep in case we re-enable this in future; not doing any harm in the meantime)
    -if yes, could rm some sample utterances from json model in UI
-re-build model and generate new fuzzy dict (wait til after above bullet in case I decide to rm some sample utts)

6/11/22 sat
-----------
X -allow passing in True/False to ConvManager to load all/no auto or custom personas
    X -update alexa CLI accordingly
X -consider getting rid of the whole prompt-level/conv-level/person-level distinction re settings
    _ -if yes, could remove some complexity from alexa.utils.Settings cls (though could also keep in case we re-enable this in future; not doing any harm in the meantime)
    _ -if yes, could rm some sample utterances from json model in UI
    UPDATE: after mapping out desired behavior, I had basically decided to get rid of this, but by writing a comment explaining why I ended up convincing myself that this was actually quite doable. /shrug
-implement settings resolution
    X -map out desired behavior (sequence of action/outcome pairs)
        UPDATE: see physical notebook. After many months of letting my brain work on this in the background, I think I finally have a clear idea on what I want this to do and how to do it. Basic algo is detailed there too.
    X -work out basic algorithm (when event x happens, what actions x,y,z do we need to make)
    X -add support for default kwargs per person
        UPDATE: added this in ConversationManager to start with. Settings class will wrap some logic around that but I think most of the data (name2kwargs) should be stored by conv manager.
    -add state queue to Settings class
    -add on_event hooks to update/resolve state accordingly
-re-build model and generate new fuzzy dict (wait til after above bullet in case I decide to rm some sample utts)
-easy options: 
    -look for things to document
-look for resources to help w/ hosting on raspberry pi

6/12/22 sun
-----------
~ -continue work on settings resolution (see physical notebook for detailed input/output pairs + basic algo. See resolve_state method in Settings cls for more details.)
    X -add state queue to Settings class
    X -add on_event hooks to update/resolve state accordingly
    X -add clear_default_kwargs method to conv manager
    X -make reset_app_state func in app.py clear default persona kwargs
    X -update settings.resolve_state to use new queue and chainmap
-re-build model and generate new fuzzy dict (wait til after above bullet in case I decide to rm some sample utts)
-easy options: 
    -look for things to document
-look for resources to help w/ hosting on raspberry pi

6/13/22 mon
-----------
X -make changes to reflect openai's change from name "engine" to "model"
    X -compile list of changes (as bulleted list below) that would need to be made (prob a good idea to do this but they did say it'll be backward compatible so if it would cause particularly hairy changes we could put this off)
    X -prompt files
    X -alexa py files
    X -jabberwocky lib py files
X -continue work on settings resolution 
    X -figure out how Settings cls will use conv.name2kwargs to populate persona dict (i.e. will changes only occur on the convmanager class and settings will just point to the same dict? Will likely need to update set() method and maybe on_conv_end/start method(s))
        UPDATE: have it point to appropriate name2kwargs dict on_conv_start, replace it with an empty dict in on_conv_end, make resolve_state also reset each value in state_queue using getattr, and call resolve_state in on_conv_start and end.
    -test changes: try following flow from physical notebook and read settings at each step
-re-build model and generate new fuzzy dict (wait til after above bullet in case I decide to rm some sample utts)
-easy options: 
    -look for things to document
-look for resources to help w/ hosting on raspberry pi

6/14/22 tues
------------
X -continue settings resolution
    X -test changes in ipython: try following flow from physical notebook and read settings at each step
    X -test subset of changes with echo (doesn't need to be all of them since I tested in ipython and alexa is slower to read them)
X -check if global state refreshes on app restart
    UPDATE: tbh not totally clear on why this works - seems like these attrs are not stored on session.state but on the settings object itself. (Also forget my reasoning for that decision 🫤.). But I tested it repeatedly and it works.
X -handle case where user tries to change person/conv level settings when no conv is active
X -update messages to say "{scope}-level" instead of "{scope}" when changing settings
X -change logging of state/settings to resolve chainmap to dict (easier to see state changing that way)
~ -test if settings change works on utt like "change person LEVEL model to 3" instead of "change person model to 3" (checked and it's not officially in utterances but fuzzy matching may get us close enough to recognize it)
    UPDATE: Yes for model-level, no to others. The problem is actually slightly larger than I expected - NO inferred intents work for changing max length or temperature. See misc.txt for error msg.
-re-build model and generate new fuzzy dict (wait til after above bullet in case I decide to rm some sample utts)
-easy options: 
    -look for things to document
-look for resources to help w/ hosting on raspberry pi

6/15/22 wed
-----------
X -investigate issues with fuzzy intent matching for change_max_length and change_temperature. See misc.txt for error msg.
	UPDATE: 1st issue is that change_temp and change_max_length intents expect param w/ name other than "number" so I changed those. 2nd issue is that we only includes numbers 1-10 (hardcoded) as sample values in our fuzzydict.
	X -update build_utterance_map func to allow variable min_num and max_num
X -re-build model and generate new fuzzy dict
-easy options: 
	-add example utterances that include scope (e.g. global) to setting change endpoints
    -look for things to document
-look for resources to help w/ hosting on raspberry pi

6/16/22 thurs
-------------
-test new utterance resolution with echo
	X -works on correct transcriptions
	X -works on inferred transcriptions
	X -works on inferred transcriptions with phrasing "{scope}-level"
	UPDATE: generally seems to work but I realized max length should allow values much larger than 100. This could increase fuzzy dict size to a point that's starting to become, if not yet problematic, certaintly hinting at future issues.
~ -experiment w/ various approaches to supporting max_lengths > 100
	X -benchmark how slow fuzzydict really is for various sizes
		UPDATE: at 10-15k items in dict (currently ~250 max length), we approach 0.1 seconds. At 50k items (currently 1k max length), we're at ~.33 seconds. A bit worrisome since I anticipate a high likelihood of adding more intents/sample utts in the future.
	X -benchmark how slow and how accurate lshdict is for various sizes
		UPDATE: on tiny test of 10 items, fuzzy dict gets 100% and lshdict gets 80%. Enough to suggest that this would be a problem. Also, misses are not necessarily numerically close.
	X -brainstorm workarounds (document below)
		-randomly sample numbers in range(min, max) rather than all numbers
		-use all numbers but create random pairs w/ various utts. I.e. if we have 10 sample utts for an intent and 1000 possible numbers, we could choose 100 values for each utt rather than all 1000. So we'd end up w/ 1k examples rather than 10k.
		-first use lsh dict to guess which intent is correct, THEN use fuzzy dict to select from that intents utterances/slots
	UPDATE: first two bullets are a little worrisome bc we really want an exact top 1 match, and randomly sampling risks leaving that out of the options. Idea 3 sounds promising - may require a moderate change to implementation, but not an earthshaking one. For a nearly .25 second reduction per turn, I think that's worth it.
X -choose lower max length limit (prev only enforced 2048 but 2048 should actually be the max for prompt + completion, not just completion)
	UPDATE: set to 900 for now. This should allow 2 long gpt responses, wiki bio, and 2 short user responses with room to spare.
-easy options: 
	-add example utterances that include scope (e.g. global) to setting change endpoints
    -look for things to document
-look for resources to help w/ hosting on raspberry pi

6/17/22 fri
-----------
X -try new idea for supporting intent inference for max_lengths > 100 (without dramatic slowdown in parsing). Idea is to first use lsh dict to guess which intent is correct, THEN use fuzzy dict to select from that intents utterances/slots.
	X -sketch out (physical notebook) what exactly I'm picturing this doing. Try to hone in on what data structures I'd have, ideally.
	X -Examine model and fuzzy dict json structure. Will current structure support my idea or do we need to reshape these a bit to do what I want?
		X -revise model parsing func(s) if necessary
		UPDATE: need an additional grouped dict pointing intent to list of utts.
	X -update implementation to infer INTENT ONLY first
	X -udpate implementation to, if an inferred intent has a high enough match, check similarities for ONLY THE UTTERANCES FROM THAT INTENT
	UPDATE: finished first pass at this but only provides a ~3x speedup which is alright but still slower than I'd hoped. See notes in nb20.
	X -try to speed up LSHDict creation
		UPDATE: multiprocessing provided 2x speedup (not sure why not 4, given cpu count?). Threads unsurprisingly didn't help at all. Code is in nb20, not yet ported. Might need to tweak 1-2 other methods to be consistent with new interface.
-easy options: 
	-add example utterances that include scope (e.g. global) to setting change endpoints
    -look for things to document
-look for resources to help w/ hosting on raspberry pi

6/18/22 sat
-----------
-nb20 
	X -LSHDict (while not as fast as I'd hoped, it's still faster than the current version)
		X -make any final tweaks/cleanup in nb to account for yesterday's changes
		X -port to htools and make pypi
		X -port to metas and make pypi
	-consider how to proceed w/ potential new intent inference system. Brainstorm further optimizations below.
		-ideas
			X -pass a much larger n_keys to infer_intent. Then use fuzzywuzzy to rerank those instead of all the ones for an intent. Pro is we reduce the fuzzywuzzy op to constant time. Con is we (I think) lose the guarantee of finding the best fuzzywuzzy match. However, if we set lsh n_keys high enough, this might be close enough (recall brute force fuzzydict approach still isn't guaranteed to be correct).
				UPDATE: realized I actually provide this functionality natively in LSHDict in the form of n_candidates (i.e. pass in large n_candidates, small n_keys). This is an order of magnitude faster than pure fuzzy dict approach. Should probably benchmark on a larger sample and/or do some qualitative user experience testing to confirm that it's a good strategy. Also should explore idea below before committing to this - while it's certainly more capable of scaling than the brute force fuzzydict approach, we still could run into problems if I dramatically increase any of (n_intents, n_utts_per_intent, n_valid_vals_per_slot).
			X -return to slot extraction idea. Extracting numbers doesn't seem like it should be that hard. Look at old alexa/utils funcs - why did I abandon this? Maybe it wasn't working so well for other intents, but I could make it only do slot extraction for numbers maybe.
				UPDATE: this is even faster than bullet above (at least w/ current lsh hypers) and it's nice that there are slightly fewer hypers to tune. Still would need to tune the "weighted_thresh" hyper though - IIRC it might actually be used to infer utterance moreso than intent atm, but this new approach would really only be using it to infer intent (also using lshdict instead of fuzzydict).
		X -if good idea arises, try implementing
		X -if not, consider whether it's worth porting this new version
			_ -maybe need to benchmark first? LSHDict seems like it would require a lower threshold since it's just a first pass filtering step.
			UPDATE: both new approaches today are much better than last night's approach. Don't port that one. New approaches require a bit more eval before porting.
-easy options: 
	-add example utterances that include scope (e.g. global) to setting change endpoints
    -look for things to document
-look for resources to help w/ hosting on raspberry pi

6/19/22 sun
-----------
X -nb20 infer intent refinement + benchmarking
	X -try altering approach 4 to use a smaller fuzzy dict (i.e. numbers 1-10 or randomly sample in range 1-900 rather than using all of them. I think this should still work pretty well. Prob won't have noticeable speedup since lsh dict is already so fast but it would reduce memory + data storage size I guess. OR maybe we can replace lsh dict w/ fuzzy dict if this reduces utt dict size sufficiently.
	X -benchmark
		_ -maybe extract real responses (both settings related and otherwise; want both positives and negatives) from logs. (First thought was transcripts and we could use these for negatives, but not positives - settings changes are of course excluded from the conv.)
	UPDATE: just hand wrote some positive and negative samples. Didn't include any big transcription errors since I'm not sure how well I can realistically make those up. New system did pretty well on this - 0 false negatives and 1 false positive, and no errors with the number extraction portion. Could raise weighted_threshold slightly (the 1 FP was *barely* high enough to qualify) but for now we'll leave as is and monitor the live logs for signs that we should change this in the future.
		~ -test new system (prob a modified version of approach 4) on these and assess results. Prob should keep dataset size low enough that I can self-label all of them.
	X -if benchmark goes well:
		X -update infer_intent to use new logic (modified version of approach 4 probably)
		X -port infer_itent to alexa/utils, see if there's anything else we need to port
		X -check if we need to make any changes to app.py
			UPDATE: doesn't look like this will be necessary.
	X -un-deprecate get_number and update docs accordingly
-qualitative assessment of new intent inference: chat w/ a few people, try to change some settings, keep an eye on live logs to see if resolutions are looking good. Can change various hypers (weighted_threshold, n_keys, etc.) if necessary.
X -fix naming error in htools/metas (accidentally ported new LSHDict class with temporary dev name)
X -see if we need to generate new fuzzy dict after recent intent changes
	X -load and check length
	X -regenerate new if necessary
	UPDATE: we had a fd w/ max_num=100. Changed it to 5.
-easy options: 
	-add example utterances that include scope (e.g. global) to setting change endpoints
    -look for things to document

6/20/22 mon
-----------
X -generate more custom personas
	UPDATE: Tested out new persona and it's pretty impressive. Better than human level performance on this task tbh, at least in my experience.
~ -brainstorm how to support multi person conv (What are options for implemenation? Is this sufficiently simple to implement now or should we move it to the long term backlog for future releases?)
	UPDATE: decided too big of a project for this release. Implementation could involve some major changes.
~ -briefly brainstorm how to mimic longer memory
	UPDATE: wrote a sample prompt format in physical nb. Thinking we want 1 "long term summary" of previous topics discussed and 1 "immediate context" for the turns that immediately preceded the included turns. Or maybe the "long term summary" is just a list of concatenated "immediate context"s since it would be slow/expensive to generate a new long summary every turn or few.
X -figure out where to warn user about dangers of using conv.me = "me" for some prompts
	UPDATE: put in conv.__init__. Personas are not prompts so they don't have an existing warning field for us to edit.
-qualitative assessment of new intent inference: chat w/ a few people, try to change some settings, keep an eye on live logs to see if resolutions are looking good. Can change various hypers (weighted_threshold, n_keys, etc.) if necessary.
-easy options: 
	X -add example utterances that include scope (e.g. global) to setting change endpoints
    -look for things to document

6/21/22 tues
------------
~ -prototype partial conv summary prompt
	X -check NLCA book for potential existing prompt (good source bc he's already tested his prompts)
		UPDATE: I think I can do better than the sample one he included.
	X -or try writing my own
		UPDATE: getting decent results with zero shot prompt. Field resolution is a bit involved so there's still some work to do in implementing this in the lib.
	X -port prompt from playground to yaml file
	X -test on excerpts from longer convs (altman (maybe edited? Unsure if that would be beneficial if we won't be editing them in jabberwocky.), dr. mills)
		UPDATE: tried on a couple excerpts from altman and sanderson. Pretty solid, though I generally avoided the more incoherent chunks.
-implement summarization in conv
	~ -write method (or adapt old one) to generate a conversation excerpt consisting of n turns (prob later need to figure out how to track which ones though - maybe have conv store a "last_summarized_idx")
		UPDATE: some cleanup to do but the basic logic is in ipython.
	-write method to perform the summarization (get correctly sized excerpt, update last summarized idx, add the summary to some kind of data structure of summaries]
X -update conv.process_name() to add period to "Dr"
-qualitative assessment of new intent inference: chat w/ a few people, try to change some settings, keep an eye on live logs to see if resolutions are looking good. Can change various hypers (weighted_threshold, n_keys, etc.) if necessary.
-look for resources to help w/ hosting on raspberry pi

6/22/22 wed
-----------
~ -implement summarization in conv
	~ -consider: is summarization that important? Or should we be more focused on extracting salient facts/anecdotes (recall replika). Or maybe both (and if so, should they be two separate prompts? I think probably 1 would be better.) Could always push this to long term backlog since I suspect this could get quite complex - eventually, we'll probably want to include the "past conversation summary" in the base prompt, which is a fairly major change.
		UPDATE: sketched out a skeleton of a fleshed out conversation prompt in misc.txt. I think it *might* be helpful to do the "relevant fact extraction" but I think those should be stored in docs for our QA system rather than auto-added to the running prompt (i.e. if you mention your dog's name, you'd want the persona to remember it for future convs, but it wouldn't be relevant info for the majority of the rest of the conv. We only want to include that type of info when needed (i.e. when the last user response is related to it)).
	~ -review ipy func to construct an excerpt str of k turns. See if we can refactor into a conv method to do this, which we can also call inside _format_prompt().
		UPDATE: moved to misc.txt for storage. Not implementing any of this now but I may use some of that when I return to this.
	_ -write method to perform the summarization (get correctly sized excerpt, update last summarized idx, add the summary to some kind of data structure of summaries]
X -qualitative assessment of new intent inference: chat w/ a few people, try to change some settings, keep an eye on live logs to see if resolutions are looking good. Can change various hypers (weighted_threshold, n_keys, etc.) if necessary.
	UPDATE: tried it out a bit and didn't encounter any problems. Not all that many infer_intent calls tbh - can keep an eye on this during future convs/dev.
X -update read_settings to read temperature as a percent
X -try summarization prompt on recent excerpt from real conv
	UPDATE: Curie gives solid results but I do feel like davinci continues to provide ever so slightly better summaries (less vague/generic? Not sure exactly what it is but it does seem a bit better.) Also fixed engine/model kwarg name in summarization prompt and template prompt.

6/23/22 thurs
-------------
X -add reprompt to _maybe_choose_person (I think this is currently missing and should be doable, but need to check)
	X -add reprompt to case where no active conv
	X -add reprompt to case where IS active conv (bit more complicated - 2 different voices)
	X -update custom_question to allow ssml reprompts
	X -update voice() to return single str rather than (str, bool) tuple. is_ssml is always True now.
-look for resources to help w/ hosting on raspberry pi
-look for resources to help w/ self/private publishing (if I actually need to do anything? Current mode seems to be working, whatever that is)

6/24/22 fri
-----------
~ -consider adding clear_all_settings intent. If yes:
	_ -update dialogue model in UI
	_ -download new model and move to data/alexa dir
	_ -generate new fuzzy dict
	_ -write endpoint
	UPDATE: decided not really necessary right now. If I find myself wanting this feature I can add it to the v3 roadmap.
	X -clean up docs a bit for various Settings methods
		X -_resolve_state
		X -clear
~ -look for resources to help w/ hosting on raspberry pi
	UPDATE: didn't find much specifically about using it to host alexa skills (moreso to create alexa skills that control a pi), but I think I can just use a generic pi as webserver tutorial.
~ -look for resources to help w/ self/private publishing (if I actually need to do anything? Current mode seems to be working, whatever that is)
	UPDATE: submitted for private release but I think it failed some automated check (seems like they automatically call all your endpoints I think, and that won't necessarily work here if they're in the wrong order). Sounds like I could probably just stick with keeping the skill in its current state (i.e. permanently in dev mode) and that would be fine, tbh. The skill might require a significant redesign to work with multiple users (I have no idea tbh) so public usage is out of scope atm, and private usage is really no different than dev mode for my purposes.
~ -try to get pi up and running
	X -confirm we can turn pi on
	_ -confirm we can ssh into pi
		UPDATE: getting "operation timed out". Think maybe I should start w/ making sure I can access it via remote desktop w/ an external monitor first, then return to ssh issue.
X -rm some old unused files/vars from alexa dir
-other options:
	-record sample audio/video clip w/ alexa skill (lex?)
	-update readme (new alexa clips, maybe instructions for setting up your own alexa skill?)

6/25/22 sat
-----------
X -backlog grooming (separate things into Long Term Jabberwocky, Long Term Misc, Won't Do, etc.)
X -ack for TODOS
	UPDATE: zero todos left in alexa, gui, scripts, or lib dirs 😮)
X -add module docstring to each alexa script
X -update makefile to include alexa dir in readme generation
X -make all readmes
X -general pi setup
	X -try to connect to external monitor with help from D
	X -try to get ssh working
	X -clone git repo onto pi
-work through webserver tutorial. Try to get jabberwocky up and running there.
-other options:
	-record sample audio/video clip w/ alexa skill (lex?)
	-update readme (new alexa clips, maybe instructions for setting up your own alexa skill?)
	-update jabberwocky version and upload to pypi

6/26/22 sun
-----------
X -general pi setup
	X -move pi to my outlet and confirm can still ssh in
	X -try to alias ssh command further (avoid needing to specify ip)
	X -do we really need password? Consider disabling or seeing if we can use some other auth method.
	UPDATE: re-enabled alias in ssh config using IP address as host instead of 'raspberrypi', copied ssh public key over to pi using ssh-copy-id cmd, edited mac ssh/known_hosts file. Pi was still asking for its password even though ssh key was accepted (checked using ssh -v). Spent a while changing dir permissions on pi (many suggestions to do this online) but in the end, fix seemed to be adding ssh to apple keychain /shrug.
	-copy over dotfiles, install vim extensions, etc.
X -try building py env on pi
	X -install pyenv
	X -install more recent python version
	X -create env
	X -install requirements
	X -install ipython
-work through webserver tutorial. Try to get jabberwocky up and running there.
-other options:
	-record sample audio/video clip w/ alexa skill (lex?)
	-update readme (new alexa clips, maybe instructions for setting up your own alexa skill?)
	-update jabberwocky version and upload to pypi

6/27/22 mon
-----------
~ -scope out rate limiter idea (prevent scenario where someone finds url and runs up my openai bill)
	UPDATE: made quite a bit of progress in nb21. Considered a few ways to do this (query freq vs token count vs cost; change GPT vs. decorate flask endpoint vs check manually inside endpoint func) and wrote some notes (see bottom of nb21). 
-look into possible stow issue on pi (vimrc, bashrc, ipython config all not working as expected. Maybe symlinking didn't work right? Filesystem doesn't see any .ipython/profile_default/ipython_config.py file). Despite the minimal changes visible in dailies, this was actually a very productive (and enjoyable) session.
-try imports in ipython
	-various jabberwocky imports
	-transformers imports (test memory usage works)
-try running alexa app
-work through webserver tutorial. Try to get jabberwocky up and running there.
-other options:
	-record sample audio/video clip w/ alexa skill (lex?)
	-update readme (new alexa clips, maybe instructions for setting up your own alexa skill?)
	-update jabberwocky version and upload to pypi
X -check on alexa approval process
	UPDATE: actually approved 🤨. I thought it looked like we clearly failed some automated endpoint test so I'm surprised.

6/28/22 tues
------------
~ -finish price monitoring capabilities (nb21)
	X -investigate possible issues w/ EngineMap.estimate_cost
		X -is cost really returned in cents? The estimates I'm seeing look like they're in dollars.
			UPDATE: 😮 these were in dollars. I guess really that just means some of my comments/docstrings were wrong/outdated. Updated comments and changed 'cost_cents' key to just be named 'cost' in EngineMap.estimate_cost.
		_ -Is the math correct here? If it really is using cents, seems like 1-2 orders of magnitude lower than I've been getting charged.
			UPDATE: skip for now - seems like the above bullet point probably was the issue.
	X -once cost calcs are confirmed, test cls again on:
		X -normal query rates/lengths
		X -elevated query lengths
		X -elevated query rates
	X -add warn option to monitor cls
		UPDATE: ended up tweaking the interface a bit. Not as pretty as before but not too bad. Hard to get something clean with both an error option and a warn option. Walrus operator would have been nice here but I've been using py 3.7 and am hesitant to change it mid-project.
	-select good values for time_window and max_cost
	-port cls to lib
	-update _reply func in app.py to make use of this.
-pi: try imports in ipython
	-various jabberwocky imports
	-transformers imports (test memory usage works)
-try running alexa app on pi
-work through webserver tutorial. Try to get jabberwocky up and running there.
-other options:
	-record sample audio/video clip w/ alexa skill (lex?)
	-update readme (new alexa clips, maybe instructions for setting up your own alexa skill?)
	-update jabberwocky version and upload to pypi

6/29/22 wed
-----------
X -wrap up pricemonitor
	X -select good values for time_window and max_cost (nb already has some tentative data to help with this)
	X -port cls to lib
	X -port QueryAllowedResult to lib
		UPDATE: also exposed a few other attrs from pricemonitor so all warnings/errors/prints only need to act on `allowed` object.
	X -update _reply func in app.py to make use of this.
		UPDATE: done but not tested yet.
-pi: try imports in ipython
	-various jabberwocky imports
	-transformers imports (test memory usage works)
-try running alexa app on pi
-work through webserver tutorial. Try to get jabberwocky up and running there.

6/30/22 thurs
-------------
~ -test pricemonitor functionality in app
	X -start app w/ show_cost=True mode and see how logs look. Maybe record some of these in misc.txt so we can later compare to the actual gpt charges to see how accurate the estimates are.
		UPDATE: fixed bug where we returned bool rather than QueryAllowedResult in pricemonitor.allowed. Fixed bug where we were sending warning email if everything was okay rather than if usage was high. Lowered max and warn rates a bit because I seem to be averaging only $0.03 / 2 min, much lower than I expected (granted, it might turn out that we're underestimating).
	X -consider how to test warning and error outcomes (could add cli option or just hardcode backend for a few minutes)
		UPDATE: just hardcode.
	~ -test error outcome (should shutdown app)
		UPDATE: a bit trickier than I expected. I'd prefer to have this a. say a relevant error message as Lou, b. send a transcript, and c. kill the script (not just end the session). Returning error message as statement() ends the session but doesn't let us execute code after that to kill the script (if we use a try/finally, it seems that the statement doesn't make it back to alexa).
	X -test warning outcome
-pi: try imports in ipython
	-various jabberwocky imports
	-transformers imports (test memory usage works)
-try running alexa app on pi
-work through webserver tutorial. Try to get jabberwocky up and running there.

7/1/22 fri
----------
X -get error mode working (i.e. read message, send transcript, then exit script entirely) on pricemonitor error in _reply (see notes from thurs for a reminder on the issue here)
	~ -consider strategies to try to accomplish the behavior I want
		X -review yesterday's notes to see what didn't work
		~ -make sure you can clearly state the problem
		_ -write down a few possible things to try
		X -select first method
		UPDATE: briefly tried my idea of try/finally w/ sleep in the finally but the return hangs too so I don't think that would work w/ alexa. Just returned to the idea of placing a check at the start of _reply - I suppose the current max price is quite low and if the bad actor can't get any more completions, then what's the harm in the app staying live for a bit?
	X -implement selected method
	X -test with echo
	X -revert hardcoded allowed() call w/ artificially high price to real calculation
X -view openai console price usage. Does my estimate of ~3cents (maybe I saw 5 at times?) per 2min look about right or are we underestimating?
	UPDATE: looks like ~5cents per 2 minutes. Maybe a bit higher than my estimates but maybe that's just due to the mock tokenizer underestimating (granted, the formula I used is directly from openai). Though I'm also realising the actual completions are usually quite a bit shorter than the max allowed tokens so if anything the real values should be significantly lower, not higher. Hmm. Usage levels aren't really important enough for this to matter I think, but I should keep in mind that these price estimates are likely underestimating things.
	> Ended up adding a not to EngineMap.estimate_cost() method.
~ -pi: try imports in ipython
	~ -various jabberwocky imports
		UPDATE: not found. In fact, basically no packages are found. Some (like numpy) were installed somewhere in /usr/something but weren't found by pip show/list.
	-transformers imports (test memory usage works)
-try running alexa app on pi
-work through webserver tutorial. Try to get jabberwocky up and running there.

7/2/22 sat
----------
~ -pi: get pip installs working
	X -check if new test package (wordlcoud) installed successfully
		UPDATE: pip show wordcloud finds nothing, python -m pip show wordcloud finds it! Importing wordcloud throws error but not an "import error", and importing numpy works.
		> Now (at night) pip show wordcloud works!?
	X -look again for where packages are installed
		UPDATE: ~/.pyenv/versions/{python_version}/envs/{env_name}/lib/python{py_version}/site-packages/
	~ -google around for solutions (could be related to pyenv active env, python version, pip version, pi, etc.)
	X -install lib requirements
		UPDATE: seems we got tripped up on the same issue at the end of the gui project - htools doesn't specify pandas_flavor version and newest version causes install errors in some contexts. Updated htools/metas and pushed to pypi, updated jabberwocky lib reqs, pushed/pulled, etc. Install still failed (pip couldn't find latest version, though it found all others. BUT python -m pip install finally worked.)
	X -add missing deps to lib (transformers, banana-dev)
	~ -test jabberwocky imports
		UPDATE: failing due to numpy import error. Pasted it in misc.txt.
	-test non-jabberwocky imports
	-test transformers imports (pipelines used in app.py - test memory usage works)
X -copy api keys to pi
-try running alexa app on pi
-work through webserver tutorial. Try to get jabberwocky up and running there.

7/3/22 sun
----------
-troubleshoot pi pip installs
	X -investigate numpy error in misc.txt
		UPDATE: upgrading to 1.21.6 seemed to fix it.
	~ -add more missing imports as needed (both to requirements somewhere and install on pi)
		UPDATE: moved unidecode to new alexa reqs file and removed it from jabberwocky deps. We only use it in conv.nearest_persona() to clean up alexa names, so we can just do that in app.py itself (prob better to use the normalized text for all other purposes anyway).
	-investigate transformers install error (sounds like maybe we need to install rust compiler and add to path?)
-confirm jabberwocky imports work w/out import errors
-confirm a few external dep imports work w/out import errors
-confirm transformers model loading works on pi w/out memory error
-try running alexa app on pi
-work through webserver tutorial. Try to get jabberwocky up and running there.

7/4/22 mon
----------
~ -more pi package installation troubleshooting
	~ -review scipy official install instructions and see if there's something I haven't tried (apt install python3-scipy looks like it works but import simply says "no module named scipy". Also tried importing python3_scipy to no avail.)
		UPDATE: pretty much exhausted stackoverflow on related issues. apt install completes without error but can't be imported. Upgrading pip and then python -m pip installing scipy allowed me to import scipy itself but (IIRC) 'from scipy.special import stats still failed with old libgfortran error. Apt installing various libgfortran variants did not resolve this. Current state is pip installed succeeded, pip show works, import scipy works, importing submodules throws libgfortran error.
	~ -investigate transformers install error (sounds like maybe we need to install rust compiler and add to path? Error now in misc.txt.)
		UPDATE: successfully installed rust
-confirm jabberwocky imports work w/out import errors
-confirm a few external dep imports work w/out import errors
-confirm transformers model loading works on pi w/out memory error
-try running alexa app on pi
-work through webserver tutorial. Try to get jabberwocky up and running there.
~ -prompting: prototype problem description -> pseudocode -> code prompts
	UPDATE: Pseudocode LOOKS reasonable (generated with davinci, NOT codex) and then codex converts it to python beautifully. BUT the code does not produce the desired results. I probably should start with a problem I either deeply understand or have the answer in front of me - seems like there's some subtle mistake the model's making. I'm guessing it's coming at the pseudocode writing stage.
	> Adding "answer key" to pseudocode generation prompt seemingly fixed this first function. Need to test more to see how it does on other problems. ALso should ocmpare to baseline of just generating codex problem -> code.
X -prototype time/space complexity prompt
	UPDATE: regular davinci is pretty good at this and gives longer explanations than codex - interesting data point to keep in mind. IIRC adding "ANSWER KEY" also added a longer explanation of the solution when using davinci - hoping it improved correctness too but unsure.
X -regenerate openai api key (slightly suspicious behavior in usage? Shows a bunch of text moderation requests. Times coincided with my usage but I don't recall selecting that model (certainly not intentionally) and it does seem like multiple requests might have been coming at the same time.) Sigh.

7/5/22 tues
-----------
X -write openai support for advice re possible api key compromise (again)
X -copy new openai api key to pi
X -check on transformers install (lower pane of tmux sess on pi. See yesterday's top bullet points for notes on current state of installs.)
	UPDATE: transformers installed without error but it didn't install torch. Tried both pip installing and downloading wheel but neither worked. Importing torch still throws a bunch of C-related errors despite many apt updates/installs.
	> With transformers/torch causing serious problems and the scipy problem unsolved, I think it's time to consider other options. We'll get into this more tomorrow - I need a break, i.e. some fun with code prompting. Let's really try to wrap this up soon before jumping too far into the code prompting stuff, as that could be several whole projects in itself.
-look into alternatives to pi deployment ☹️. This is sucking up more time than it justifies.
	-look into lambda option
		-find example script for alexa skill using lambda
		-features I need to ensure are possible:
			-data access (loading prompts, etc.)
			-accessing private api keys
			-cloning git repo (once, not every request)
			-loading HF models happens only once, not every request
	-find another option?
	-consider pros/cons of running on laptop in background all the time
-otherwise, return to more pi package installation troubleshooting
	-get scipy import working
	-get transformers import working
	-confirm jabberwocky imports work w/out import errors
	-confirm a few external dep imports work w/out import errors
	-confirm transformers model loading works on pi w/out memory error
	-try running alexa app on pi
	-work through webserver tutorial. Try to get jabberwocky up and running there.
X -revise time complexity prompt to include space complexity more explicitly? (Latest trial only completed time complexity.) Or could move space complexity to a different prompt, but I'm leaning towards keeping them together.
X -play around w/ some recommender prompts
	UPDATE: music recs were decent. General "thing recommender" didn't work that well.

7/6/22 wed
----------
X -reply to openai chat re new cost estimate formula
~ -look into alternatives to pi deployment ☹️. 
	-look into lambda option
		-find example script for alexa skill using lambda
		-features I need to ensure are possible:
			-data access (loading prompts, etc.)
			-accessing private api keys
			-cloning git repo (once, not every request)
			-loading HF models happens only once, not every request
	X -find another option?
		UPDATE: try using old laptop. Still think lambda is probably a better option long term but I'm worried about kicking off another rabbit hole of technical issues.
	X -consider pros/cons of running on laptop in background all the time
		UPDATE: see misc.txt.
~ -old laptop setup
	X -get git ssh working
	X -update dotfiles
	_ -install pyenv
		UPDATE: tried initially but brew no longer supports this OS 😬.
	~ -create env
		X -create empty conda env
		~ -install jabberwocky env packages
			UPDATE: in progress.
	-confirm jabberwocky imports work
	-try running app locally
-other options:
	-record sample audio/video clip w/ alexa skill (lex?)
	-update readme (new alexa clips, maybe instructions for setting up your own alexa skill?)
	-update jabberwocky version and upload to pypi

7/7/22 thurs
------------
X -check openai customer service response
	UPDATE: no response
~ -try again to determine if lambda could meet my requirements (old laptop is looking a bit shaky, not an ideal long term solution)
	X -watch video tutorial
		UPDATE: not that useful. Could try reading/watching more.
	X -find example script for alexa skill using lambda
	-features I need to ensure are possible:
		X -data access (loading prompts, etc.)
			UPDATE: I suppose I already implemented loading remote prompts so I know this is possible.
		X -accessing private api keys
			UPDATE: pretty sure I could get this to work using google drive. AWS also has some KeyManagementService that might work. Looks like lambda can also set env vars but that might be less secure (?).
		X -cloning git repo (once, not every request)
			UPDATE: I guess this isn't strictly necessary. Jabberwocky can be pip installed (eventually), prompts and other files can be loaded from google drive, etc. Might need to dump everything in app.py and alexa/utils.py into 1 lambda.py file but that's not impossible, just ugly.
		X -running flask-ask
			UPDATE: yes, though we prob lose support for our cli.
		-loading HF models happens only once, not every request
~ -old laptop setup
	X -create conda env
		X -check if jabberwocky install cmd appeared to work
			UPDATE: no, timed out installing scipy (same package that killed my raspberry pi hopes 💀). Tried relaunching.
	X -confirm jabberwocky imports work in ipython
	~ -try running app locally
		UPDATE: tons of env issues to work out but I'm sooo close. We now get up to line 847 (out of 868) before failing to load UTT2META obj. Fix this tomorrow.
X -fix bug when ask creates logger on new system (previously assumed file existed)
-other options:
	-record sample audio/video clip w/ alexa skill (lex?)
	-update readme (new alexa clips, maybe instructions for setting up your own alexa skill?)
	-update jabberwocky version and upload to pypi

7/8/22 fri
----------
X -airdrop utt2meta file to old laptop or commit it to git
	UPDATE: commit to git. It's small, shouldn't have any private info, and maybe this will make things easier if I have to switch to the lambda method? (Probably not though, don't think I can clone the repo with that method.)
~ -figure out how to add the following to alexa/requirements.txt or some kind of install script
	X -spacy en_core_web_sm to requirements.txt (or some kind of make-env script)
	X -nltk punkt
	UPDATE: added install to bash script and makefile. Found possible spacy solution but not nltk. Since we need both, might as well just handle both together. Considered doing this in the library itself (try/except, download if error) but I'm a little concerned about what this would do if we have to go the lambda route).
-try again to determine if lambda could meet my requirements (old laptop is looking a bit shaky, not an ideal long term solution)
	-confirm slow processes (e.g. loading HF models happens only once), not every request
	-confirm other state persists (e.g. convmanager)
X -old laptop setup
	X -try running app locally
	X -try using ngrok and confirm I can access healthcheck endpoint from main computer
		UPDATE: see slack_drafts for url. Feel like maybe I shouldn't push that to github (?).
X -look into ngrok persistence policy (is url meant to persist as long as it seems to or is this some weird coincidence?)
	UPDATE: sounds like it actually should be permanent - the 2 hour expiration was for anonymou users (no account).
-other options:
	-record sample audio/video clip w/ alexa skill (lex?)
	-update readme (new alexa clips, maybe instructions for setting up your own alexa skill?)
	-update jabberwocky version and upload to pypi

7/9/22 sat
----------
~ -update url in alexa console to use new ngrok url
	UPDATE: updated dev version but it looks like I need to go through a new approval process to make the live version use it.
X -briefly try out a range of features (start conv, choose person, talk, email, quit, etc.) to make sure everything still works
X -try to set up ssh so I can more easily check logs (i.e. just like I can go `ssh pi`, I should be able to ssh in very easily. Maybe can do all work on that machine that way then.)
	UPDATE: see ~/.ssh/config. Ran into issue again where mac asked for password after ssh. This time the solution was to delete the public key on the old device (which I'd originally copied over with scp) and instead use ssh-copy-id (maybe that does something special with permissions?).
X -get ngrok running in background
X -fix openai backend bug
	UPDATE: just a version issue. Recall I had to update openai package at some point and the endpoints changed a bit.
X -fix bug in QueryAllowedResult
	UPDATE: just a typo.
-try again to determine if lambda could meet my requirements (old laptop is looking a bit shaky, not an ideal long term solution)
	-confirm slow processes (e.g. loading HF models happens only once), not every request
	-confirm other state persists (e.g. convmanager)

7/10/22 sun
-----------
~ -look into recent issue where alexa no longer knows my name/email
	UPDATE: had to re-enable skill in phone app. Also turns out the live skill is using the new url that I added to the dev skill - not sure how that happened. Thought I needed to resubmit it first but maybe not. New problem though: seems like the old mac sort of goes to sleep or shuts down or something when unused, and makes the app unreachable. Might need to return to lambda idea. First try to find out if there's some easy mac setting I can change to avoid this.
X -some changes to make future resuming conv easier
	X -save alexa transcripts in alexa/conversations/{name}/{datetime}.txt rather than alexa/conversations/{name}__{datetime}.txt.
	X -document resume_text and resume_path args in start_conv
~ -look into issue of laptop seemingly shutting down (or sleeping? Seems like something halfway in between.) between uses
	UPDATE: not 100% sure but I think it's just sleeping since I didn't have to reopen terminal/tmux etc. Tried changing setting in /etc/ssh config to keep ssh alive but not sure if it will work yet. Also tried running app with nohup prefix in the hopes that will help. Might need gunicorn and/or something like systemd though.
	> This did not seem to resolve the issue. Skill still becomes unavailable when computer sleeps.
~ -try again to determine if lambda could meet my requirements (old laptop is looking a bit shaky, not an ideal long term solution)
	~ -maybe easiest way is to start creating a hello world alexa skill w/ lambda. Think this will be more effective than trying to research answers more.
		UPDATE: created new skill from template using lambda option instead of custom. Haven't filled anything in yet. Chose conversation template in case it might give me new ideas or knowledge of best practices for chatbot-like skills. Only changed invocation name to "lambda test" but haven't filled anything else out yet.
		-confirm slow processes (e.g. loading HF models happens only once), not every request
		-confirm other state persists (e.g. convmanager)

7/11/22 mon
-----------
~ -flesh out dummy lambda skill ("lambda test")
	~ -look through code and ensure functionality is there for minimal example
		UPDATE: got a quick sense, maybe we can figure the rest out through trial and error.
	X -try testing in AWS console
	~ -try replacing code w/ flask ask
	UPDATE: import error with flask even though I added it to the requirements.txt file. I think a more promising approach is to work through the zappa tutorial.
	-try adding in 1 slow process (e.g. loading HF QA pipe) to make sure it only happens once
	-check that object state can persist (e.g. for convmanager attributes; use much simpler toy example)
	-try read/write from local file system
	-try to see how we can load private api keys
X -old laptop approach
	X -try setting sleep to never
	~ -test in console
		UPDATE: worked at first but after a while when I tried to start the skill alexa would just flash blue for a bit and silently fail 🤷‍♂️. Argh. I think it's clear this is not a solution either. It's either going to be the personal laptop or the lambda.
	X -change url back to local for now
~ -another option (try above first though to get basic sense of lambda): read through flask-ask/zappa/lambda tutorial. Looks like these might all be compatible at once?
	~ -create virtualenv
		UPDATE: thought I'd sorted this out already when I created conda env commands but I encountered lots of issues still. Maybe it's the difference between installing an editable local dir vs a pypi package? Either way, I think my virtualenv should mostly work now, though it does require two separate requirements files (1 with --no-deps) and I'm back to the question of how to handle the nltk and spacy downloads. 
X -restart ngrok in background, restart app, update url in console (while testing this I accidentally killed my ngrok tunnel)
	X -update ngrok makefile cmd to run in background, get url and filter with jq
	X -update alexa/make_env.sh
X -push new jabberwocky version to pypi and update alexa deps with it
-add personas to git? Want to make them more easily accessible on old mac/lambda etc.
	-or make it possible to load them from git, like how prompts can be loaded remotely

7/12/22 tues
------------
X -check that alexa skill is back up after recent restart (as of EOD mon it was not, but I think it just takes a few minutes. Seems like when I stop the flask app it disables the skill in my alexa phone app, but it's re-enabled now.)
	UPDATE: oddly, it re-disabled it in my phone app. But after I re-enabled it the skill works again.
~ -continue working through flask-ask/zappa/lambda tutorial
	X -create virtualenv
		X -try adding spacy dep to requirements file
			_ -if fails, try adding to module
		X -add nltk download in module
		X -add option to run app on diff port so I don't have to bring down my prod skill
		X -test that skill can run in virtualenv on diff port
	~ -proceed to next steps in tutorial
		X -create new IAM role
		X -awscli local configuration
		X -create zappa config w/ `zappa init`
		~ -zappa deploy dev 🤞
			UPDATE: first try was too big. Added option in zappa config to allow bigger size but I suspect it still won't be enough - venv dir alone is 700mb.
			> Second try with slim settings maybe got farther? Threw error bc request to '/' caused error but that's expected w/ my current app. Seems like maybe it did get the app running? Or at least the env wasn't instantly judged to be too big.
-add personas to git? Want to make them more easily accessible on old mac/lambda etc.
	-or make it possible to load them from git, like how prompts can be loaded remotely

7/13/22 wed
-----------
~ -continue working through flask-ask/zappa/lambda tutorial
	~ -proceed to next steps in tutorial
		~ -zappa deploy dev 🤞
			~ -Deal with this Error: Warning! Status check on the deployed lambda failed. A GET request to '/' yielded a 502 response code. (Basically, seems like zappa uses root url as healthcheck. My app has an explicit /health endpoint instead.)
-add personas to git? Want to make them more easily accessible on old mac/lambda etc.
				UPDATE: internet seems to think it's not as simple as adding a '/' endpoint but none of their ideas seemed to apply to my situation. Tried making a slightly simpler copy of app.py w/ hardcoded cli args and a '/' endpoint.
				> This produced the same 502 error. I found `zappa status` does provide my url and visiting it reveals that the `app` var is None but the cause of that is unknown atm. Oddly, they try to call app() instead of app.run(). Wonder if we need to use that flask-ask lambda function wrapper. Next: tried further paring down tmp_app.py (rm almost all imports, all openai stuff, all local file loading, etc.). Also tried moving zappa conf to alexa dir.
				> Still no. Next trying making an entirely new repo, copying their minimal example, and seeing if I can even reproduce that.
				> New minimal example works! Note that s3 bucket name must be globally unique. Next try renaming lambda_handler function and see if that breaks it.
				> No, still works. Next try removing zappa from requirements.txt and see if that breaks it.
				> No, still works. Try setting slim_handler=True and see if that breaks it.
				> No, still works. Try copying big venv over and see if that breaks it.
				> Yep, that breaks it. That may be a tough one to get around.
				> Trying reverting to slim_handler=False to see if that restores it.
-add personas to git? Want to make them more easily accessible on old mac/lambda etc.
	-or make it possible to load them from git, like how prompts can be loaded remotely

7/14/22 thurs
-------------
X -continue working through flask-ask/zappa/lambda tutorial
	~ -proceed to next steps in tutorial
		X -check if lambda works again (refresh chrome tab) after setting slim_handler=False. (Doubtful)
			UPDATE: no :(. Error about zipped size being too big is back.
		X -see if we can pare down jabberwocky deps at all
			UPDATE: managed to move Pillow dep to gui deps. Could move youtube-transcript-api without much trouble but I don't think that's very big.
		X -briefly google around to see if anyone found some crazy workaround to using a giant venv. Seems unlikely but this seems to be the culprit.
			UPDATE: not finding anything too relevant/promising.
		X -could also try the "memory_size" arg in zappa config that someone suggested for spacy install errors - maybe look into lambda pricing first? I'm guessing that may cost extra.
			UPDATE: tried bumping from 512 to 768. alexa subdir is 694 mb pre-zipped, figured it was worth a shot to see if this 1 change fixes things. But I doubt it.
			> Nope. Next try bumping jabberwocky version to 2.0.3 (no pillow) and bumping zappa memory_size to 1024.
			> No 😞
	UPDATE: undeployed. Accepted that lambda does not seem to be the solution either.
X -mv requirements to gui/, add pillow dep, rm requirements-dev, update makefile and readme
X -mv all old alexa conversations to fit new dir structure
	UPDATE: done in ipython :)
X -delete toy lambda skill in console
-add personas to git? Want to make them more easily accessible on old mac/lambda etc.
	-or make it possible to load them from git, like how prompts can be loaded remotely

7/15/22 fri
-----------
X -maybe one last try with zappa? looks like memory_size actually goes up to 10.24 gb...
	UPDATE: tried with 5gb memory size.
	> Zappa has limit of 3.008gb. Trying again w/ memory_size=3000.
	> Still no. 500 status code instead of 502 but error is the same about app being None. Zappa says a newer version is available but I checked and it's only a minor increment. Undeployed again - I think it's clear lambda is not the way to go at least right now.
~ -see if we can put venv elsewhere? I think this is making git commit very slow due to having to search all the files in venv
	UPDATE: Found virtualenvwrapper (a pypi package) that stores envs in ~/.virtualenvs, so it is possible, but that lib also hasn't been updated in years, and I read that *moving* a virtualenv can be tricky. For now I instead updated my pre-commit hook to skip acking gui/venv and alexa/venv.
-look into best practices re running flask forever (e.g. thinking I should probably be using gunicorn? And maybe something like systemd also? Unsure.)
	-combine into 1 makefile cmd? And if so, maybe cleanup old separate commands.
~ -test alexa env creation on old machine
	UPDATE: appeared to install w/out error but running the app raised error when trying to create HF pipeline that there is no pytorch or tensorflow>=2.0.0. Surprisingly, this is true - I guess HF doesn't install those? Installing this on old machine atm.	
	> Health check works now. Added torch to lib requirements (pipeline is imported there too). Bumped version of lib and published, then updated version in alexa requirements file.
~ -other options:
	-record sample audio/video clip w/ alexa skill (lex?)
	~ -update readme (new alexa clips, maybe instructions for setting up your own alexa skill?)
		UPDATE: cleaned up some old commands and instructions, added mentions to alexa skill. Still need to write docs on how to USE the alexa skill. Maybe also on how to create it in aws console since mine is private.
X -add personas to git? Want to make them more easily accessible on old mac/lambda etc.
	_ -or make it possible to load them from git, like how prompts can be loaded remotely
		UPDATE: I'm intrigued by this idea but I think we have to hold off this for now for "practical reasons". Actually, I'm also interested in the idea that we could skip saving the image entirely and just store the URL, and load it on start_conv. It would slow that step down a bit but that's a 1 time "cost" per conversation, not on every response, and it would allow me to save a single yaml file per person rather than needing a dir w/ multiple files. Then again..."practical reasons".

7/16/22 sat
-----------
~ -readme updates
	X -docs on using alexa skill (what are intents, sample utterances, etc.)
		UPDATE: this doesn't look like a ton of progress but it was actually a ton and took almost 2.5 hours.
	-record sample audio/video clip w/ alexa skill
		-lex for subreddit?
		-general purpose demo that shows all functionality: changing settings, generating person, ending conv and getting transcript, etc.
-look into best practices re running flask forever (e.g. thinking I should probably be using gunicorn? And maybe something like systemd also? Unsure.)
	-combine into 1 makefile cmd? And if so, maybe cleanup old separate commands.

7/17/22 sun
-----------
X -add pillow dep...somewhere? (Noticed it wasn't included in alexa env. Check if it needs to be in jabberwocky, alexa, gui, etc.)
	UPDATE: just forgot to rm import from lib utils. It's not actually used there anymore. Fixed.
X -investigate git hook issue (running ack from command line is WAY faster - seems like ignore-dir may not be working in script?)
	UPDATE: ack seems to (understandably) interpret ignore-dirs as absolute when the main path to search is absolute. I was using relative paths for ignore-dirs. Fixed.
~ -readme updates
	~ -alexa readme
		-add mention of needing to give permission for email/name
		X -see what it looks like if we rm spaces between quickstart replies and have 1 giant blockquote (diff than before bc names are bold now)
			UPDATE: yep, much better.
	-main readme
		X -revise opening paragraph
		~ -consider moving some gui stuff to gui/readme?
			UPDATE: moved some stuff. Still undecided if I should move more or not.
		X -if I like the no-space look in the alexa readme, update the quickstart here too
	~ -record sample audio/video clip w/ alexa skill
		~ -lex for subreddit?
			UPDATE: recorded a bunch of options. Should check on charges tomorrow to see if I need to edit my monthly limit.
		_ -general purpose demo that shows all functionality: changing settings, generating person, ending conv and getting transcript, etc.
			UPDATE: I think we can skip some of that. The docs show enough of that stuff.
X -allow GPT backend switch to be non-verbose
	UPDATE: a little ugly but it seems to work okay.
-check that gui still works
	-install on old device
	-launch gui
-look into best practices re running flask forever (e.g. thinking I should probably be using gunicorn? And maybe something like systemd also? Unsure.)
	-combine into 1 makefile cmd? And if so, maybe cleanup old separate commands.

7/18/22 mon
-----------
~ -readme updates
	X -alexa readme
		X -add mention of needing to give permission for email/name
		X -write instructions for code setup
		X -write instructions for console setup
			UPDATE: not sure how easily followable this really is but I don't really expect anyone to use this. If someone asks I can always help troubleshoot.
	-main readme
		-consider if there's anything else I want to move to/from gui readme
X -check charges for 7/17 or 7/18 (not sure which they'll show up on). See if I need to change limit for this month.
	UPDATE: not bad at all, $1-2. I guess a lot of those convs were very short.
-video demo
	-transfer all recordings to computer
	-select 1 or more for readme and/or reddit
	-consider trying to record another sanderson brainstorming sess
-look into possibility of adding option so gpt.query only logs to jsonlines for a single query (no stdout)
-look into best practices re running flask forever (e.g. thinking I should probably be using gunicorn? And maybe something like systemd also? Unsure.)
	-combine into 1 makefile cmd? And if so, maybe cleanup old separate commands.
X -add alexa dialog model json to git so people can more easily recreate skill in console

7/19/22 tues
------------
X -look into markdown rendering problems on readme + alexa/readme (blockquoted conv renders correctly in jupyterlab but not github)
	X -also affects sample utterances for alexa/readme intents section
	UPDATE: solution was to move <br> tags to lie flush at end of line provide a newline after. Previously had them on their own line, which rendered fine in jupyterlab but not github. Fyi, grip renders the same way github does, at least in this scenario.
X -maybe move instructions on python env and running gui to gui/readme
~ -video demo
	X -transfer all recordings to computer
		UPDATE: silently failed when I tried to airdrop all at once. Had to do them 1 by 1. They are currently in downloads.
	-select 1 or more for readme and/or reddit
	-consider trying to record another sanderson brainstorming sess
	-add video to readme (this probably has multiple substeps, e.g. do I need to upload to youtube or somewhere else?)
X -look into possibility of adding option so gpt.query only logs to jsonlines for a single query (no stdout)
	X -update htools logger cls
	X -update htools version in jabberwocky lib requirements and in alexa venv
	X -update GPTBackend cls
	X -test in LLM debugger nb
	X -add GPT method to wrap the logger's method

7/20/22 wed
-----------
-look into best practices re running flask forever (e.g. thinking I should probably be using gunicorn? And maybe something like systemd also? Unsure.)
	-combine into 1 makefile cmd? And if so, maybe cleanup old separate commands.
X -video demo
	~ -watch samples (currently in ~/downloads) and select 1 or more for readme and/or reddit
		-narrowed down to 3 candidates. Notes in misc.txt. Open in imovie.	
	_ -consider trying to record another sanderson brainstorming sess
		UPDATE: think we can skip this.
	~ -start cleaning up audio
		UPDATE: trying out imovie for this. Having some struggles but I think we'll get there.
	-add video to readme (this probably has multiple substeps, e.g. do I need to upload to youtube or somewhere else?)

7/20/22 wed
-----------
-audio cleanup
	-look up intended way to lower volume for a portion of audio
	-fix audio for 74
		-soften mouth noises
		-rm any unnecessary parts
	-fix audio for 73
		-soften mouth noises
		-rm any unnecessary parts
	-fix audio for 72
		-soften mouth noises
		-rm any unnecessary parts
	-add video to readme (this probably has multiple substeps, e.g. do I need to upload to youtube or somewhere else?)
-look into best practices re running flask forever (e.g. thinking I should probably be using gunicorn? And maybe something like systemd also? Unsure.)
	-combine into 1 makefile cmd? And if so, maybe cleanup old separate commands.


Backlog (short term)
--------------------
-check that gui still works
	-install on old device
	-launch gui
-check that alexa install works on new device after adding pytorch dep
-check if I need to validate that request was sent by alexa or if flask-ask handles that
	https://developer.amazon.com/en-US/docs/alexa/custom-skills/host-a-custom-skill-as-a-web-service.html#checking-the-signature-of-the-request

Easy Backlog (short term)
-------------------------
-look for things to document

Won't Do (at least short term; undecided long term)
---------------------------------------------------
-incorporate new bio cleanup prompt into convmanager
	REASON: slows things down a little, maybe not that big a difference in my memory. Would probably work better with the "Edit" models rather than completions but I haven't played around with those as much, and IIRC they only support zero shot (?).
-try to reproduce error w/ gpt banana: querying w/ np=2 and n=3 only produced 2 text responses. Second try worked as expected (6 responses - unsure why the difference. Maybe autoreload in ipython reset something since I edited docs first call?)
-more thorough testing for new stopword streaming functionality for paid backends
    -openai
    -gooseai
    -test backends on other cases
        -np > 1
        -nc > 1
        -nc > 1 AND np > 1
	REASON: Seems to be working fine so far, and I kind of forget which changes this was referring to tbh.
-update gui to support newer jabberwocky version
    -adjust gui accordingly (cur offer different radio buttons for each neo size; could instead have 1 radio and have it actually use the engine_i slider)
    -add gooseai backend option
	REASON: GUI was mostly just a learning experience and alexa is generally a far better interface for this kind of thing.
-support multiple echo users
	-change alexa/utils/Settings so all user settings are stored in flask state rather than only some. Reason is that if I ever want to even consider supporting multiple users, the current state would cause a big problem. (Initially remembered it as model kwargs being stored in session and others not, but after a quick look it actually seems to be the opposite.)
	REASON: I think a rather large redesign would be required, and a public skill would likely need openai approval.

Long Term Backlog (Jabberwocky V3 )
-----------------------------------
-update convmanager to support more generic conversation prompt (conversation_generalized.yaml). Prob wait for revised Prompt class with better support for default args, logic in assigning args from other args, etc.
-summarizing old conversations (this is quite a project in and of itself. See misc.txt on 6/22/22 for some thoughts on a potential updated prompt and thoughts on implementation. IMO a promising route would be adding a sliding window of summaries to the prompt.)
	-extract and save "relevant facts to remember" to some longer term storage solution (document search? 😀). Impractical/undesirable to add these all to the prompt. E.g. "my dad is a truck driver" would be a "relevant fact"; "{User} and {person} discuss {user}'s parents' careers" is a summary.
-multiple-person conversations (would need to revise method of storing conv.user_turns and conv.gpt_turns. Would no longer be able to assume we just interleave those two lists to get full conv.)
-consider if we can extend GPTBackend to handle other types of tasks (e.g. instead of GPT.query(), also allow us to go GPT.embed() or GPT.qa() or GPT.code()).
	-prob would need different config format for other tasks - some require multiple models, for instance
-idea: prompt mixin? I.e. add snippet of text to start/end of a prompt, e.g. "You are a friendly, polite AI assistant.".
-When generating a persona, assign a "whimsy" score indicating how serious/sciency vs. creative/whimsical the person is. We can then use this to auto-set temperature differently for each persona.
-redesign skill to work on multiple devices (i.e. a step towards making public usage technically feasible, even if I don't actually choose to do that)
	-see what happens if I try to use this on 2 devices at once. Is this even possible? Guessing this totally breaks everything bc I didn't design for multi users, but let's confirm that. Guess we need to create a new ConvManager for each user and store all settings in alexa session object - right now I think dot access usually uses session for vars in reserved_keys, but query kwargs set with getitem don't.

Long term backlog (misc)
------------------------
-prompts/use cases to prototype:
    -"unreleased work from {author}"
    -python stack trace -> stackoverflow solution
	-Global DSL - unsure of exact idea here but my experience w/ emotion markup suggests gpt is quite well suited to defining small DSLs. Have to think about this a bit but I think it might help to have some sort of general DSL prompt or function that makes it super easy to create/use one. (I.e. despite how easy it was to define the EML in the playground, I still don't feel like I instantly know how to create and use it programmatically. Might just be a matter of breaking down the steps and trying it out, or maybe some additional helpers are needed).
    -task with python library -> code snippet (get up to speed quickly with new library)
	-something to help with debugging code. Undecided on best way to approach this atm - could just make stack traces less technical/abstruse and hypothesize an explanation/solution in natural language (sort of like a more ambitious version of pypi package pretty-errors; could make custom debugger to drop in for pdb that lets you just ask questions instead of using rigid commands; could do something like jurigged (sp?), basically take advantage of hot reloading to make gpt try to debug/patch *running code in real time* 😱)
	-compute space and time complexity of code snippet
    -journal-based decision simulator (start an entry for a future date with desirable state ("Today was an amazing day because")or a specific positive outcome ("I met my SO 1 year ago at") or with a possible decision ("It's been six months since I took that job at {company} and I feel").
    -conv commentary (given a couple conv turns, gpt can comment "User seems a bit insensitive" or "user takes himself too seriously".
        -variant: given conv turns 1...n, generate a few options for n+1 and compare it to my real response at n+1.
	-emotion markup (Already started this but it requires some revision - see notes in prompt yaml. Note: most gpt options will likely be too slow for alexa, but the idea is cool enough that I think it's worth trying even if I don't know the exact use case yet.)
		-have gpt3 insert tags directly (i.e. Edit rather than complete)
		-have gpt3 select from options (almost like classification. Maybe just use logit_bias.) then fill in template programmatically.
-unit tests for natural language? Still a bit unsure what I have in mind here, this might just be a binary classifier. (Here are some rambling thoughts on the topic: if we want a response to be cheerful, we can write the instructions to increase the probability of that happening, but the prompt may have multiple parts and tone may get lost in the shuffle. We could increase our odds of a cheerful response by generating k responses, passing each through an additional prompt ("natural language unit test") asking "is this response cheerful?". We should be able to use the logit_bias arg with some positive weight to heavily incentivize the test to output "pass/fail" or "yes/no" (6/25 update: or just use gpt classification endpoint). Even if this is basically just a classifier, it's one we can specify in natural language (serves as documentation and technical specification of desired system behavior) and it potentially requires no training examples.)
