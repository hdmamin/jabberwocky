model: 2
temperature: 0.3
max_tokens: 64
stop:
- "tl;dr"
version: 0
reminder: 'This sets max tokens to 64. You may wish to adjust that value. The default model is Curie (2) but you could try a different model.'
doc: |-
    Zero shot prompt to summarize the user input text.
prompt: |-
    {}
    
    tl;dr:
