model: 2
temperature: .3
max_tokens: 250
stop: 
- "Abstract:"
- "Idea:"
version: 0
reminder: "I haven't tuned hyperparameters much - even model=1 sometimes provides good resuls but it seems a bit inconsistent."
doc: |-
    Few shot prompt to convert a 1-line machine learning paper idea into an abstract.
prompt: |-
    A fellow grad student came up with an interesting idea for a machine learning paper. Please write a more detailed abstract expanding on their idea.
    
    Idea:
    Graph attention networks, a neural network architecture that uses masked self-attention to operate on graphs, addresses shortcomings of prior methods based on graph convolutions.
    
    Abstract:
    We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).
    
    Idea:
    VQ-VAE uses multi-scale hierarchical organization and powerful priors over the latent codes to generate high quality samples.
    
    Abstract:
    We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.
    
    Idea:
    Mixup is a simple data augmentation method that improves the generalization neural networks.
    
    Abstract:
    Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.
    
    Idea:
    {}
    
    Abstract:
