New single-file prompt format for Jabberwocky>=2.0.0.

***
Prompt Name | Doc
---|---
analyze_writing | Few shot prompt to analyze a short paragraph and produce a bulleted list of words describing the writing style\.
conversation | The conversational prompt that powers jabberwocky\. Name is provided by the user and summary is usually auto\-retrieved from the person's wikipedia bio\. The GUI was originally developed with a version using temperature=\.5 and frequency\_penalty=0\.1\, but I've adjusted those settings a bit based on my perceptions of completion weaknesses and recommendations from the NLCA book\.
conversation_generalized | A more generalized template for some kind of verbal interaction between two people\. This could be a conversation\, an argument\, a negotiation\, a podcast\, a therapy session\, a private lesson\, a brainstorming session\, a debate\, etc\. Person could refer to a name or job title\. Summary can be used to provide any details to help flesh out the situation\. Arg "a" should either be "a" or "an" depending on the value of your "interaction" arg \- eventually this should be auto\-configured\, but I need to revamp the library a bit to handle this \(probably some jinja\-based solution\)\.
conversation_transcript | Slightly tweaked wording of the \`conversation\` prompt to use the phrasing "transcript"\. Unclear if this works better/worse/the same\.
debate | 1 shot prompt to provide bulleted arguments both for and against a subjective stance\.
default | Short prompt that only uses the user's input with no additional text\. You can pass in an empty string to get a truly empty prompt \(I believe gpt will start with the "<\|endoftext\|>" token in that case\)\.
eli | Zero shot prompt to rephrase user input to use simpler language\. Like "explain like I'm 5" but technically we specify a slightly older child\.
emotion_markup_language | Experimental one shot prompt to reformat text \(intended to be conversational\) to contain tags from a made up "Emotion Markup Language"\. Technically EML does exist though its syntax is different than what I specify here\. This is a work in progress and should probably eventually change to use something more like <emotion type='sadness'> instead of <sadness> \- seems like it might be desirable to make the output parsable as XML\. Also consider what length inputs should be \(1 sentence? 1 paragraph? Vary lengths dramatically?\)\. Also consider if logit bias can/should be used to constrain to a subset of tags\.
extract_backend_slot | Experimental prompt to help with slot extraction for jabberwocky\-alexa\. I ended up settling on a different method based on fuzzy matching so this is currently unused\.
how_to | Few shot prompt to break down a simple task into a numbered list of instructions\.
ml_abstract | Few shot prompt to convert a 1\-line machine learning paper idea into an abstract\.
mma | Few shot prompt to analyze MMA matchups and predict winners\. Note that the examples in the prompt were revised after the fights to more closely match the actual outcome\, in the hopes that gpt will try to mimic particularly prescient analysts\.
punctuate | Punctuate a portion of an auto\-generated YouTube transcript\. This may be poorly suited for other contexts\, and we provide separate punctuation prompts for Alexa and the jabberwocky gui\.
punctuate_alexa | Add punctuation to transcribed text\. This is intended for use with Alexa\, which lowercases user speech and does not include punctuation\. Because of my intended use case\, this is rather conversation\-oriented\.
punctuate_transcription | Add punctuation and fix transcription errors for short snippets of text recorded in the jabberwocky GUI \(which uses a Google transcription API\)\. This may not be a good fit for longer transcriptions\.
short_dates | Normalizes dates from various formats into a consistent natural language format\.  Mostly used for debugging purposes when I want to try something on a short\, relatively predictable prompt; a truly effective date normalizing prompt would require some more tuning\, and would probably be more useful if it output something more computer\-friendly than human\-friendly\.
shortest | This is just a short prompt for debugging purposes\.
simplify_ml | Few shot prompt to explain an excerpt of a machine learning paper in a simple way\. Inputs are typically a few sentences and outputs are usually a bit shorter than their inputs\. This was an old prompt and could likely be improved \- for instance\, we might try making the descriptive portion more specific\, since we currently provide no hints that the inputs are from machine learning papers\. We could also try prompting by proxy\. Someone like Jeremy Howard\, for instance\, would probably be better at explaining ML concepts in simple terms than the generic "I" referenced in the prompt\.
social_hypotheses | One shot prompt to generate several plausible interpretations of a social situation\.  The input should be a sentence describing something that "Person A" did\. \(We could probably just use the person's name but I don't think I actually tried that\.\)
summarize_conversation | \[WIP\]: zero shot prompt \(since it often ends up being quite long already\) to summarize part of an ongoing conversation \(generated via the \`conversation\` or \`conversation\_transcript\` prompt\)\. Bio is \`conv\.name2base\[conv\.current\['persona'\]\]\`\, excerpt is a nicely formatted version of the last n turns \(10 each seems like a decent choice \- that seems to produce an overall prompt length of ~1\_000 tokens\)\, name is conv\.current\['persona'\] \(maybe just use first name?\)\, and me is \`conv\.me\` \(maybe should replace "Me" with "I" if no name has been set there\.\)\.
test_professional | Experimental zero shot prompt to check if a piece of text uses professional \(i\.e\. work\-appropriate\) language\. This uses the logit\_bias arg to effectively serve as a classifier\.
tldr | Zero shot prompt to summarize the user input text\.
translate | Translate a short piece of English text to another language\. Eventually it may be useful to see if we can dynamically change max\_tokens based on the input length\, but in the meantime the stopwords should be reasonably effective at preventing overly long outputs\.
wiki_bio_cleanup | Experimental prompt to clean up the results of the wikipedia bios I've been retrieving with the "wikipedia" python lib\. I could imagine using a more varied set of inputs to try to create a generalized "webscraping cleaner" prompt but that will take some experimentation\.
word2number | Experimental few shot prompt to convert written numbers to numerals\. The original motive was to help parse Alexa slots but IIRC the issue disappeared when I restricted the valid values for the slot \- previously it accepted both numeric words and non\-numeric words\, so I think that made alexa resolve everything to non\-numeric\. Now that all valid "model" values are numeric\, it provides numerals for us\.