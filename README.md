<p align='center'>
<img src='data/icons/icon.png' height='100'>
<h1 align='center'>Jabberwocky</h1>
</p>

https://user-images.githubusercontent.com/40480855/132139847-0d0014b9-022e-4684-80bf-d46031ca4763.mp4

# Project Description

This project provides two audio interfaces (a conversational alexa skill and a desktop GUI) to GPT-3 and a few of its open source variants. One goal is to provide a convenient way to interact with various experts or public figures, as mimicked by GPT-3: imagine discussing physics with Einstein or hip hop with Kanye (or hip hop with Einstein? ðŸ¤”). Previously, this required going to the OpenAI playground, writing a bio, and periodically deleting parts of your conversation as you exceeded the max prompt window. Sometimes I just couldn't be bothered. But now, it's as simple as "Alexa, start Quick Chat"...

While the alexa skill is (currently) purely conversational, the GUI also provides Task Mode which contains built-in prompts for a number of sample tasks:

- Summarization
- Explain like I'm 5
- Translation
- How To (step by step instructions for performing everyday tasks)
- Writing Style Analysis
- Explain machine learning concepts in simple language
- Generate ML paper abstracts
- MMA Fight Analysis and Prediction

I anticipate that further development will take place on the alexa skill but I don't plan to actively maintain the GUI.


## Alexa Skill Usage

The video below provides a brief demo of the skill's basic functionality. You can view more thorough documentation in [alexa/README](alexa/README.md).

\# TODO: add demo vid

## GUI Usage

### Conversation Mode

In conversation mode, you can chat with a number of pre-defined personas or add new ones. New personas can be autogenerated or defined manually. 

![](data/clips/demo/add_persona.gif)

See `data/conversation_personas` for examples of autogenerated prompts. You can likely achieve better results using custom prompts though.

Conversation mode only supports spoken input, though you can edit flawed transcriptions manually. Querying GPT-3 with nonsensical or ungrammatical text will negatively affect response quality.

### Task Mode

In task mode, you can ask GPT-3 to perform a number pre-defined tasks. Written and spoken input are both supported. By default, GPT-3's response is both typed out and read aloud.

![](data/clips/demo/punctuation.gif)
Transcripts of responses from a small subset of non-conversation tasks can be found in the `data/completions` directory. You can also save your own completions while using the app.

### Usage Notes

The first time you speak, the speech transcription back end will take a few seconds to calibrate to the level of ambient noise in your environment. You will know it's ready for transcription when you see a "Listening..." message appear below the Record button. Calibration only occurs once to save time.

### Hotkeys

**CTRL + SHIFT**: Start recording audio (same as pressing the "Record" button).  
**CTRL + a**: Get GPT-3's response to whatever input you've recorded (same as pressing the "Get Response" button).

## Dev Notes

### Project Members
* Harrison Mamin

### Repo Structure
```
jabberwocky/
â”œâ”€â”€ lib          # Python package providing helpers for dealing with the openai api. Powers a lot of the functionality in both the GUI and alexa skill. Note that this also includes some other stuff that ultimately went unused - I initially experimented a bit with youtube transcripts, for example. Most useful functionality lives in the openai_utils module at this point.
â”œâ”€â”€ alexa        # Code used to create a conversational alexa skill.
â”œâ”€â”€ gui          # GUI scripts. The main script should be run from the project root directory. 
â”œâ”€â”€ data         # Raw and processed data. Some files are excluded from github but the ones needed to run the app are there. This also includes miscellaneous gpt3 prompt files and conversational personas.
â”œâ”€â”€ notes        # Miscellaneous notes from the development process stored as raw text files.
â”œâ”€â”€ notebooks    # Jupyter notebooks for experimentation and exploratory analysis.
â””â”€â”€ reports      # Markdown reports (performance reports, blog posts, etc.)
```

The `docker` and `setup` dirs contain remnants from previous attempts to package the app. While I ultimately decided to go with a simpler approach, I left them in the repo so I have the option of picking up where I left off if I decide to work on a new version.


### Getting Started

Use the following steps to set up your environment and run the GUI.

1. Clone the repo.

```
git clone https://github.com/hdmamin/jabberwocky.git
```

2. Install the necessary packages. I recommend using a virtual environment of some kind (virtualenv, conda, etc). If you're using Mac OS and virtualenv, you can use the command

```
make gui_env
```

to create a virtual environment for the GUI or

```
make alexa_env
````

to create a virtual environment for the alexa skill. If you're not using Mac OS or prefer to use a different environment manager, see `gui/make_env.sh` or `alexa/make_env.sh` to see what logic is actually being executed. Note: the GUI uses an older version of Jabberwocky. The library has since undergone many major changes and is not backward compatible.


3. Add your openai API key somewhere the program can access it. There are two ways to do this:

```
echo your_openai_api_key > ~/.openai
```

or

```
export OPENAI_API_KEY=your_openai_api_key
```

(Make sure to use your actual key, not the literal text `your_openai_api_key`.)

4. Run the app from inside your virtual environment.

```
source gui/venv/bin/activate
python gui/main.py
```

I also recommend using the command `make hooks` to install a git pre-commit hook to prevent you from accidentally exposing your openai API key. You only need to run this once.