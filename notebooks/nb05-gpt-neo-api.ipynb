{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "`Try getting GPT Neo predictions using the Huggingface API.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T03:47:11.819457Z",
     "start_time": "2021-05-26T03:47:11.803281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T02:52:25.431632Z",
     "start_time": "2021-05-27T02:52:25.369857Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.openai_utils import load_prompt, load_openai_api_key, \\\n",
    "    print_response\n",
    "from jabberwocky.utils import load_huggingface_api_key\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T03:47:52.065588Z",
     "start_time": "2021-05-26T03:47:52.035290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T04:15:31.706275Z",
     "start_time": "2021-05-26T04:15:31.627561Z"
    }
   },
   "outputs": [],
   "source": [
    "HF_API_KEY = load_huggingface_api_key()\n",
    "HEADERS = {'Authorization': f'Bearer api_{HF_API_KEY}'}\n",
    "URL_FMT = 'https://api-inference.huggingface.co/models/{}'\n",
    "# These accept different parameters. For now just start with the basics, but\n",
    "# keep these around in case I want to do something with them later.\n",
    "_task2suff = {'generate': 'EleutherAI/gpt-neo-2.7B',\n",
    "              'summarize': 'facebook/bart-large-cnn',\n",
    "              'chat': 'microsoft/DialoGPT-large',\n",
    "              'q&a': 'deepset/roberta-base-squad2'}\n",
    "TASK2URL = DotDict({k: URL_FMT.format(v) for k, v in _task2suff.items()})\n",
    "NEO_URL = URL_FMT.format(_task2suff['generate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T03:50:09.454483Z",
     "start_time": "2021-05-28T03:50:09.374407Z"
    }
   },
   "outputs": [],
   "source": [
    "@valuecheck\n",
    "def query_gpt_neo(prompt, top_k=None, top_p=None, temperature=1.0,\n",
    "                  repetition_penalty=None, max_tokens=250, api_key=None,\n",
    "                  size:('125M', '1.3B', '2.7B')='2.7B',\n",
    "                  **kwargs):\n",
    "    # Docs say we can return up to 256 tokens but API sometimes throws errors\n",
    "    # if we go above 250.\n",
    "    headers = {'Authorization':\n",
    "               f'Bearer api_{api_key or load_huggingface_api_key()}'}\n",
    "    # Notice the names don't always align with parameter names - I wanted\n",
    "    # those to be more consistent with query_gpt3() function. Also notice\n",
    "    # that types matter: if Huggingface expects a float but gets an int, we'll\n",
    "    # get an error.\n",
    "    if repetition_penalty is not None:\n",
    "        repetition_penalty = float(repetition_penalty)\n",
    "    data = {'inputs': prompt,\n",
    "            'parameters': {'top_k': top_k, 'top_p': top_p,\n",
    "                           'temperature': float(temperature),\n",
    "                           'max_new_tokens': min(max_tokens, 250),\n",
    "                           'repetition_penalty': repetition_penalty,\n",
    "                           'return_full_text': False}}\n",
    "    url = 'https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-{}'\n",
    "    r = requests.post(url.format(size), headers=headers, \n",
    "                      data=json.dumps(data))\n",
    "    r.raise_for_status()\n",
    "    res = r.json()[0]['generated_text']\n",
    "    if 'stop' in kwargs:\n",
    "        idx = [idx for idx in map(res.find, tolist(kwargs['stop'])) \n",
    "               if idx >= 0]\n",
    "        stop_idx = min(idx) if idx else None\n",
    "        res = res[:stop_idx]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T03:24:15.942336Z",
     "start_time": "2021-05-28T03:24:14.028385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mI love to\u001b[0m read. I’ve read a ton throughout my life, and I’ve read more recently. One of the\n"
     ]
    }
   ],
   "source": [
    "text = 'I love to'\n",
    "res = query_gpt_neo(text, max_tokens=25)\n",
    "\n",
    "print_response(text, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T03:24:25.478463Z",
     "start_time": "2021-05-28T03:24:24.976132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mI love to play the drums because it really relaxes me and\u001b[0m gives me a chance to be creative.” He has been playing since he was 5, and his father brought drums to his first band while in high school.\n",
      "\n",
      "“It was a blast,” he writes in a blog post announcing his move to New York. “It felt like an escape from the daily rat race. I took all that frustration and pressure and decided to take it out on the street. I had no clue what I was getting myself into, but I thought music was the only way I could express myself.”\n",
      "\n",
      "But the street turned out to be the very spot where Mr. Lefsetz was introduced to the music he loves. During a trip to Australia in high school, he was introduced to guitar by a local high school teacher.\n",
      "\n",
      "“That was my salvation,” he says. “At first I was a little skeptical because the guy was a total guitar nerd. Now I’m a huge fan. He showed me how you can play any song on the guitar in an authentic and effective way, and how important it is to find the right song.”\n",
      "\n",
      "He started playing bass, then switched to drums a couple years after his\n"
     ]
    }
   ],
   "source": [
    "text = 'I love to play the drums because it really relaxes me and'\n",
    "res = query_gpt_neo(text)\n",
    "\n",
    "print_response(text, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T03:26:15.765498Z",
     "start_time": "2021-05-28T03:26:15.407448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\u001b[0m April 11, 2021\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_kwargs = load_prompt('short_dates', verbose=False)\n",
    "res = query_gpt_neo(**dates_kwargs)\n",
    "\n",
    "print_response(dates_kwargs['prompt'], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T03:26:07.637635Z",
     "start_time": "2021-05-28T03:26:07.127123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\u001b[0m April 11, 2021\n",
      "\n",
      "Input: 01/\n"
     ]
    }
   ],
   "source": [
    "dates_kwargs = load_prompt('short_dates', verbose=False)\n",
    "res = query_gpt_neo(**select(dates_kwargs, drop=['stop']))\n",
    "\n",
    "print_response(dates_kwargs['prompt'], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T03:22:42.078380Z",
     "start_time": "2021-05-27T03:22:25.891772Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "\u001b[1mI love to play hockey because\u001b[0m I want to win! But in reality it’s more complicated than that… and much of what you think makes it that way isn’t at all true!\n",
      "\n",
      "It is easy to tell you to “play simple” yet I’ll bet if you showed up to a hockey game, most of the times that’s exactly what you’d be seeing. Because the truth is, if you look past the stats and try to analyse the game on its deepest level you quickly realise that the hockey world is littered with some of the most ridiculous rules and structures known to man.\n",
      "\n",
      "We have a bunch of “goals for” lists floating around and there is no better example of this than the infamous “10,000 shots in a season” statistic. It is widely believed that a player is judged by the number of shots they rack up – however, just as you can only judge a number of games with the available information, you can only really judge the number of shots a team has shot in one season – the statistics don’t always tell you that much.\n",
      "\n",
      "Now, we all know, it’s not just the number you shoot that matters\n"
     ]
    }
   ],
   "source": [
    "res = query_gpt_neo('I love to play hockey because', max_tokens=250)\n",
    "\n",
    "print_response(*res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T03:23:39.408773Z",
     "start_time": "2021-05-27T03:23:02.418514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "\u001b[1mA fellow grad student came up with an interesting idea for a machine learning paper. Please write a more detailed abstract expanding on their idea.\n",
      "\n",
      "Idea:\n",
      "Graph attention networks, a neural network architecture that uses masked self-attention to operate on graphs, addresses shortcomings of prior methods based on graph convolutions.\n",
      "\n",
      "Abstract:\n",
      "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).\n",
      "\n",
      "Idea:\n",
      "VQ-VAE uses multi-scale hierarchical organization and powerful priors over the latent codes to generate high quality samples.\n",
      "\n",
      "Abstract:\n",
      "We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.\n",
      "\n",
      "Idea:\n",
      "Mixup is a simple data augmentation method that improves the generalization neural networks.\n",
      "\n",
      "Abstract:\n",
      "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\n",
      "\n",
      "Idea:\n",
      "Stochastic tokenization allows us to achieve better model generalization on small datasets.\n",
      "\n",
      "Abstract:\u001b[0m\n",
      "In this paper, we present a novel approach to tokenization of unsupervised text data. We introduce the concept of stochastic tokenization, which allows us to achieve better model generalization on small datasets. We also show that our method can be used to improve the performance of state-of-the-art unsupervised text models.\n",
      "\n",
      "Idea:\n",
      "We propose a novel approach to the problem of image generation. Our method is based on a neural network architecture that operates on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph\n"
     ]
    }
   ],
   "source": [
    "ml_txt = \"\"\"Stochastic tokenization allows us to achieve better model\n",
    "generalization on small datasets.\"\"\".replace('\\n', ' ')\n",
    "ml_kwargs = load_prompt('ml_abstract', ml_txt, verbose=False)\n",
    "prompt, res = query_gpt_neo(**ml_kwargs)\n",
    "\n",
    "print_response(prompt, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T04:09:17.985722Z",
     "start_time": "2021-05-28T04:09:17.483155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n",
      "in finally\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-53e00c128d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'in finally'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B"
     ]
    }
   ],
   "source": [
    "from requests import HTTPError\n",
    "\n",
    "ml_txt = \"\"\"Dropout is a regularization method that enables models to\n",
    "generalize.\"\"\".replace('\\n', ' ')\n",
    "ml_kwargs = load_prompt('ml_abstract', ml_txt, verbose=False)\n",
    "try:\n",
    "    res = query_gpt_neo(**ml_kwargs)\n",
    "except HTTPError as e:\n",
    "    print('testing')\n",
    "    raise RuntimeError(str(e)) from None\n",
    "finally:\n",
    "    print('in finally')\n",
    "\n",
    "# print_response(prompt, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T04:01:53.462183Z",
     "start_time": "2021-05-28T04:01:53.397145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.exceptions.HTTPError"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T03:51:51.373626Z",
     "start_time": "2021-05-28T03:51:50.704060Z"
    }
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-93da899deb1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'one '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_gpt_neo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hmamin/pythonhm/htools/htools/meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 raise ValueError(f'Invalid argument for parameter {k}. '\n\u001b[1;32m   1507\u001b[0m                                  f'Value must be in {choices}.')\n\u001b[0;32m-> 1508\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-214-3f07b503352d>\u001b[0m in \u001b[0;36mquery_gpt_neo\u001b[0;34m(prompt, top_k, top_p, temperature, repetition_penalty, max_tokens, api_key, size, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     r = requests.post(url.format(size), headers=headers, \n\u001b[1;32m     24\u001b[0m                       data=json.dumps(data))\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'stop'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hmamin/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B"
     ]
    }
   ],
   "source": [
    "text = 'one ' * 20\n",
    "res = query_gpt_neo(text)\n",
    "\n",
    "print_response(text, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
