{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Make sure updated BackendSelector class is working properly. DO NOT PUSH THIS TO GITHUB (added to gitignore so api keys are not exposed).\n",
    "\n",
    "UPDATE: renamed to GPTBackend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:47:30.308266Z",
     "start_time": "2022-04-06T03:47:30.244657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:47:32.822589Z",
     "start_time": "2022-04-06T03:47:30.321265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.openai_utils import load_prompt, load_openai_api_key, \\\n",
    "    GPTBackend, PromptManager, ConversationManager, load_api_key, HF_API_KEY\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:47:32.892058Z",
     "start_time": "2022-04-06T03:47:32.827752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:47:32.956458Z",
     "start_time": "2022-04-06T03:47:32.896327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTBackend <current_name: openai>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTBackend()\n",
    "gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:47:34.926229Z",
     "start_time": "2022-04-06T03:47:34.896364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n"
     ]
    }
   ],
   "source": [
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:47:52.713046Z",
     "start_time": "2022-04-06T03:47:52.668136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "Switching openai backend to \"openai\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "Switching  backend back to \"openai\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "gooseai\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "Switching openai backend to \"gooseai\".\n",
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "Switching  backend back to \"openai\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "huggingface\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "Switching openai backend to \"huggingface\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt_huggingface at 0x1202be048>\n",
      "Switching  backend back to \"openai\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "hobby\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "Switching openai backend to \"hobby\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt_j at 0x12023fea0>\n",
      "Switching  backend back to \"openai\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "repeat\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "Switching openai backend to \"repeat\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt_repeat at 0x1202be0d0>\n",
      "Switching  backend back to \"openai\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "banana\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "Switching openai backend to \"banana\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt_banana at 0x1202be2f0>\n",
      "Switching  backend back to \"openai\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in gpt.name2key:\n",
    "    print(name)\n",
    "    gpt.ls()\n",
    "    with gpt(name):\n",
    "        gpt.ls()\n",
    "        assert gpt.key() == gpt.name2key[name]\n",
    "    gpt.ls()\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:48:02.077672Z",
     "start_time": "2022-04-06T03:48:02.032001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"openai\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"gooseai\".\n",
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt3 at 0x1202be158>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"huggingface\".\n",
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt_huggingface at 0x1202be048>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"hobby\".\n",
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt_j at 0x12023fea0>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"repeat\".\n",
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt_repeat at 0x1202be0d0>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"banana\".\n",
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt_banana at 0x1202be2f0>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in gpt.name2key:\n",
    "    gpt.switch(name)\n",
    "    gpt.ls()\n",
    "    assert gpt.key() == gpt.name2key[name]\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:48:05.521883Z",
     "start_time": "2022-04-06T03:48:05.492233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt_banana at 0x1202be2f0>\n"
     ]
    }
   ],
   "source": [
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:29:08.005310Z",
     "start_time": "2022-03-30T03:29:07.955640Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How historically accurate is', 'How historically accurate is')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.query(prompt='How historically accurate is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:29:09.693710Z",
     "start_time": "2022-03-30T03:29:09.661153Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How historically accurate is', 'How historically accurate is')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.query(prompt='How historically accurate is', engine_i=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T03:04:57.084504Z",
     "start_time": "2022-03-29T03:04:57.048805Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How historically accurate is', 'How historically accurate is')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.query(prompt='How historically accurate is',\n",
    "          log_path='data/logs/test.json', engine_i=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T03:06:05.410602Z",
     "start_time": "2022-03-29T03:06:05.132479Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"hobby\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:69: UserWarning: GPT-J api does not support other kwargs: {'engine_i': 0, 'frequency_penalty': 0.0}\n",
      "  warnings.warn(f'GPT-J api does not support other kwargs: {kwargs}')\n"
     ]
    },
    {
     "ename": "MockFunctionException",
     "evalue": "HTTPConnectionPool(host='api.vicgalle.net', port=5000): Max retries exceeded with url: /generate?context=How+accurate+is&token_max_length=50&temperature=0.7&top_p=1.0 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12a0b95c0>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMockFunctionException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-93075e127eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hobby'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'How accurate is'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pythonhm/htools/htools/meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1936\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1937\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(cls, prompt, log_path, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prompt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_query_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmock_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmock_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_gpt3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmock_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmock_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mquery_gpt3\u001b[0;34m(prompt, engine_i, temperature, frequency_penalty, max_tokens, logprobs, stream, mock, return_full, strip_output, mock_func, mock_mode, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mMockFunctionException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmock_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mmock_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'warn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mquery_gpt3\u001b[0;34m(prompt, engine_i, temperature, frequency_penalty, max_tokens, logprobs, stream, mock, return_full, strip_output, mock_func, mock_mode, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                     \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                     \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 )[1]\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mMockFunctionException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mquery_gpt_j\u001b[0;34m(prompt, temperature, max_tokens, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMockFunctionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMockFunctionException\u001b[0m: HTTPConnectionPool(host='api.vicgalle.net', port=5000): Max retries exceeded with url: /generate?context=How+accurate+is&token_max_length=50&temperature=0.7&top_p=1.0 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12a0b95c0>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "gpt.switch('hobby')\n",
    "gpt.query('How accurate is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:48:18.555995Z",
     "start_time": "2022-04-06T03:48:18.517889Z"
    }
   },
   "outputs": [],
   "source": [
    "gpt.switch('gooseai')\n",
    "gpt.ls()\n",
    "gpt.query('test', max_tokens=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T03:07:20.254526Z",
     "start_time": "2022-03-29T03:07:19.724007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test', '_set_max')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.query('test', max_tokens=4, log_path='data/logs/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PromptManager and ConversationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:31:18.097921Z",
     "start_time": "2022-03-30T03:31:17.978259Z"
    }
   },
   "outputs": [],
   "source": [
    "conv = ConversationManager(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:31:18.396740Z",
     "start_time": "2022-03-30T03:31:18.143218Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplify_ml: This uses the expensive davinci model and doesn't work so well without it. Temperature is set to 0.3 but this hasn't been extensively tuned.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "shortest: This prompt takes no input.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "extract_backend_slot: WARNING: This seemed to work okay when testing in the gooseai console but further testing suggests it needs more work.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "ml_abstract: I haven't tuned hyperparameters much - even engine_i=1 sometimes provides good resuls but it seems a bit inconsistent.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "translate: User input should consist of 1 line like 'Translate to German:' followed by a second line with a sentence in English. You may want to use a variable number of tokens defined as a function of the input length. Weaker engines sometimes work but sometimes translate into the wrong language.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "default: This uses the weakest engine by default.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "how_to: Should be a single line starting with the words \"How to\" and ending in a colon. You may need a stronger engine for good results.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "debate: Non-zero frequency penalty was initially included by accident, but in at least 1 test removing it noticeably worsened results.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "analyze_writing: Lower engines could get quite repetitive.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "mma: Haven't really experimented with hyperparameters.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "punctuate_transcription: You should probably adjust max_tokens based on the length of the input or just set max length high and let stop phrases do the rest. Bumping up to engine 2 or 3 might help a little, but engine 1 is serviceable (probably best to avoid 0 though).\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "punctuate_alexa: You should probably adjust max_tokens based on the length of the input or just set max length high and let stop phrases do the rest. Bumping up to engine 2 or 3 might help a little, but engine 1 is serviceable (probably best to avoid 0 though). We use a separate prompt for alexa because its transcriptions seem to handle casing a bit differently than the python speech_recognition package I use in the GUI.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "punctuate: You should probably adjust max_tokens based on the length of the input. Bumping up to engine 2 or 3 might help a little, but engine 1 is serviceable (probably best to avoid 0 though). You should probably try training a huggingface model to add punctuation instead of using gpt3 credits though.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "word2number: This was not designed to work with negatives or terms like 'half', 'quarter', etc.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "tldr: This sets max tokens to 64. You may wish to adjust that value. The default engine is Curie (2) but you could try a different engine.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "eli: This uses the expensive davinci model and doesn't work so well without it.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "conversation: Might want to try tweaking frequency penalty.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "short_dates: This prompt takes no input.\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompter = PromptManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T03:07:48.607331Z",
     "start_time": "2022-03-29T03:07:48.565704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"openai\".\n",
      "{'engine_i': 3, 'temperature': 0.3, 'frequency_penalty': 0.3, 'max_tokens': 200, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': None, 'mock_mode': 'raise', 'stop': ['Translate to ', 'Translation:']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"gooseai\".\n",
      "{'engine_i': 3, 'temperature': 0.3, 'frequency_penalty': 0.3, 'max_tokens': 200, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': None, 'mock_mode': 'raise', 'stop': ['Translate to ', 'Translation:']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"huggingface\".\n",
      "{'engine_i': 3, 'temperature': 0.3, 'frequency_penalty': 0.3, 'max_tokens': 200, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': <function query_gpt_huggingface at 0x12a1411e0>, 'mock_mode': 'raise', 'stop': ['Translate to ', 'Translation:']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"hobby\".\n",
      "{'engine_i': 3, 'temperature': 0.3, 'frequency_penalty': 0.3, 'max_tokens': 200, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': <function query_gpt_j at 0x11081bd08>, 'mock_mode': 'raise', 'stop': ['Translate to ', 'Translation:']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"repeat\".\n",
      "{'engine_i': 3, 'temperature': 0.3, 'frequency_penalty': 0.3, 'max_tokens': 200, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': <function query_gpt_repeat at 0x12a1410d0>, 'mock_mode': 'raise', 'stop': ['Translate to ', 'Translation:']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in gpt.name2key:\n",
    "    gpt.switch(name)\n",
    "    kwargs_ = prompter.kwargs('translate')\n",
    "    print(kwargs_)\n",
    "    assert kwargs_['mock_func'] == gpt.name2mock[name]\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T03:07:53.486364Z",
     "start_time": "2022-03-29T03:07:53.449489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"openai\".\n",
      "{'engine_i': 3, 'temperature': 0.5, 'frequency_penalty': 0.1, 'max_tokens': 250, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': None, 'mock_mode': 'raise', 'stop': ['\\n\\nMe:', 'This is a conversation with']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"gooseai\".\n",
      "{'engine_i': 3, 'temperature': 0.5, 'frequency_penalty': 0.1, 'max_tokens': 250, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': None, 'mock_mode': 'raise', 'stop': ['\\n\\nMe:', 'This is a conversation with']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"huggingface\".\n",
      "{'engine_i': 3, 'temperature': 0.5, 'frequency_penalty': 0.1, 'max_tokens': 250, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': <function query_gpt_huggingface at 0x12a1411e0>, 'mock_mode': 'raise', 'stop': ['\\n\\nMe:', 'This is a conversation with']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"hobby\".\n",
      "{'engine_i': 3, 'temperature': 0.5, 'frequency_penalty': 0.1, 'max_tokens': 250, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': <function query_gpt_j at 0x11081bd08>, 'mock_mode': 'raise', 'stop': ['\\n\\nMe:', 'This is a conversation with']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"repeat\".\n",
      "{'engine_i': 3, 'temperature': 0.5, 'frequency_penalty': 0.1, 'max_tokens': 250, 'logprobs': None, 'stream': False, 'mock': False, 'return_full': False, 'strip_output': True, 'mock_func': <function query_gpt_repeat at 0x12a1410d0>, 'mock_mode': 'raise', 'stop': ['\\n\\nMe:', 'This is a conversation with']}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in gpt.name2key:\n",
    "    gpt.switch(name)\n",
    "    kwargs_ = conv.kwargs('Albert Einstein')\n",
    "    print(kwargs_)\n",
    "    assert kwargs_['mock_func'] == gpt.name2mock[name]\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T03:09:34.810632Z",
     "start_time": "2022-03-29T03:09:29.213054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"openai\".\n",
      "A book, a\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"gooseai\".\n",
      "I think the best\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"huggingface\".\n",
      "I would recommend a\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching openai backend to \"repeat\".\n",
      "This is a conversation with Albert Einstein. Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist, widely acknowledged to be one of the greatest physicists of all time. Einstein is known for developing the theory of relativity, but he also made important contributions to the development of the theory of quantum mechanics.\n",
      "\n",
      "Me: Hi Albert. What is a book you'd recommend?\n",
      "\n",
      "Albert Einstein:\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in gpt.name2key:\n",
    "    # Broken at the moment.\n",
    "    if name == 'hobby': continue\n",
    "    gpt.switch(name)\n",
    "    with conv.converse('Albert Einstein'):\n",
    "        res = conv.query(\n",
    "            'Hi Albert. What is a book you\\'d recommend?',\n",
    "            max_tokens=4,\n",
    "            engine_i=0\n",
    "        )\n",
    "    print(res[1])\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T03:08:38.529219Z",
     "start_time": "2022-03-29T03:08:38.496711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "This is a conversation with Albert Einstein. Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist, widely acknowledged to be one of the greatest physicists of all time. Einstein is known for developing the theory of relativity, but he also made important contributions to the development of the theory of quantum mechanics.\n",
      "\n",
      "Me: Hi Albert. What is a book you'd recommend?\n",
      "\n",
      "Albert Einstein:\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('repeat')\n",
    "with conv.converse('Albert Einstein'):\n",
    "    res = conv.query(\n",
    "        'Hi Albert. What is a book you\\'d recommend?',\n",
    "        max_tokens=4,\n",
    "        engine_i=0\n",
    "    )\n",
    "    print(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T22:10:05.910478Z",
     "start_time": "2022-04-02T22:10:05.824772Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_at_first_stop(text, stop_phrases, finish_reason='',\n",
    "                           trunc_full=True, trunc_partial=True):\n",
    "    if trunc_full:\n",
    "        idx = [idx for idx in map(text.find, stop_phrases) if idx >= 0]\n",
    "        stop_idx = min(idx or [None])\n",
    "        text = text[:stop_idx]\n",
    "\n",
    "    # If the completion was cut short due to length AND the completion ends\n",
    "    # with the majority of a stop phrase, we infer that this should be stripped\n",
    "    # from the end. This rule won't be perfect but it seems like a decent bet.\n",
    "    if trunc_partial and finish_reason == 'length':\n",
    "        for phrase in stop_phrases:\n",
    "            chunk = phrase[:max(int(round(.8 * len(phrase))), 4)]\n",
    "            if text.endswith(chunk):\n",
    "                warnings.warn(\n",
    "                    'Guessing that truncation is reasonable because '\n",
    "                    'finish_reason=\"length\" and completion ends with a '\n",
    "                    'partial stop phrase.'\n",
    "                )\n",
    "                return text.rpartition(chunk)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T22:10:08.114915Z",
     "start_time": "2022-04-02T22:10:08.052928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'object', 'created', 'model', 'choices'])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock = load(C.mock_stream_paths[False])\n",
    "mock.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T22:10:08.648263Z",
     "start_time": "2022-04-02T22:10:08.575322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'length'"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock.choices[0].finish_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T22:19:36.253412Z",
     "start_time": "2022-04-02T22:19:36.181995Z"
    }
   },
   "outputs": [],
   "source": [
    "no_stop = \"\"\"\n",
    "Einstein: How are you?\"\"\"\n",
    "\n",
    "full_stop = \"\"\"\n",
    "Einstein: How are you?\n",
    "\n",
    "Me:\"\"\"\n",
    "\n",
    "extra_stop = \"\"\"\n",
    "Einstein: How are you?\n",
    "\n",
    "Me: I am good.\"\"\"\n",
    "\n",
    "partial_stop = \"\"\"\n",
    "Einstein: How are you?\n",
    "\n",
    "Me\"\"\"\n",
    "\n",
    "tricky_stop = \"\"\"\n",
    "Einstein: How are you?\n",
    "\n",
    "Me, I'm fine.\n",
    "\"\"\"\n",
    "\n",
    "correct = no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T22:20:00.243910Z",
     "start_time": "2022-04-02T22:20:00.190006Z"
    }
   },
   "outputs": [],
   "source": [
    "assert truncate_at_first_stop(no_stop, conv._kwargs['stop']) == correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T22:20:58.541558Z",
     "start_time": "2022-04-02T22:20:58.468302Z"
    }
   },
   "outputs": [],
   "source": [
    "assert truncate_at_first_stop(full_stop, conv._kwargs['stop']) == correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T22:20:58.541558Z",
     "start_time": "2022-04-02T22:20:58.468302Z"
    }
   },
   "outputs": [],
   "source": [
    "assert truncate_at_first_stop(extra_stop, conv._kwargs['stop']) == correct\n",
    "assert truncate_at_first_stop(extra_stop, conv._kwargs['stop'], \n",
    "                              finish_reason='length') == correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T22:20:58.541558Z",
     "start_time": "2022-04-02T22:20:58.468302Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Guessing that truncation is reasonable because finish_reason=\"length\" and completion ends with a partial stop phrase.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "assert truncate_at_first_stop(partial_stop, \n",
    "                              conv._kwargs['stop']) == partial_stop\n",
    "assert truncate_at_first_stop(partial_stop, conv._kwargs['stop'],\n",
    "                              finish_reason='length',\n",
    "                              trunc_partial=False) == partial_stop\n",
    "assert truncate_at_first_stop(partial_stop, conv._kwargs['stop'],\n",
    "                              finish_reason='length') == correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T22:20:39.976654Z",
     "start_time": "2022-04-02T22:20:39.859528Z"
    }
   },
   "outputs": [],
   "source": [
    "assert truncate_at_first_stop(tricky_stop, \n",
    "                              conv._kwargs['stop']) == tricky_stop\n",
    "assert truncate_at_first_stop(tricky_stop, conv._kwargs['stop'],\n",
    "                              finish_reason='length',\n",
    "                              trunc_partial=False) == tricky_stop\n",
    "assert truncate_at_first_stop(tricky_stop, conv._kwargs['stop'],\n",
    "                              finish_reason='length') == tricky_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:38:33.365568Z",
     "start_time": "2022-03-30T03:38:32.603315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "This is a conversation with Albert Einstein. Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist, widely acknowledged to be one of the greatest physicists of all time. Einstein is known for developing the theory of relativity, but he also made important contributions to the development of the theory of quantum mechanics.\n",
      "\n",
      "Me: Hi Albert.\n",
      "\n",
      "Albert Einstein: Hi.\n",
      "\n",
      "Me:\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('gooseai')\n",
    "conv.start_conversation('Albert Einstein')\n",
    "res = conv.query(\n",
    "    'Hi Albert.',\n",
    "    max_tokens=50,\n",
    "    engine_i=0,\n",
    "    return_full=True\n",
    ")\n",
    "print(conv.full_conversation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:39:44.599165Z",
     "start_time": "2022-03-30T03:39:44.267437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a conversation with Albert Einstein. Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist, widely acknowledged to be one of the greatest physicists of all time. Einstein is known for developing the theory of relativity, but he also made important contributions to the development of the theory of quantum mechanics.\n",
      "\n",
      "Me: Hi Albert.\n",
      "\n",
      "Albert Einstein: Hi.\n",
      "\n",
      "Me:\n",
      "\n",
      "Me: What open question in physics do you most hope to learn an answer to?\n",
      "\n",
      "Albert Einstein: Well\n"
     ]
    }
   ],
   "source": [
    "res_strict_limit = conv.query(\n",
    "    'What open question in physics do you most hope to learn an answer to?',\n",
    "    max_tokens=1,\n",
    "    engine_i=0,\n",
    "    return_full=True\n",
    ")\n",
    "print(conv.full_conversation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:39:50.078713Z",
     "start_time": "2022-03-30T03:39:50.020598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.\\n\\nMe:'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:39:55.979337Z",
     "start_time": "2022-03-30T03:39:55.917806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_strict_limit[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:39:57.054544Z",
     "start_time": "2022-03-30T03:39:57.001751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stop'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[2]['choices'][0]['finish_reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:40:00.354683Z",
     "start_time": "2022-03-30T03:40:00.309337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'length'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_strict_limit[2]['choices'][0]['finish_reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:54:16.880646Z",
     "start_time": "2022-03-30T03:54:16.836847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_at_first_stop(res[1], conv.kwargs()['stop'], res[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:54:17.410523Z",
     "start_time": "2022-03-30T03:54:17.373949Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_at_first_stop(res_strict_limit[1], conv.kwargs()['stop'],\n",
    "                       res_strict_limit[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:54:37.211584Z",
     "start_time": "2022-03-30T03:54:37.174934Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Guessing that truncation is reasonable because finish_reason=\"length\" and completion ends with a partial stop phrase.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Well'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Well\\n\\nMe'\n",
    "truncate_at_first_stop(text, conv.kwargs()['stop'],\n",
    "                       res_strict_limit[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:46:07.474926Z",
     "start_time": "2022-03-30T03:46:07.425808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nMe:',\n",
       " 'This is a conversation with',\n",
       " '\\n\\nAlbert Einstein:',\n",
       " '\\n\\nJeremy Howard:',\n",
       " '\\n\\nI:']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = conv.kwargs()['stop'] + ['\\n\\nAlbert Einstein:', \n",
    "                                 '\\n\\nJeremy Howard:',\n",
    "                                 '\\n\\nI:']\n",
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:46:08.696903Z",
     "start_time": "2022-03-30T03:46:08.666636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n\\nMe:': '\\n\\nMe',\n",
       " 'This is a conversation with': 'This is a conversation',\n",
       " '\\n\\nAlbert Einstein:': '\\n\\nAlbert Einst',\n",
       " '\\n\\nJeremy Howard:': '\\n\\nJeremy Howa',\n",
       " '\\n\\nI:': '\\n\\nI:'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{stop: stop[:max(int(round(.8 * len(stop))), 4)] for stop in stops}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:58:14.264575Z",
     "start_time": "2022-03-30T03:58:12.257224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Switching openai backend to \"huggingface\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:153: UserWarning: query_gpt_huggingface received unused kwargs {'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'logprobs': None, 'n': 1, 'stream': False, 'logit_bias': None}.\n",
      "  warnings.warn('query_gpt_huggingface received unused kwargs '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching  backend back to \"gooseai\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    res_hf = gpt.query('Who are you?', engine_i=0, return_full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T03:58:31.111430Z",
     "start_time": "2022-03-30T03:58:31.067029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-2onIQZc241dqfnum3Uducepxxa39u at 0x12862f200> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": {\n",
       "        \"text_offset\": [\n",
       "          147,\n",
       "          149,\n",
       "          150,\n",
       "          152,\n",
       "          153,\n",
       "          155,\n",
       "          156,\n",
       "          157,\n",
       "          162,\n",
       "          163,\n",
       "          166,\n",
       "          167,\n",
       "          169,\n",
       "          170,\n",
       "          174,\n",
       "          175,\n",
       "          181,\n",
       "          182,\n",
       "          190,\n",
       "          193,\n",
       "          194,\n",
       "          199,\n",
       "          200,\n",
       "          201,\n",
       "          206,\n",
       "          207,\n",
       "          212,\n",
       "          213,\n",
       "          219,\n",
       "          220,\n",
       "          225,\n",
       "          226,\n",
       "          227,\n",
       "          232,\n",
       "          233,\n",
       "          236,\n",
       "          237,\n",
       "          239,\n",
       "          240,\n",
       "          244,\n",
       "          245,\n",
       "          251,\n",
       "          252,\n",
       "          255,\n",
       "          256,\n",
       "          258,\n",
       "          259,\n",
       "          263,\n",
       "          264,\n",
       "          265\n",
       "        ],\n",
       "        \"token_logprobs\": [\n",
       "          -5.010095,\n",
       "          -0.07669473,\n",
       "          -0.11393089,\n",
       "          -0.2081038,\n",
       "          -0.2685986,\n",
       "          -0.18119302,\n",
       "          -0.12463011,\n",
       "          -0.25594696,\n",
       "          -0.009890747,\n",
       "          -2.9197333,\n",
       "          -0.05882589,\n",
       "          -3.2186384,\n",
       "          -0.020903256,\n",
       "          -3.3703144,\n",
       "          -0.04415964,\n",
       "          -0.16602105,\n",
       "          -0.00207874,\n",
       "          -1.774754,\n",
       "          -0.067975275,\n",
       "          -0.008811459,\n",
       "          -0.015094891,\n",
       "          -0.08882021,\n",
       "          -0.0592656,\n",
       "          -0.38780832,\n",
       "          -0.0077960584,\n",
       "          -7.67215,\n",
       "          -1.1175944,\n",
       "          -0.2418642,\n",
       "          -0.0027772784,\n",
       "          -0.08681413,\n",
       "          -0.15174149,\n",
       "          -0.08628697,\n",
       "          -0.4371677,\n",
       "          -0.009785712,\n",
       "          -3.1245492,\n",
       "          -0.024241686,\n",
       "          -2.3223312,\n",
       "          -0.010141442,\n",
       "          -1.221259,\n",
       "          -0.028191218,\n",
       "          -0.18216114,\n",
       "          -0.0020476829,\n",
       "          -3.246,\n",
       "          -0.002533144,\n",
       "          -0.09807059,\n",
       "          -0.004849469,\n",
       "          -0.031385567,\n",
       "          -0.06089116,\n",
       "          -0.06655156,\n",
       "          -0.33267894\n",
       "        ],\n",
       "        \"tokens\": [\n",
       "          \" 4\",\n",
       "          \"/\",\n",
       "          \"11\",\n",
       "          \"/\",\n",
       "          \"21\",\n",
       "          \"\\n\",\n",
       "          \"\\n\",\n",
       "          \"Input\",\n",
       "          \":\",\n",
       "          \" 01\",\n",
       "          \"/\",\n",
       "          \"20\",\n",
       "          \"/\",\n",
       "          \"2017\",\n",
       "          \"\\n\",\n",
       "          \"Output\",\n",
       "          \":\",\n",
       "          \" January\",\n",
       "          \" 20\",\n",
       "          \",\",\n",
       "          \" 2017\",\n",
       "          \"\\n\",\n",
       "          \"\\n\",\n",
       "          \"Input\",\n",
       "          \":\",\n",
       "          \" 2017\",\n",
       "          \"\\n\",\n",
       "          \"Output\",\n",
       "          \":\",\n",
       "          \" 2017\",\n",
       "          \"\\n\",\n",
       "          \"\\n\",\n",
       "          \"Input\",\n",
       "          \":\",\n",
       "          \" 07\",\n",
       "          \"/\",\n",
       "          \"01\",\n",
       "          \"/\",\n",
       "          \"2017\",\n",
       "          \"\\n\",\n",
       "          \"Output\",\n",
       "          \":\",\n",
       "          \" 07\",\n",
       "          \"/\",\n",
       "          \"01\",\n",
       "          \"/\",\n",
       "          \"2017\",\n",
       "          \"\\n\",\n",
       "          \"\\n\",\n",
       "          \"Input\"\n",
       "        ],\n",
       "        \"top_logprobs\": [\n",
       "          {\n",
       "            \" 04\": -2.4684024,\n",
       "            \" April\": -1.7035612,\n",
       "            \" November\": -0.91182953\n",
       "          },\n",
       "          {\n",
       "            \" November\": -4.218803,\n",
       "            \"-\": -4.200261,\n",
       "            \"/\": -0.07669473\n",
       "          },\n",
       "          {\n",
       "            \"10\": -3.6921072,\n",
       "            \"11\": -0.11393089,\n",
       "            \"12\": -3.8763573\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -6.3947086,\n",
       "            \",\": -1.7023474,\n",
       "            \"/\": -0.2081038\n",
       "          },\n",
       "          {\n",
       "            \"20\": -2.3687325,\n",
       "            \"2017\": -3.025337,\n",
       "            \"21\": -0.2685986\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.18119302,\n",
       "            \"\\n\\n\": -2.406815,\n",
       "            \",\": -3.6320055\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.12463011,\n",
       "            \"<|endoftext|>\": -4.0127277,\n",
       "            \"Input\": -3.4765031\n",
       "          },\n",
       "          {\n",
       "            \"Input\": -0.25594696,\n",
       "            \"Output\": -2.4697456,\n",
       "            \"The\": -4.293031\n",
       "          },\n",
       "          {\n",
       "            \"/\": -7.874479,\n",
       "            \":\": -0.009890747,\n",
       "            \"s\": -5.7541285\n",
       "          },\n",
       "          {\n",
       "            \" 03\": -2.4402366,\n",
       "            \" 04\": -1.8315728,\n",
       "            \" 11\": -2.4333127\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -6.729456,\n",
       "            \"-\": -3.162448,\n",
       "            \"/\": -0.05882589\n",
       "          },\n",
       "          {\n",
       "            \"01\": -2.2484198,\n",
       "            \"04\": -3.0673704,\n",
       "            \"11\": -2.80992\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -4.6655216,\n",
       "            \"-\": -6.517379,\n",
       "            \"/\": -0.020903256\n",
       "          },\n",
       "          {\n",
       "            \"19\": -2.9104333,\n",
       "            \"20\": -1.5845724,\n",
       "            \"22\": -2.644738\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.04415964,\n",
       "            \" Output\": -5.7255354,\n",
       "            \"Output\": -4.0714903\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -2.1591334,\n",
       "            \"Output\": -0.16602105,\n",
       "            \"Production\": -5.939807\n",
       "          },\n",
       "          {\n",
       "            \" :\": -8.85904,\n",
       "            \":\": -0.00207874,\n",
       "            \"s\": -9.349521\n",
       "          },\n",
       "          {\n",
       "            \" 01\": -1.4938341,\n",
       "            \" 1\": -1.3787476,\n",
       "            \" January\": -1.774754\n",
       "          },\n",
       "          {\n",
       "            \" 19\": -4.5955725,\n",
       "            \" 20\": -0.067975275,\n",
       "            \" 21\": -4.237115\n",
       "          },\n",
       "          {\n",
       "            \" 2017\": -6.0902734,\n",
       "            \",\": -0.008811459,\n",
       "            \"th\": -6.129834\n",
       "          },\n",
       "          {\n",
       "            \" 2017\": -0.015094891,\n",
       "            \" 2018\": -5.5754356,\n",
       "            \"2017\": -5.590189\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.08882021,\n",
       "            \"\\n\\n\": -3.1148937,\n",
       "            \"<|endoftext|>\": -4.0333405\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.0592656,\n",
       "            \"<|endoftext|>\": -3.7323363,\n",
       "            \"Input\": -5.7491436\n",
       "          },\n",
       "          {\n",
       "            \"Input\": -0.38780832,\n",
       "            \"Output\": -2.2340264,\n",
       "            \"The\": -3.8970342\n",
       "          },\n",
       "          {\n",
       "            \"/\": -8.186094,\n",
       "            \":\": -0.0077960584,\n",
       "            \"s\": -5.8882113\n",
       "          },\n",
       "          {\n",
       "            \" 01\": -2.1928318,\n",
       "            \" 03\": -2.3588436,\n",
       "            \" 04\": -2.0098932\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -1.1175944,\n",
       "            \"-\": -1.2459066,\n",
       "            \"/\": -1.2654244\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -1.870141,\n",
       "            \"2017\": -5.4820585,\n",
       "            \"Output\": -0.2418642\n",
       "          },\n",
       "          {\n",
       "            \" :\": -8.485966,\n",
       "            \":\": -0.0027772784,\n",
       "            \"s\": -8.579437\n",
       "          },\n",
       "          {\n",
       "            \" 2016\": -4.9209414,\n",
       "            \" 2017\": -0.08681413,\n",
       "            \" 2018\": -3.7304745\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.15174149,\n",
       "            \"\\n\\n\": -3.5581906,\n",
       "            \"-\": -2.931847\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.08628697,\n",
       "            \"<|endoftext|>\": -3.2731342,\n",
       "            \"Input\": -5.6829205\n",
       "          },\n",
       "          {\n",
       "            \"Input\": -0.4371677,\n",
       "            \"Output\": -2.1884952,\n",
       "            \"The\": -3.7845721\n",
       "          },\n",
       "          {\n",
       "            \"/\": -7.846905,\n",
       "            \":\": -0.009785712,\n",
       "            \"s\": -5.7876544\n",
       "          },\n",
       "          {\n",
       "            \" 01\": -2.1481304,\n",
       "            \" 04\": -2.6545238,\n",
       "            \" 2017\": -2.5231113\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -7.04655,\n",
       "            \"-\": -4.0766153,\n",
       "            \"/\": -0.024241686\n",
       "          },\n",
       "          {\n",
       "            \"01\": -2.3223312,\n",
       "            \"11\": -3.090718,\n",
       "            \"20\": -2.593867\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -4.892093,\n",
       "            \"-\": -7.45354,\n",
       "            \"/\": -0.010141442\n",
       "          },\n",
       "          {\n",
       "            \"20\": -2.0023718,\n",
       "            \"2017\": -1.221259,\n",
       "            \"2018\": -2.8698606\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.028191218,\n",
       "            \"\\n\\n\": -6.0045066,\n",
       "            \"Output\": -5.030211\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -2.0749836,\n",
       "            \"2017\": -5.8476763,\n",
       "            \"Output\": -0.18216114\n",
       "          },\n",
       "          {\n",
       "            \" :\": -8.818442,\n",
       "            \":\": -0.0020476829,\n",
       "            \"s\": -9.307506\n",
       "          },\n",
       "          {\n",
       "            \" 2017\": -2.1420486,\n",
       "            \" 7\": -0.7052601,\n",
       "            \" July\": -1.5261801\n",
       "          },\n",
       "          {\n",
       "            \"-\": -6.825964,\n",
       "            \".\": -7.4314785,\n",
       "            \"/\": -0.002533144\n",
       "          },\n",
       "          {\n",
       "            \"01\": -0.09807059,\n",
       "            \"02\": -5.0334053,\n",
       "            \"1\": -3.2211037\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -5.9796357,\n",
       "            \",\": -6.5062838,\n",
       "            \"/\": -0.004849469\n",
       "          },\n",
       "          {\n",
       "            \"17\": -3.9673405,\n",
       "            \"2016\": -6.3723392,\n",
       "            \"2017\": -0.031385567\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.06089116,\n",
       "            \"\\n\\n\": -3.701987,\n",
       "            \"<|endoftext|>\": -4.364405\n",
       "          },\n",
       "          {\n",
       "            \"\\n\": -0.06655156,\n",
       "            \"<|endoftext|>\": -3.7790704,\n",
       "            \"Input\": -5.3089466\n",
       "          },\n",
       "          {\n",
       "            \"Input\": -0.33267894,\n",
       "            \"Output\": -2.0554059,\n",
       "            \"The\": -4.3769083\n",
       "          }\n",
       "        ]\n",
       "      },\n",
       "      \"text\": \"\\n\\nI am a woman who has been in the business for over 20 years. I have been a member of the board of directors of the American Association of Retired Persons (AARP) since the mid-1990s. I have been a\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1618458570,\n",
       "  \"id\": \"cmpl-2onIQZc241dqfnum3Uducepxxa39u\",\n",
       "  \"model\": \"ada:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_hf[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for \"finish_reason\"\n",
    "\n",
    "Do HF/vic galle backends provide finish_reason in their response?\n",
    "\n",
    "(These are very messy versions - the cleaner final versions are in jabberwocky.opena_utils.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts on desired interface. np refers to number of prompts, nc refers to number of completions.\n",
    "\n",
    "```\n",
    "# np>1, nc=1\n",
    "prompt_, response_ = qpt.query(['a', 'b', 'c'])\n",
    "prompt_ # ['a', 'b', 'c']\n",
    "response_ # ['after a', 'after b', 'after c']\n",
    "\n",
    "# np=1, nc=1\n",
    "prompt_, response_ = gpt.query('a')\n",
    "prompt_ # 'a'                 ['a']\n",
    "response_ # 'after a'         [['a']]\n",
    "\n",
    "# np=1, nc>1\n",
    "prompt_, response_ = gpt.query('a', n=2)\n",
    "prompt_ # 'a'                                      ['a']\n",
    "response_ # ['after a 1', 'after a 2']             [['after a 1', 'after a 2']]\n",
    "\n",
    "# np>1, nc>1\n",
    "prompt_, response_ = gpt.query(['a', 'b'], n=2)\n",
    "prompt_ # ['a', 'b']\n",
    "response_ # [['after a 1', 'after a 2'], ['after b 1', 'after b2']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:15:51.940457Z",
     "start_time": "2022-04-02T21:15:51.872228Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from jabberwocky.openai_utils import MockFunctionException, query_gpt_banana\n",
    "from jabberwocky.utils import with_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T23:20:17.253928Z",
     "start_time": "2022-04-02T23:20:17.175625Z"
    }
   },
   "outputs": [],
   "source": [
    "def squeeze(*args, n=1):\n",
    "    \"\"\"Return either the input `args` or the first item of each arg, depending\n",
    "    on our choice of n. We effectively treat n as a boolean. This is used by\n",
    "    the query_gpt_{} functions to return either a (str, dict) tuple or a \n",
    "    (list[str], list[dict]) tuple, depending on the number of completions\n",
    "    we ask for.\n",
    "    \"\"\"\n",
    "    return tuple(arg[0] for arg in args) if n == 1 else args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:41:36.621952Z",
     "start_time": "2022-04-02T21:41:36.542437Z"
    }
   },
   "outputs": [],
   "source": [
    "@valuecheck\n",
    "def wip_query_gpt_huggingface(\n",
    "        prompt, engine_i=0, temperature=1.0, repetition_penalty=None,\n",
    "        max_tokens=50, top_k=None, top_p=None, n=1, **kwargs\n",
    "):\n",
    "    \"\"\"Query EleuetherAI gpt models using the Huggingface API. This was called\n",
    "    query_gpt_neo in a former version of the library (which is used by the\n",
    "    GUI) but the API now hosts a GPT-J model as well so I renamed it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt: str\n",
    "    engine_i: int\n",
    "        Determines which Huggingface model API to query. See\n",
    "        config.C.backend_engines['huggingface'].\n",
    "        Those names refer to the number of\n",
    "        parameters in the model, where bigger models generally produce higher\n",
    "        quality results but may be slower (in addition to the actual inference\n",
    "        being slower to produce, the better models are also more popular so the\n",
    "        API is hit with more requests).\n",
    "    temperature: float\n",
    "        Between 0 and 1. 0-0.4 is good for straightforward informational\n",
    "        queries (e.g. reformatting, writing business emails) while 0.7-1 is\n",
    "        good for more creative works. Warning: huggingface docs say this\n",
    "        actually goes from 0-100 - should check if they're using this value\n",
    "        differently than the openai API.\n",
    "    top_k: None or int\n",
    "        Kind of like top_p in that smaller values may produce more\n",
    "        sensible but less creative responses. While top_p limits options to\n",
    "        a cumulative percentage, top_k limits it to a discrete number of\n",
    "        top choices.\n",
    "    top_p: None or float\n",
    "        Value in [0.0, 1.0] if provided. Kind of like temperature in that\n",
    "        smaller values may produce more sensible but less creative responses.\n",
    "    repetition_penalty\n",
    "    max_tokens: int\n",
    "        Sets max response length. One token is ~.75 words.\n",
    "    kwargs: any\n",
    "        Just lets us absorb extra kwargs when used in place of query_gpt3().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    # TODO update\n",
    "    # Currently returns List[str], List[dict] where index i in each list gives\n",
    "    # us the ith completion. We always pass in a single prompt.\n",
    "    \n",
    "    tuple or iterator: When stream=False, we return a tuple where the first\n",
    "    item is the prompt (str) and the second is the response text(str). If\n",
    "    return_full is True, a third item consisting of the whole response object\n",
    "    is returned as well. When stream=True, we return an iterator where each\n",
    "    step contains a single token. This will either be the text response alone\n",
    "    (str) or a tuple of (text, response) if return_full is True. Unlike in\n",
    "    non-streaming mode, we don't return the prompt - that seems less\n",
    "    appropriate for many time steps.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str]: Prompt, response tuple, just like query_gpt_3().\n",
    "    \"\"\"\n",
    "    if not isinstance(prompt, str):\n",
    "        raise TypeError(f'Prompt must be str, not {type(prompt)}.')\n",
    "    \n",
    "    # Hardcode backend in case we use this function outside of the \n",
    "    # GPTBackend.query wrapper.\n",
    "    engine = GPTBackend.engine(engine_i, backend='huggingface')\n",
    "\n",
    "    # Docs say we can return up to 256 tokens but API sometimes throws errors\n",
    "    # if we go above 250.\n",
    "    headers = {'Authorization':\n",
    "               f'Bearer api_{HF_API_KEY}'}\n",
    "    # Notice the names don't always align with parameter names - I wanted\n",
    "    # those to be more consistent with query_gpt3() function. Also notice\n",
    "    # that types matter: if Huggingface expects a float but gets an int, we'll\n",
    "    # get an error.\n",
    "    if repetition_penalty is not None:\n",
    "        repetition_penalty = float(repetition_penalty)\n",
    "    stop = tolist(kwargs.pop('stop', []))\n",
    "    if kwargs:\n",
    "        warnings.warn('query_gpt_huggingface received unused kwargs '\n",
    "                      f'{kwargs}.')\n",
    "\n",
    "    data = {'inputs': prompt,\n",
    "            'parameters': {'top_k': top_k, 'top_p': top_p,\n",
    "                           'temperature': float(temperature),\n",
    "                           'max_new_tokens': min(max_tokens, 250),\n",
    "                           'repetition_penalty': repetition_penalty,\n",
    "                           'return_full_text': False,\n",
    "                           'num_return_sequences': n}}\n",
    "    url = f'https://api-inference.huggingface.co/models/EleutherAI/{engine}'\n",
    "    try:\n",
    "        # Put the request itself inside try too in case of timeout.\n",
    "        r = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        r.raise_for_status()\n",
    "    except requests.HTTPError as e:\n",
    "        raise MockFunctionException(str(e)) from None\n",
    "\n",
    "    res = r.json()\n",
    "    # Structure: text, full response\n",
    "    # List[str], List[dict]\n",
    "    return [row['generated_text'] for row in res], res\n",
    "    \n",
    "\n",
    "\n",
    "#     # TODO: r is a list of dicts with only 1 key ('generated_text'). Can't\n",
    "#     # get logprobs. Experimenting with making second item in returned val be\n",
    "#     # a list of completions in case n > 1.\n",
    "\n",
    "#     # Huggingface doesn't natively provide the `stop` parameter that OpenAI\n",
    "#     # does so we have to do this manually.\n",
    "#     completions = [row['generated_text'] for row in r.json()]\n",
    "#     if stop:\n",
    "#         truncated = []\n",
    "#         for completion in completions:\n",
    "#             idx = [idx for idx in map(completion.find, stop) if idx >= 0]\n",
    "#             stop_idx = min(idx or [None])\n",
    "#             truncated.append(completion[:stop_idx])\n",
    "#         completions = truncated\n",
    "#     return prompt, completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:42:44.452238Z",
     "start_time": "2022-04-02T21:42:44.335086Z"
    }
   },
   "outputs": [],
   "source": [
    "# API still down, can't test this atm.\n",
    "def wip_query_gpt_j(prompt, temperature=0.7, max_tokens=50, **kwargs):\n",
    "    \"\"\"Queries free GPT-J API. GPT-J has 6 billion parameters and is, roughly\n",
    "    speaking, the open-source equivalent of Curie (3/19/22 update: size sounds\n",
    "    more like Babbage actually). It was trained on more\n",
    "    code than GPT3 though so it may do surprisingly well at those kinds of\n",
    "    tasks. This function should be usable as a mock_func argument in\n",
    "    query_gpt_3.\n",
    "\n",
    "    API uptime may be questionable though. There's an accompanying front end\n",
    "    here:\n",
    "    http://api.vicgalle.net:8000/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt: str\n",
    "    temperature: float\n",
    "    max_tokens: int\n",
    "    kwargs: any\n",
    "        Only supported options are top_p (float) and stop (Iterable[str]).\n",
    "        Notice that stream mode is not supported.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, dict]: Response text, full response dict.\n",
    "    \"\"\"\n",
    "    params = {'context': prompt,\n",
    "              'token_max_length': max_tokens,\n",
    "              'temperature': temperature,\n",
    "              'top_p': kwargs.pop('top_p', 1.0)}\n",
    "\n",
    "    # Ensure that we end up with a list AND that stop is still Falsy if user\n",
    "    # explicitly passes in stop=None.\n",
    "    stop = tolist(kwargs.pop('stop', None) or [])\n",
    "    if stop: params['stop_sequence'] = stop[0]\n",
    "\n",
    "    # Must keep this after the block of stop-related logic above.\n",
    "    if kwargs:\n",
    "        warnings.warn(f'GPT-J api does not support other kwargs: {kwargs}')\n",
    "\n",
    "    try:\n",
    "        res = requests.post('http://api.vicgalle.net:5000/generate',\n",
    "                            params=params)\n",
    "        res.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise MockFunctionException(str(e)) from None\n",
    "    res = res.json()\n",
    "    return res['text'], res\n",
    "\n",
    "#     # Endpoint doesn't support multiple stop sequences so we have to\n",
    "#     # postprocess. Even with a single stop sequence, it includes it while gpt3\n",
    "#     # and my gpt-neo function exclude it, so we need to handle that here.\n",
    "#     idx = min([i for i in map(res['text'].find, stop) if i >= 0] or [None])\n",
    "#     completion = res['text'][:idx]\n",
    "#     return res['prompt'], completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:55:29.822296Z",
     "start_time": "2022-04-02T21:55:29.690730Z"
    }
   },
   "outputs": [],
   "source": [
    "def wip_query_gpt3(prompt, engine_i=0, temperature=0.7, top_p=1.0,\n",
    "               frequency_penalty=0.0, presence_penalty=0.0,\n",
    "               max_tokens=50, logprobs=None, n=1, stream=False,\n",
    "               logit_bias=None, **kwargs):\n",
    "    \"\"\"Convenience function to query gpt3. Mostly serves 2 purposes:\n",
    "    1. Build in some mocking functionality for cheaper/free testing.\n",
    "    2. Explicitly add some parameters and descriptions to the function\n",
    "    docstring, since openai.Completion.create does not include most kwargs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt: str\n",
    "    engine_i: int\n",
    "        Corresponds to engines defined in config, where 0 is the cheapest, 3\n",
    "        is the most expensive, etc.\n",
    "    temperature: float\n",
    "        Between 0 and 1. 0-0.4 is good for straightforward informational\n",
    "        queries (e.g. reformatting, writing business emails) while 0.7-1 is\n",
    "        good for more creative works.\n",
    "    top_p: float\n",
    "        Value in (0.0, 1.0] that limits the model to sample from tokens making\n",
    "        up the top_p percent combined. I.e. higher values allow for more\n",
    "        creativity (like high temperature) and low values are closer to argmax\n",
    "        sampling (like low temperature). API recommends setting a sub-maximal\n",
    "        value for at most one of this and temperature, not both.\n",
    "    frequency_penalty: float\n",
    "        Value in [-2.0, 2.0] where larger (more positive) values more heavily\n",
    "        penalize words that have already occurred frequently in the text.\n",
    "        Usually reasonable to keep this in [0, 1].\n",
    "    presence_penalty: float\n",
    "        Value in [-2.0, 2.0] where larger (more positive) values more heavily\n",
    "        penalize words that have already occurred in the text. Usually\n",
    "        reasonable to keep this in [0, 1].\n",
    "    max_tokens: int\n",
    "        Sets max response length. One token is ~.75 words.\n",
    "    logprobs: int or None\n",
    "        Get log probabilities for top n candidates at each time step. This\n",
    "        will only be useful if you set return_full=True.\n",
    "    n: int\n",
    "        Number of possible completions to return. Careful: values > 1 can add\n",
    "        up quickly w.r.t. cost.\n",
    "    stream: bool\n",
    "        If True, return an iterator instead of a str/tuple. See the returns\n",
    "        section as the output is slightly different. I believe each chunk\n",
    "        returns one token when stream is True.\n",
    "    logit_bias: dict or None\n",
    "        If provided, should map string(s) (NUMERIC INDEX of word tokens,\n",
    "        not the tokens themselves) to ints between -100 and 100 (inclusive?).\n",
    "        Values in (-1, 1) should be used to nudge the model, while larger\n",
    "        values can effectively ban/compel the model to use certain words.\n",
    "    kwargs: any\n",
    "        Additional kwargs to pass to gpt3. Should rarely be necessary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple or iterator: When stream=False, we return a tuple where the first\n",
    "    item is the prompt (str) and the second is the response text(str). If\n",
    "    return_full is True, a third item consisting of the whole response object\n",
    "    is returned as well. When stream=True, we return an iterator where each\n",
    "    step contains a single token. This will either be the text response alone\n",
    "    (str) or a tuple of (text, response) if return_full is True. Unlike in\n",
    "    non-streaming mode, we don't return the prompt - that seems less\n",
    "    appropriate for many time steps.\n",
    "    \"\"\"\n",
    "    if not isinstance(prompt, str):\n",
    "        raise TypeError(f'Prompt must be str, not {type(prompt)}.')\n",
    "    if stream and n > 1:\n",
    "        raise RuntimeError('Stream=True and n>1 not supported.')\n",
    "        \n",
    "#     if stream and strip_output:\n",
    "#         warnings.warn('strip_output is automatically set to False when stream '\n",
    "#                       'is True. It would be impossible to correctly '\n",
    "#                       'reconstruct outputs otherwise.')\n",
    "    if temperature < 1 and top_p < 1:\n",
    "        warnings.warn('You set both temperature and top_p to values < 1. '\n",
    "                      'API recommends setting only one of these to '\n",
    "                      'sub-maximal value.')\n",
    "#     if logprobs and not return_full:\n",
    "#         warnings.warn('You set logprobs to a nonzero value but '\n",
    "#                       'return_full=False. If you want to access the logprobs, '\n",
    "#                       'you should set return_full=True.')\n",
    "\n",
    "    res = openai.Completion.create(\n",
    "        engine=GPTBackend.engine(engine_i, 'openai'),\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        max_tokens=max_tokens,\n",
    "        logprobs=logprobs,\n",
    "        n=n,\n",
    "        stream=stream,\n",
    "        logit_bias=logit_bias or {},\n",
    "        **kwargs\n",
    "    )\n",
    "        \n",
    "    # Extract text and return. Zip maintains lazy evaluation.\n",
    "    if stream:\n",
    "        # Each item in zipped object is (str, dict-like).\n",
    "        texts = (chunk.choices[0].text for chunk in res)\n",
    "        chunks = (dict(chunk.choices[0]) for chunk in res)\n",
    "        return zip(texts, chunks)\n",
    "    \n",
    "    # Structure: (List[str], List[dict-like])\n",
    "    return [row.text for row in res.choices], \\\n",
    "        [dict(choice) for choice in res.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:45:05.870470Z",
     "start_time": "2022-04-02T21:45:05.778256Z"
    }
   },
   "outputs": [],
   "source": [
    "@with_signature(wip_query_gpt3, keep=True)\n",
    "@mark(requires_stopword_truncation=True)\n",
    "def wip_query_gpt_goose(prompt, **kwargs):\n",
    "    return wip_query_gpt3(prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:41:37.524726Z",
     "start_time": "2022-04-02T21:41:37.462546Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Who is the', \n",
    "    ('I am a', 'You are a'),\n",
    "]\n",
    "ns = [1, 3]\n",
    "kwargs = DotDict(\n",
    "    zip(\n",
    "        ['single_both', 'multi_out', 'multi_in', 'multi_both'],\n",
    "        [{'prompt': p, 'n': n, 'max_tokens': 5} \n",
    "         for p in prompts for n in ns]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:41:39.728019Z",
     "start_time": "2022-04-02T21:41:38.424743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_in Prompt must be str, not <class 'tuple'>.\n",
      "multi_both Prompt must be str, not <class 'tuple'>.\n"
     ]
    }
   ],
   "source": [
    "hf = DotDict()\n",
    "for k, v in tqdm(kwargs.items()):\n",
    "    try:\n",
    "        hf[k] = wip_query_gpt_huggingface(**v, engine_i=1) \n",
    "    except Exception as e:\n",
    "        print(k, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:41:43.655631Z",
     "start_time": "2022-04-02T21:41:43.563094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' President? When is the'], [{'generated_text': ' President? When is the'}])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.single_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:41:45.786012Z",
     "start_time": "2022-04-02T21:41:45.698228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' real “Man of', ' most memorable character from the', ' real Trump? (and'],\n",
       " [{'generated_text': ' real “Man of'},\n",
       "  {'generated_text': ' most memorable character from the'},\n",
       "  {'generated_text': ' real Trump? (and'}])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.multi_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:54:21.802121Z",
     "start_time": "2022-04-02T21:54:21.372004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"openai\".\n",
      " most\n",
      "{'text': ' important', 'index': 0, 'logprobs': <OpenAIObject at 0x1706c01a8> JSON: {\n",
      "  \"text_offset\": [\n",
      "    15\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.2002852\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" important\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" important\": -2.2002852,\n",
      "      \" likely\": -2.7987826,\n",
      "      \" popular\": -3.2215893\n",
      "    }\n",
      "  ]\n",
      "}, 'finish_reason': None}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      " player\n",
      "{'text': ' for', 'index': 0, 'logprobs': <OpenAIObject at 0x16fe24728> JSON: {\n",
      "  \"text_offset\": [\n",
      "    32\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.2986562\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" for\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" for\": -2.2986562,\n",
      "      \" in\": -0.9703081,\n",
      "      \" on\": -1.8214434\n",
      "    }\n",
      "  ]\n",
      "}, 'finish_reason': None}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"gooseai\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('openai'):\n",
    "    for tok, data in wip_query_gpt3(**kwargs.single_both, logprobs=3, \n",
    "                                    engine_i=0, stream=True):\n",
    "        print(tok)\n",
    "        print(data)\n",
    "        print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:51:54.874720Z",
     "start_time": "2022-04-02T21:51:54.395152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"openai\".\n",
      "Switching  backend back to \"gooseai\".\n"
     ]
    }
   ],
   "source": [
    "g = DotDict()\n",
    "with gpt('openai'):\n",
    "    g['single_both'] = wip_query_gpt3(**kwargs.single_both, logprobs=3, engine_i=0)\n",
    "    g['multi_out'] = wip_query_gpt3(**kwargs.multi_out, logprobs=3, engine_i=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:51:55.733777Z",
     "start_time": "2022-04-02T21:51:55.677216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' President of the United States'],\n",
       " [{'text': ' President of the United States',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x1706c0308> JSON: {\n",
       "     \"text_offset\": [\n",
       "       10,\n",
       "       20,\n",
       "       23,\n",
       "       27,\n",
       "       34\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -4.364347,\n",
       "       -0.7310643,\n",
       "       -0.6073798,\n",
       "       -1.8252875,\n",
       "       -0.011561717\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" President\",\n",
       "       \" of\",\n",
       "       \" the\",\n",
       "       \" United\",\n",
       "       \" States\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" author\": -3.6692274,\n",
       "         \" best\": -3.04953,\n",
       "         \" most\": -3.2980042\n",
       "       },\n",
       "       {\n",
       "         \" of\": -0.7310643,\n",
       "         \"?\": -1.7396235,\n",
       "         \"?\\\"\": -2.611957\n",
       "       },\n",
       "       {\n",
       "         \" The\": -4.221831,\n",
       "         \" the\": -0.6073798,\n",
       "         \" this\": -4.6789083\n",
       "       },\n",
       "       {\n",
       "         \" Board\": -3.5698235,\n",
       "         \" United\": -1.8252875,\n",
       "         \" University\": -3.6078427\n",
       "       },\n",
       "       {\n",
       "         \" Kingdom\": -5.8680987,\n",
       "         \" Nations\": -6.1417484,\n",
       "         \" States\": -0.011561717\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'finish_reason': 'length'}])"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.single_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T21:53:12.484545Z",
     "start_time": "2022-04-02T21:53:12.407682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<OpenAIObject at 0x1706c0468> JSON: {\n",
       "    \" author\": -3.6692274,\n",
       "    \" best\": -3.04953,\n",
       "    \" most\": -3.2980042\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c0570> JSON: {\n",
       "    \" of\": -0.39385706,\n",
       "    \"?\": -1.6094236,\n",
       "    \"?\\\"\": -3.9906347\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c05c8> JSON: {\n",
       "    \" \\\"\": -3.2908247,\n",
       "    \" the\": -1.431546,\n",
       "    \" this\": -0.88533527\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c06d0> JSON: {\n",
       "    \" article\": -1.0468491,\n",
       "    \" book\": -2.1007652,\n",
       "    \" post\": -3.169608\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c0728> JSON: {\n",
       "    \" and\": -3.2372837,\n",
       "    \",\": -4.04183,\n",
       "    \"?\": -0.15100385\n",
       "  }],\n",
       " [<OpenAIObject at 0x1706c0830> JSON: {\n",
       "    \" author\": -3.6692274,\n",
       "    \" best\": -3.04953,\n",
       "    \" most\": -3.2980042\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c0888> JSON: {\n",
       "    \" character\": -0.8551719,\n",
       "    \" protagonist\": -3.3901486,\n",
       "    \" villain\": -3.3227801\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c08e0> JSON: {\n",
       "    \" in\": -1.4130918,\n",
       "    \" of\": -1.8321267,\n",
       "    \"?\": -1.2511454\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c0938> JSON: {\n",
       "    \" the\": -1.1516712,\n",
       "    \" this\": -1.1880902,\n",
       "    \" your\": -3.5508714\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c0990> JSON: {\n",
       "    \" book\": -2.2914226,\n",
       "    \" game\": -2.1019979,\n",
       "    \" story\": -1.7118553\n",
       "  }],\n",
       " [<OpenAIObject at 0x1706c0a98> JSON: {\n",
       "    \" author\": -3.6692274,\n",
       "    \" best\": -3.04953,\n",
       "    \" most\": -3.2980042\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c0af0> JSON: {\n",
       "    \" man\": -3.162459,\n",
       "    \" one\": -1.8821301,\n",
       "    \" person\": -1.5477071\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c0b48> JSON: {\n",
       "    \" in\": -2.444486,\n",
       "    \" that\": -2.2740557,\n",
       "    \" who\": -1.2757761\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c0ba0> JSON: {\n",
       "    \" can\": -1.1256156,\n",
       "    \" has\": -2.679357,\n",
       "    \" knows\": -2.835012\n",
       "  },\n",
       "  <OpenAIObject at 0x1706c0bf8> JSON: {\n",
       "    \" do\": -2.6999028,\n",
       "    \" save\": -2.6591103,\n",
       "    \" stop\": -2.9301693\n",
       "  }]]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row['logprobs']['top_logprobs'] for row in g.multi_out[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T03:28:44.231790Z",
     "start_time": "2022-04-01T03:28:40.602179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Horologist',\n",
       " \"\\n\\nA horologist is a watchmaker or watchmaker's apprentice who is trained in horology, the study of watches.  The horologist is responsible for repairing watches, making and maintaining a watchmaker's tools, and for maintaining the quality\")"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gpt_banana('Horologist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T03:18:43.921542Z",
     "start_time": "2022-04-01T03:18:43.873064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_ = load(C.mock_stream_paths[True])\n",
    "len(tmp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T03:19:16.194691Z",
     "start_time": "2022-04-01T03:19:16.135404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PROMPT> make'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_[0].choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T03:19:19.747063Z",
     "start_time": "2022-04-01T03:19:19.716284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_[1].choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:19:40.810273Z",
     "start_time": "2022-03-31T03:19:40.299205Z"
    }
   },
   "outputs": [],
   "source": [
    "res30 = wip_query_gpt_huggingface('Who is the',\n",
    "                                  n=2,\n",
    "                                  engine_i=1,\n",
    "                                  max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:19:45.586889Z",
     "start_time": "2022-03-31T03:19:45.542296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Who is the',\n",
       " [' most dangerous cat to dogcat people?\\n\\n',\n",
       "  ' true heir of David, the Goliath of G'])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:53:13.036211Z",
     "start_time": "2022-03-31T03:53:12.613031Z"
    }
   },
   "outputs": [],
   "source": [
    "res30_arr = wip_query_gpt_huggingface(\n",
    "    ['Who is the', 'I am the'],\n",
    "    n=1,\n",
    "    engine_i=1,\n",
    "    max_tokens=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:53:34.854748Z",
     "start_time": "2022-03-31T03:53:34.817842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': ' Father?\\n \\n'}],\n",
       " [{'generated_text': ' head of a local chapter'}]]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res30_arr.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T03:09:34.008302Z",
     "start_time": "2022-04-02T03:09:33.953946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': ' Father?\\n \\n'}],\n",
       " [{'generated_text': ' head of a local chapter'}]]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res30_arr.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:53:59.226642Z",
     "start_time": "2022-03-31T03:53:56.912228Z"
    }
   },
   "outputs": [],
   "source": [
    "res30_multi = wip_query_gpt_huggingface(\n",
    "    ['Who is the', 'I am the'],\n",
    "    n=2,\n",
    "    engine_i=1,\n",
    "    max_tokens=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:54:00.528670Z",
     "start_time": "2022-03-31T03:54:00.462428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': ' most evil person of the'},\n",
       "  {'generated_text': ' most qualified on these forums'}],\n",
       " [{'generated_text': ' owner, or an authorized'},\n",
       "  {'generated_text': ' author of the books \"'}]]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res30_multi.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gptj (broken)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T03:10:55.484226Z",
     "start_time": "2022-04-02T03:10:55.246242Z"
    }
   },
   "outputs": [
    {
     "ename": "MockFunctionException",
     "evalue": "HTTPConnectionPool(host='api.vicgalle.net', port=5000): Max retries exceeded with url: /generate?context=Who+is+the&token_max_length=10&temperature=0.7&top_p=1.0 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x128705e48>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMockFunctionException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-16175a7919ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m res30_j = wip_query_gpt_j(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'Who is the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-163-da1867e437ea>\u001b[0m in \u001b[0;36mwip_query_gpt_j\u001b[0;34m(prompt, temperature, max_tokens, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMockFunctionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMockFunctionException\u001b[0m: HTTPConnectionPool(host='api.vicgalle.net', port=5000): Max retries exceeded with url: /generate?context=Who+is+the&token_max_length=10&temperature=0.7&top_p=1.0 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x128705e48>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "res30_j = wip_query_gpt_j(\n",
    "    'Who is the', max_tokens=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:35:23.390099Z",
     "start_time": "2022-03-31T03:35:22.474089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:109: UserWarning: You set logprobs to a nonzero value but return_full=False. If you want to access the logprobs, you should set return_full=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching  backend back to \"gooseai\".\n"
     ]
    }
   ],
   "source": [
    "# res30_o for date, openai. \n",
    "# Seeing what response obj looks like w/ >1 completions.\n",
    "with gpt('gooseai'):\n",
    "    res30_o = wip_query_gpt3(\n",
    "        'Who is the', max_tokens=10, engine_i=0, n=2, logprobs=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:35:24.772999Z",
     "start_time": "2022-03-31T03:35:24.743091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res30_o['choices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:36:14.219920Z",
     "start_time": "2022-03-31T03:36:14.178820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n",
      " real winner in the new law?\n",
      "\n",
      "The\n",
      "[<OpenAIObject at 0x12891be60> JSON: {\n",
      "  \" best\": -2.83984375,\n",
      "  \" first\": -3.68359375,\n",
      "  \" man\": -4.12890625,\n",
      "  \" most\": -3.193359375,\n",
      "  \" real\": -3.591796875\n",
      "}, <OpenAIObject at 0x12891b048> JSON: {\n",
      "  \" culprit\": -4.57421875,\n",
      "  \" enemy\": -4.1796875,\n",
      "  \" victim\": -4.39453125,\n",
      "  \" winner\": -3.578125,\n",
      "  \" \\ufffd\": -4.46875\n",
      "}, <OpenAIObject at 0x12891beb8> JSON: {\n",
      "  \" and\": -3.1484375,\n",
      "  \" here\": -1.97265625,\n",
      "  \" in\": -1.162109375,\n",
      "  \" of\": -1.4462890625,\n",
      "  \"?\": -2.232421875\n",
      "}, <OpenAIObject at 0x12891be08> JSON: {\n",
      "  \" a\": -4.12890625,\n",
      "  \" all\": -2.935546875,\n",
      "  \" the\": -0.90625,\n",
      "  \" these\": -4.71484375,\n",
      "  \" this\": -1.40234375\n",
      "}, <OpenAIObject at 0x12891bf10> JSON: {\n",
      "  \" Brexit\": -3.87109375,\n",
      "  \" US\": -4.32421875,\n",
      "  \" current\": -4.07421875,\n",
      "  \" new\": -4.39453125,\n",
      "  \" war\": -4.07421875\n",
      "}, <OpenAIObject at 0x12891bf68> JSON: {\n",
      "  \" U\": -4.31640625,\n",
      "  \" US\": -4.26953125,\n",
      "  \" tax\": -3.32421875,\n",
      "  \" world\": -3.41015625,\n",
      "  \" \\ufffd\": -4.1328125\n",
      "}, <OpenAIObject at 0x12891bfc0> JSON: {\n",
      "  \" allowing\": -3.716796875,\n",
      "  \" on\": -3.09375,\n",
      "  \" that\": -2.787109375,\n",
      "  \",\": -3.16796875,\n",
      "  \"?\": -0.57470703125\n",
      "}, <OpenAIObject at 0x12892f048> JSON: {\n",
      "  \" A\": -4.31640625,\n",
      "  \" Is\": -4.02734375,\n",
      "  \" It\": -3.6796875,\n",
      "  \" The\": -2.779296875,\n",
      "  \"bytes:'\\\\n'\": -0.78369140625\n",
      "}, <OpenAIObject at 0x12892f0a0> JSON: {\n",
      "  \"================================\": -7.7578125,\n",
      "  \"A\": -7.546875,\n",
      "  \"In\": -7.7421875,\n",
      "  \"The\": -6.03125,\n",
      "  \"bytes:'\\\\n'\": -0.019256591796875\n",
      "}, <OpenAIObject at 0x12892f0f8> JSON: {\n",
      "  \"A\": -3.4765625,\n",
      "  \"By\": -3.306640625,\n",
      "  \"I\": -3.578125,\n",
      "  \"In\": -3.453125,\n",
      "  \"The\": -2.103515625\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "length\n",
      " biggest problem for the NHL? The answer is the\n",
      "[<OpenAIObject at 0x12892f200> JSON: {\n",
      "  \" best\": -2.83984375,\n",
      "  \" first\": -3.68359375,\n",
      "  \" man\": -4.12890625,\n",
      "  \" most\": -3.193359375,\n",
      "  \" real\": -3.591796875\n",
      "}, <OpenAIObject at 0x12892f258> JSON: {\n",
      "  \" fan\": -4.0859375,\n",
      "  \" loser\": -3.056640625,\n",
      "  \" problem\": -3.90625,\n",
      "  \" threat\": -2.283203125,\n",
      "  \" winner\": -4.05859375\n",
      "}, <OpenAIObject at 0x12892f2b0> JSON: {\n",
      "  \" facing\": -2.07421875,\n",
      "  \" for\": -2.91015625,\n",
      "  \" in\": -1.083984375,\n",
      "  \" with\": -1.5439453125,\n",
      "  \"?\": -3.513671875\n",
      "}, <OpenAIObject at 0x12892f308> JSON: {\n",
      "  \" our\": -4.28515625,\n",
      "  \" the\": -1.1455078125,\n",
      "  \" women\": -4.0859375,\n",
      "  \" you\": -3.412109375,\n",
      "  \" your\": -3.900390625\n",
      "}, <OpenAIObject at 0x12892f360> JSON: {\n",
      "  \" NFL\": -3.78515625,\n",
      "  \" U\": -3.857421875,\n",
      "  \" US\": -3.955078125,\n",
      "  \" United\": -3.919921875,\n",
      "  \" world\": -4.27734375\n",
      "}, <OpenAIObject at 0x12892f3b8> JSON: {\n",
      "  \" and\": -2.599609375,\n",
      "  \" in\": -2.21484375,\n",
      "  \" right\": -3.1484375,\n",
      "  \"'s\": -3.134765625,\n",
      "  \"?\": -0.9814453125\n",
      "}, <OpenAIObject at 0x12892f410> JSON: {\n",
      "  \" Is\": -2.517578125,\n",
      "  \" It\": -3.3984375,\n",
      "  \" The\": -2.546875,\n",
      "  \" Why\": -4.25390625,\n",
      "  \"bytes:'\\\\n'\": -0.97216796875\n",
      "}, <OpenAIObject at 0x12892f468> JSON: {\n",
      "  \" NHL\": -3.53125,\n",
      "  \" answer\": -1.736328125,\n",
      "  \" biggest\": -3.9140625,\n",
      "  \" league\": -4.16015625,\n",
      "  \" players\": -3.578125\n",
      "}, <OpenAIObject at 0x12892f4c0> JSON: {\n",
      "  \" is\": -0.7421875,\n",
      "  \" may\": -3.22265625,\n",
      "  \" might\": -3.42578125,\n",
      "  \" to\": -2.4140625,\n",
      "  \",\": -2.412109375\n",
      "}, <OpenAIObject at 0x12892f518> JSON: {\n",
      "  \" not\": -3.09375,\n",
      "  \" obvious\": -2.599609375,\n",
      "  \" probably\": -2.666015625,\n",
      "  \" simple\": -1.76171875,\n",
      "  \" the\": -3.021484375\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in res30_o['choices']:\n",
    "    print(row.finish_reason)\n",
    "    print(row.text)\n",
    "    print(row.logprobs.top_logprobs)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:40:11.783855Z",
     "start_time": "2022-03-31T03:40:10.984642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:109: UserWarning: You set logprobs to a nonzero value but return_full=False. If you want to access the logprobs, you should set return_full=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching  backend back to \"gooseai\".\n"
     ]
    }
   ],
   "source": [
    "# arr refers to passing in array of prompts.\n",
    "# Seeing what response obj looks like w/ >1 completions.\n",
    "with gpt('gooseai'):\n",
    "    res30_arr = wip_query_gpt3(\n",
    "        ['The', 'The quick', 'The quick brown', 'The quick brown fox'],\n",
    "        max_tokens=1, engine_i=0, n=1, logprobs=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:40:35.698160Z",
     "start_time": "2022-03-31T03:40:35.649728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res30_arr.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:41:10.640392Z",
     "start_time": "2022-03-31T03:41:10.597203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n",
      " invention\n",
      "[<OpenAIObject at 0x12892f728> JSON: {\n",
      "  \" authors\": -3.5859375,\n",
      "  \" content\": -4.5625,\n",
      "  \" following\": -4.3984375,\n",
      "  \" invention\": -3.349609375,\n",
      "  \" present\": -2.2734375\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "length\n",
      " brown\n",
      "[<OpenAIObject at 0x12892f938> JSON: {\n",
      "  \" and\": -2.09375,\n",
      "  \" answer\": -2.45703125,\n",
      "  \" brown\": -3.111328125,\n",
      "  \",\": -3.083984375,\n",
      "  \"-\": -2.646484375\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "length\n",
      " fox\n",
      "[<OpenAIObject at 0x12892fb48> JSON: {\n",
      "  \" (\": -5.3203125,\n",
      "  \" Fox\": -4.2734375,\n",
      "  \" and\": -5.39453125,\n",
      "  \" f\": -5.171875,\n",
      "  \" fox\": -0.1324462890625\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "length\n",
      " jumped\n",
      "[<OpenAIObject at 0x12892fc50> JSON: {\n",
      "  \" is\": -4.96484375,\n",
      "  \" jumped\": -1.75,\n",
      "  \" jumps\": -0.259033203125,\n",
      "  \",\": -5.37890625,\n",
      "  \"bytes:'\\\\n'\": -5.57421875\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in res30_arr.choices:\n",
    "    print(row.finish_reason)\n",
    "    print(row.text)\n",
    "    print(row.logprobs.top_logprobs)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mutiple prompts AND multiple completions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:42:57.702445Z",
     "start_time": "2022-03-31T03:42:57.042868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:109: UserWarning: You set logprobs to a nonzero value but return_full=False. If you want to access the logprobs, you should set return_full=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching  backend back to \"gooseai\".\n"
     ]
    }
   ],
   "source": [
    "# arr refers to passing in array of prompts.\n",
    "# Seeing what response obj looks like w/ >1 completions.\n",
    "with gpt('gooseai'):\n",
    "    res30_multi = wip_query_gpt3(\n",
    "        ['The', 'The quick', 'The quick brown'],\n",
    "        max_tokens=1, engine_i=0, n=2, logprobs=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:43:05.540931Z",
     "start_time": "2022-03-31T03:43:05.502398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res30_multi.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:43:11.499915Z",
     "start_time": "2022-03-31T03:43:11.465177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n",
      " present\n",
      "[<OpenAIObject at 0x128566410> JSON: {\n",
      "  \" authors\": -3.5859375,\n",
      "  \" content\": -4.5625,\n",
      "  \" following\": -4.3984375,\n",
      "  \" invention\": -3.349609375,\n",
      "  \" present\": -2.2734375\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "length\n",
      " present\n",
      "[<OpenAIObject at 0x1285664c0> JSON: {\n",
      "  \" authors\": -3.5859375,\n",
      "  \" content\": -4.5625,\n",
      "  \" following\": -4.3984375,\n",
      "  \" invention\": -3.349609375,\n",
      "  \" present\": -2.2734375\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "length\n",
      " and\n",
      "[<OpenAIObject at 0x128566150> JSON: {\n",
      "  \" and\": -2.09375,\n",
      "  \" answer\": -2.45703125,\n",
      "  \" brown\": -3.111328125,\n",
      "  \",\": -3.083984375,\n",
      "  \"-\": -2.646484375\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "length\n",
      ",\n",
      "[<OpenAIObject at 0x128566570> JSON: {\n",
      "  \" and\": -2.09375,\n",
      "  \" answer\": -2.45703125,\n",
      "  \" brown\": -3.111328125,\n",
      "  \",\": -3.083984375,\n",
      "  \"-\": -2.646484375\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "length\n",
      " fox\n",
      "[<OpenAIObject at 0x128566678> JSON: {\n",
      "  \" (\": -5.3203125,\n",
      "  \" Fox\": -4.2734375,\n",
      "  \" and\": -5.39453125,\n",
      "  \" f\": -5.171875,\n",
      "  \" fox\": -0.1324462890625\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "length\n",
      " fox\n",
      "[<OpenAIObject at 0x128566780> JSON: {\n",
      "  \" (\": -5.3203125,\n",
      "  \" Fox\": -4.2734375,\n",
      "  \" and\": -5.39453125,\n",
      "  \" f\": -5.171875,\n",
      "  \" fox\": -0.1324462890625\n",
      "}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looks like outer loop is determined by n prompts and inner loops is\n",
    "# determined by n completions.\n",
    "for row in res30_multi.choices:\n",
    "    print(row.finish_reason)\n",
    "    print(row.text)\n",
    "    print(row.logprobs.top_logprobs)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T03:46:58.853722Z",
     "start_time": "2022-03-31T03:46:58.803492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' present', ' present'], [' and', ','], [' fox', ' fox']]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_prompts = 3\n",
    "n_completions = 2\n",
    "[[row.text for row in res30_multi.choices[i:i + n_completions]]\n",
    " for i in range(0, n_prompts * n_completions, n_completions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
