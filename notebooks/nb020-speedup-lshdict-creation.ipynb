{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "See if we can speed up lshdict instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T02:52:40.365078Z",
     "start_time": "2022-06-18T02:52:40.349755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:40:04.025407Z",
     "start_time": "2022-06-18T03:40:03.950063Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.openai_utils import load_prompt, load_openai_api_key\n",
    "from jabberwocky.utils import ReturningThread\n",
    "from htools import *\n",
    "from htools.structures import _FuzzyDictBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T02:53:33.198550Z",
     "start_time": "2022-06-18T02:53:33.154094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:44:30.470413Z",
     "start_time": "2022-06-18T03:44:30.403428Z"
    }
   },
   "outputs": [],
   "source": [
    "class NewLSHDict(_FuzzyDictBase):\n",
    "    \"\"\"Dictionary that returns the value corresponding to a key's nearest\n",
    "    neighbor if the key isn't present in the dict. This is intended for use\n",
    "    as a word2index dict when using embeddings in deep learning: e.g. if we\n",
    "    have domain embeddings for the top 100k websites, some of our options for\n",
    "    dealing with unknown domains are:\n",
    "\n",
    "    1. Encode all of them as <UNK>. This loses a lot of information.\n",
    "    2. Create a FuzzyKeyDict which will search for similar keys using variants\n",
    "    of Levenshtein distance. Lookup is O(N) and for 100k domains, that comes\n",
    "    out to 0.6 seconds per item. We might have thousands or millions of\n",
    "    lookups over the course of training so this can be a significant cost.\n",
    "    3. Create an LSHDict (lookups are O(1)). Indexing into the dict as usual\n",
    "    (e.g. my_lsh_dict[key]) will provide the key's index if present and the\n",
    "    (approximate) nearest neighbor's index otherwise. Either way, the result\n",
    "    can be used to index into your embedding layer.\n",
    "    4. Create an LSHDict and use the `similar_values` method to return n>1\n",
    "    neighbors. Then pass their indices to an Embedding layer and\n",
    "    compute the sum/average/weighted average of the results. This may be\n",
    "    preferable to #3 cases such as web domain lookup, where similar URLs are\n",
    "    not guaranteed to represent similar sites. (This is basically\n",
    "    equivalent to an EmbeddingBag layer, but in torch that doesn't store\n",
    "    intermediate representations so we wouldn't be able to use our pretrained\n",
    "    embeddings.)\n",
    "\n",
    "    LSHDict does NOT support pickling as of version 6.0.6 (note: setitem seems\n",
    "    to be called before init when unpickling, meaning we try to access\n",
    "    self.forest in self._update_forest before it's been defined. Even if we\n",
    "    change setitem so reindexing does not occur by default, it still tries to\n",
    "    hash the new word and add it to the forest so unpickling will still fail).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, n_candidates=None, n_keys=3, ngram_size=3,\n",
    "                 scorer=fuzz.ratio, chunksize=100):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: dict or list[tuple]\n",
    "            The base dictionary. Unlike FuzzyKeyDict, we require this since\n",
    "            adding items one by one is computationally infeasible for large\n",
    "            datasets. Just build up your dictionary first.\n",
    "        n_candidates: int or None\n",
    "            Number of reasonably similar keys to retrieve when trying to index\n",
    "            in with a key that's missing (or when using the `similar` method).\n",
    "            You can override this in `similar` but not when using\n",
    "            __getitem__'s square bracket syntax. If not specified, this will\n",
    "            be auto initialized to vocab size/1,000, clipped to lie in\n",
    "            [20, 500]. See `similar` docstring for more on this.\n",
    "        n_keys: int\n",
    "            Default number of similar keys to retrieve in `similar`.\n",
    "        scorer: function\n",
    "            Default scoring function to use to narrow `n_candidates` keys down\n",
    "            to `n_keys`. Should be a fuzzywuzzy function where scores lie in\n",
    "            [0, 100] and higher values indicate high similarity.\n",
    "        \"\"\"\n",
    "        if len(data) < 10_000 and len(next(iter(data))) < 100:\n",
    "            warnings.warn(\n",
    "                'It looks like you\\'re working with a relatively small '\n",
    "                'amount of data. FuzzyKeyDict may be fast enough for your '\n",
    "                'use case and would provide the set of strictly most similar '\n",
    "                'keys rather than an approximation of that set.'\n",
    "            )\n",
    "\n",
    "        super().__init__(data)\n",
    "        self.scorer = scorer\n",
    "        self.hash_word = partial(self.lsh_hash_word, n=ngram_size)\n",
    "        self.forest = MinHashLSHForest(num_perm=128)\n",
    "        self.chunksize = chunksize\n",
    "        self._initialize_forest()\n",
    "\n",
    "        # Datasketch's LSH implementation usually gives pretty decent results\n",
    "        # even with numbers as low as 5-10, but increasing that by a factor of\n",
    "        # 10 comes with minimal time cost: Fuzzywuzzy matching doesn't get\n",
    "        # particularly slow until we get into the thousands. The fact that\n",
    "        # we cap this at 500 makes this lookup asymptotically O(1) while\n",
    "        # FuzzyKeyDict's is O(N).\n",
    "        self.n_candidates = n_candidates or np.clip(len(self) // 1_000,\n",
    "                                                    20, 500)\n",
    "        self.n_keys = n_keys\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        \"\"\"Try to add keys all at once in the constructor because adding new\n",
    "        keys can be extremely slow.\n",
    "        \"\"\"\n",
    "        super().__setitem__(key, val)\n",
    "        self._update_forest(key)\n",
    "\n",
    "    def _update_forest(self, key, index=True):\n",
    "        \"\"\"Used in __setitem__ to update our LSH Forest. Forest's index method\n",
    "        seems to recompute everything so adding items to a large LSHDict will\n",
    "        be incredibly slow. Luckily, our deep learning use case rarely/never\n",
    "        requires us to update object2index dicts after instantiation so that's\n",
    "        not as troubling as it might seem.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key: str\n",
    "        index: bool\n",
    "            If True, reindex the forest (essentially making the key\n",
    "            queryable). This should be False when initializing the forest so\n",
    "            we just index once after everything's been added.\n",
    "        \"\"\"\n",
    "        self.forest.add(key, self.hash_word(key))\n",
    "        if index: self.forest.index()\n",
    "\n",
    "    def _initialize_forest(self):\n",
    "        \"\"\"Called once in __init__ to add all items to LSH Forest. This is\n",
    "        necessary because dict specifically calls its own __setitem__, not\n",
    "        its children's.\n",
    "        \"\"\"\n",
    "#         threads = [ReturningThread(target=self.hash_word, args=(key,)) \n",
    "#                    for key in self.keys()]\n",
    "#         for thread in threads: thread.start()\n",
    "#         hashes = [thread.join() for thread in threads]\n",
    "        hashes = parallelize(self.hash_word, self.keys(), total=len(self),\n",
    "                             chunksize=self.chunksize)\n",
    "        for hash_, key in zip(hashes, self.keys()):\n",
    "            self.forest.add(key, hash_)\n",
    "        self.forest.index()\n",
    "\n",
    "    @add_docstring(_FuzzyDictBase._filter_similarity_pairs)\n",
    "    def similar(self, key, mode='keys_values', n_candidates=None,\n",
    "                n_keys=None, scorer=None):\n",
    "        \"\"\"Find a list of similar keys. This is used in __getitem__ but can\n",
    "        also be useful as a user-facing method if you want to get more than\n",
    "        1 neighbor or you want to get similarity scores as well.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key: str\n",
    "            Word/URL/etc. to find similar keys to.\n",
    "        mode: str\n",
    "            See section below `Returns`.\n",
    "        n_candidates: int or None\n",
    "            Number of similar candidates to retrieve. This uses Jaccard\n",
    "            Similarity which isn't always a great metric for string\n",
    "            similarity. This is also where the LSH comes in so they're not\n",
    "            strictly the n best candidates, but rather a close approximation\n",
    "            of that set. If None, this will fall back to self.n_candidates.\n",
    "            Keep in mind this determines how many keys to\n",
    "        n_keys: int or None\n",
    "            Number of similar keys to return. If None, this will fall back to\n",
    "            self.n_keys.\n",
    "        scorer: function or None\n",
    "            Fuzzywuzzy scoring function, e.g. fuzz.ratio or\n",
    "            fuzz.partial_ratio, which will be used to score each candidate and\n",
    "            select which to return. Higher scores indicate higher levels of\n",
    "            similarity. If None, this will fall back to self.scorer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list: List if `mode` is \"keys\" or \"values\". List of tuples otherwise.\n",
    "        \"\"\"\n",
    "        candidates = self.forest.query(self.hash_word(key),\n",
    "                                       n_candidates or self.n_candidates)\n",
    "        if not candidates: raise KeyError('No similar keys found.')\n",
    "\n",
    "        # List of (key, score) where higher means more similar.\n",
    "        pairs = process.extract(key, candidates,\n",
    "                                limit=n_keys or self.n_keys,\n",
    "                                scorer=scorer or self.scorer)\n",
    "        return self._filter_similarity_pairs(pairs, mode=mode)\n",
    "\n",
    "    @staticmethod\n",
    "    @add_docstring(ngrams)\n",
    "    def lsh_hash_word(word, num_perm=128, **ngram_kwargs):\n",
    "        \"\"\"Hash an input word (str) and return a MinHash object that can be\n",
    "        added to an LSHForest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word: str\n",
    "            Word to hash.\n",
    "        num_perm: int\n",
    "        ngram_kwargs: any\n",
    "            Forwarded to `ngrams`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        datasketch MinHash object\n",
    "        \"\"\"\n",
    "        mhash = MinHash(num_perm=num_perm)\n",
    "        for ng in ngrams(word, **ngram_kwargs):\n",
    "            mhash.update(ng.encode('utf8'))\n",
    "        return mhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T02:56:37.077297Z",
     "start_time": "2022-06-18T02:56:37.017296Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_utterance_map(model_json, fuzzy=True,\n",
    "                        exclude_types=('AMAZON.Person', 'AMAZON.SearchQuery'),\n",
    "                        save_=False, model_path='data/alexa/dialog_model.json',\n",
    "                        meta_path='data/alexa/utterance2meta.pkl',\n",
    "                        min_num=0, max_num=100):\n",
    "    \"\"\"Given a dictionary copied from Alexa's JSON Editor, return a\n",
    "    dict or FuzzyKeyDict mapping each possible sample utterance to its\n",
    "    corresponding intent. This allows our delegate() function to do some\n",
    "    utterance validation before blindly forwarding an utterance to _reply() or\n",
    "    the next queued function.\n",
    "\n",
    "    Warning: because each intent may have several utterances and\n",
    "    each utterance may contain multiple slots and each slot may have multiple\n",
    "    sample values, the dimensionality can blow up quickly here.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_json\n",
    "    exclude_types: Iterable[str]\n",
    "        One or more slot types where we want to exclude intents that contain\n",
    "        any of them from the output map. For example, AMAZON.SearchQuery is\n",
    "        meant to capture whole utterances matching no particular format as a\n",
    "        fallback intent, so it wouldn't make sense to try to fuzzy match\n",
    "        these utterances to an intent. I could see AMAZON.Person being included\n",
    "        in some contexts but in this skill, we only use it for the choosePerson\n",
    "        utterance which consists solely of a name. There really shouldn't be a\n",
    "        reason to fuzzy match that.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict: Maps sample utterance to dict containing 'intent' str and 'slots'\n",
    "    dict.\n",
    "    \"\"\"\n",
    "    exclude_types = set(exclude_types)\n",
    "    model = model_json['interactionModel']['languageModel']\n",
    "    type2vals = {type_['name']: [row['name']['value']\n",
    "                                 for row in type_['values']]\n",
    "                 for type_ in model['types']}\n",
    "    type2vals['AMAZON.NUMBER'] = list(map(str, range(min_num, max_num + 1)))\n",
    "    utt2meta = {}\n",
    "    for intent in model['intents']:\n",
    "        slot2vals = {}\n",
    "        try:\n",
    "            for slot_ in intent.get('slots', []):\n",
    "                assert slot_['type'] not in exclude_types\n",
    "                slot2vals[slot_['name']] = type2vals[slot_['type']]\n",
    "        except AssertionError:\n",
    "            continue\n",
    "\n",
    "        # Replace all slot names with common slot values.\n",
    "        for row in intent['samples']:\n",
    "            for args in product(*slot2vals.values()):\n",
    "                kwargs = dict(zip(slot2vals, args))\n",
    "                utt2meta[row.format(**kwargs)] = {'intent': intent['name'],\n",
    "                                                  'slots': kwargs}\n",
    "    meta = FuzzyKeyDict(utt2meta) if fuzzy else utt2meta\n",
    "    if save_:\n",
    "        save(model_json, model_path)\n",
    "        save(meta, meta_path)\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:46:48.001747Z",
     "start_time": "2022-06-18T03:46:47.871999Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer_intent(\n",
    "    utt,\n",
    "    fuzzy_dict,\n",
    "    n_keys=5,\n",
    "    top_1_thresh=0.9,\n",
    "    weighted_thresh=0.7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Try to infer the user's intent from an utterance. Alexa should detect\n",
    "    this automatically but it sometimes messes up. This also helps if the user\n",
    "    gets the utterance slightly wrong, e.g. \"Lou, set backend to goose ai\"\n",
    "    rather than \"Lou, switch backend to goose ai\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    utt\n",
    "    fuzzy_dict\n",
    "    n_keys\n",
    "    top_1_thresh\n",
    "    weighted_thresh\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: Contains keys \"intent\", \"confidence\", \"reason\", and \"res\".\n",
    "    Intent is the name of the closest matching intent if one was sufficiently\n",
    "    close (empty string otherwise), confidence is a float between 0 and 1\n",
    "    indicating our confidence in this being correct (sort of, not anything\n",
    "    rigorous though; -1 if no matching intent is found), and reason is a string\n",
    "    indicating our method for determining this ('top_1' means we found 1 sample\n",
    "    utterance that was very close to the input, 'weighted' means that most of\n",
    "    the nearest matching utterances tended to belong to the same intent, empty\n",
    "    string means no matching intent was found). Res is always just the raw\n",
    "    results of our fuzzy_dict similar() method call, a list of tuples\n",
    "    containing all n_keys matching utterances, their corresponding intents,\n",
    "    and similarity scores.\n",
    "    \"\"\"\n",
    "    res = fuzzy_dict.similar(utt, n_keys=n_keys,\n",
    "                             mode='keys_values_similarities')\n",
    "    top_1_pct = res[0][-1] / 100\n",
    "    if top_1_pct >= top_1_thresh:\n",
    "        return {'intent': res[0][1]['intent'],\n",
    "                'slots': res[0][1]['slots'],\n",
    "                'confidence': top_1_pct,\n",
    "                'reason': 'top_1',\n",
    "                'res': res}\n",
    "    df = pd.DataFrame(res, columns=['txt', 'intent', 'score'])\\\n",
    "        .assign(slots=lambda df_: df_.intent.apply(lambda x: x['slots']),\n",
    "                intent=lambda df_: df_.intent.apply(lambda x: x['intent']))\n",
    "    weighted = df.groupby('intent').score.sum()\\\n",
    "        .to_frame()\\\n",
    "        .assign(pct=lambda x: x / (n_keys * 100))\n",
    "    if weighted.pct.iloc[0] > weighted_thresh:\n",
    "        intent = weighted.iloc[0].name\n",
    "        slots = df.loc[df.intent == intent, 'slots'].iloc[0]\n",
    "        return {'intent': intent,\n",
    "                'slots': slots,\n",
    "                'confidence': weighted.iloc[0].pct,\n",
    "                'reason': 'weighted',\n",
    "                'res': res}\n",
    "    # In this case, confidence is a bit different but it's loosely intended to\n",
    "    # mean \"confidence that the utterance matched no pre-defined intent\".\n",
    "    # Value simply needs to be higher than 1 - weighted_thresh.\n",
    "    return {'intent': '',\n",
    "            'slots': {},\n",
    "            'confidence': 1 - weighted.iloc[0].pct,\n",
    "            'reason': '',\n",
    "            'res': res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:51:24.922922Z",
     "start_time": "2022-06-18T03:51:24.871875Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_intent2utts(fd):\n",
    "    df = pd.DataFrame(fd).T\n",
    "    intent2utts = dict(df.reset_index().groupby('intent')['index'].apply(set).items())\n",
    "    return intent2utts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T02:54:20.900391Z",
     "start_time": "2022-06-18T02:54:20.810524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/alexa/dialog_model.json.\n"
     ]
    }
   ],
   "source": [
    "model = load('data/alexa/dialog_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:29:35.025172Z",
     "start_time": "2022-06-18T03:29:34.847786Z"
    }
   },
   "outputs": [],
   "source": [
    "fd = build_utterance_map(model, save_=False, max_num=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:51:56.117570Z",
     "start_time": "2022-06-18T03:51:54.735357Z"
    }
   },
   "outputs": [],
   "source": [
    "intent2utts = build_intent2utts(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:31:32.271039Z",
     "start_time": "2022-06-18T03:30:23.741220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a857429b9904b4ebb45ec23efb1f82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIMER]: Block executed in 68.481 seconds.\n"
     ]
    }
   ],
   "source": [
    "with block_timer():\n",
    "    lsh = LSHDict(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:30:17.533091Z",
     "start_time": "2022-06-18T03:29:44.413347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIMER]: Block 'HASHING' executed in 31.848 seconds.\n",
      "[TIMER]: Block 'UPDATE' executed in 1.022 seconds.\n",
      "[TIMER]: Block executed in 33.075 seconds.\n"
     ]
    }
   ],
   "source": [
    "with block_timer():\n",
    "    lsh2 = NewLSHDict(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:32:08.332210Z",
     "start_time": "2022-06-18T03:31:33.085565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIMER]: Block 'HASHING' executed in 33.982 seconds.\n",
      "[TIMER]: Block 'UPDATE' executed in 0.967 seconds.\n",
      "[TIMER]: Block executed in 35.201 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Chunksize 100\n",
    "with block_timer():\n",
    "    lsh2 = NewLSHDict(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:35:40.366019Z",
     "start_time": "2022-06-18T03:35:02.332588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIMER]: Block 'HASHING' executed in 36.665 seconds.\n",
      "[TIMER]: Block 'UPDATE' executed in 1.029 seconds.\n",
      "[TIMER]: Block executed in 37.956 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Chunksize 100, explicitly set processes=cpu_count()\n",
    "with block_timer():\n",
    "    lsh2 = NewLSHDict(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:43:23.237608Z",
     "start_time": "2022-06-18T03:42:13.398189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIMER]: Block executed in 69.805 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Threaded version\n",
    "with block_timer():\n",
    "    lsh2 = NewLSHDict(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:48:12.572153Z",
     "start_time": "2022-06-18T03:48:12.516263Z"
    }
   },
   "outputs": [],
   "source": [
    "utt = 'Lou, please change my temp to 36'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T03:48:13.066346Z",
     "start_time": "2022-06-18T03:48:12.678046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'changeTemperature',\n",
       " 'slots': {'Number': '36', 'Scope': 'global'},\n",
       " 'confidence': 0.782,\n",
       " 'reason': 'weighted',\n",
       " 'res': [('Lou change temp to 36',\n",
       "   {'intent': 'changeTemperature',\n",
       "    'slots': {'Number': '36', 'Scope': 'global'}},\n",
       "   79),\n",
       "  ('Lou change temp to 136',\n",
       "   {'intent': 'changeTemperature',\n",
       "    'slots': {'Number': '136', 'Scope': 'global'}},\n",
       "   78),\n",
       "  ('Lou change temp to 236',\n",
       "   {'intent': 'changeTemperature',\n",
       "    'slots': {'Number': '236', 'Scope': 'global'}},\n",
       "   78),\n",
       "  ('Lou change temp to 306',\n",
       "   {'intent': 'changeTemperature',\n",
       "    'slots': {'Number': '306', 'Scope': 'global'}},\n",
       "   78),\n",
       "  ('Lou change temp to 316',\n",
       "   {'intent': 'changeTemperature',\n",
       "    'slots': {'Number': '316', 'Scope': 'global'}},\n",
       "   78)]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_intent(utt, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T04:08:10.212014Z",
     "start_time": "2022-06-18T04:08:10.042359Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIMER]: Block 'lsh block' executed in 0.010 seconds.\n",
      "[TIMER]: Block 'fuzzy block' executed in 0.117 seconds.\n"
     ]
    }
   ],
   "source": [
    "with block_timer('lsh block'):\n",
    "    res = infer_intent(utt, lsh2, weighted_thresh=.5)\n",
    "with block_timer('fuzzy block'):\n",
    "    if res['intent']:\n",
    "        utts = intent2utts[res['intent']]\n",
    "        utt_match, score = process.extractOne(utt, utts, scorer=fuzz.ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T04:06:23.557201Z",
     "start_time": "2022-06-18T04:06:23.522642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lou change temp to 36\n",
      "79\n",
      "{'Number': '36', 'Scope': 'global'}\n"
     ]
    }
   ],
   "source": [
    "print(utt_match)\n",
    "print(score)\n",
    "print(fd[utt_match]['slots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T04:06:26.265777Z",
     "start_time": "2022-06-18T04:06:26.210214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14416"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intent2utts[res['intent']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T04:06:27.886435Z",
     "start_time": "2022-06-18T04:06:27.522305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIMER]: Block executed in 0.325 seconds.\n"
     ]
    }
   ],
   "source": [
    "with block_timer():\n",
    "    res = infer_intent(utt, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T04:06:28.818180Z",
     "start_time": "2022-06-18T04:06:28.778349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number': '36', 'Scope': 'global'}\n"
     ]
    }
   ],
   "source": [
    "print(res['slots'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Status**\n",
    "\n",
    "- lshDict method is a little faster but not as much as I hoped (no big O change). Mostly because a single intent can still have a large number of utts so we still end up fuzzy dict searching a big list.\n",
    "- For additional complexity to be worth it, I think I'd need to find some further optimizations. Should be at least an order of magnitude faster to be worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
