{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Try out openai api and develop a few convenience functions for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:20:59.397852Z",
     "start_time": "2021-04-16T03:20:59.362703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:20:59.648894Z",
     "start_time": "2021-04-16T03:20:59.600858Z"
    }
   },
   "outputs": [],
   "source": [
    "from enum import IntEnum, Enum\n",
    "from functools import total_ordering\n",
    "import matplotlib.pyplot as plt\n",
    "from multipledispatch import dispatch\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from htools import *\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.utils import load_openai_api_key, load_prompt, openai_auth, \\\n",
    "    bold, print_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T03:02:40.054955Z",
     "start_time": "2021-04-13T03:02:40.018726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T03:11:33.489842Z",
     "start_time": "2021-04-13T03:11:33.452551Z"
    }
   },
   "outputs": [],
   "source": [
    "openai_auth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T03:11:36.662861Z",
     "start_time": "2021-04-13T03:11:36.622979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ada'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.engines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T03:05:37.327376Z",
     "start_time": "2021-04-13T03:05:37.284230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.006, 0.006)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.prices['curie'], C.prices[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:38:17.433016Z",
     "start_time": "2021-04-11T19:38:17.349786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to change a lightbulb:\n",
      "1. Turn off the light switch. \n",
      "2. Using a cloth, push the bulb in, furn anti clockwise and remove it.\n",
      "3. Insert the new bulb and twist to find the right fit. Push it in and turn clockwise.\n",
      "4. Turn on the light to test it.\n",
      "\n",
      "How to tie a tie:\n",
      "1. Place tie around your neck.\n",
      "2. Cross the wide end over the thinner end.\n",
      "3. Run wide end under tie and pull it across again.\n",
      "4. Pull the wide end through the center.\n",
      "5. Loop through the knot.\n",
      "6. Tighten the knot.\n",
      "\n",
      "How to fry an egg:\n",
      "1. Crack the Eggs. \n",
      "2. Add the Eggs to the Pan. \n",
      "3. Cover When the Edges Turn White.\n",
      "4. Wait until it's done.\n",
      "5. Serve.\n",
      "\n",
      "How to\n"
     ]
    }
   ],
   "source": [
    "prompt = load_prompt('how_to')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:11:27.802698Z",
     "start_time": "2021-04-16T03:11:27.730579Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def query_gpt3(prompt, engine_i=0, temperature=0.7, max_tokens=50, \n",
    "               logprobs=3, mock=False, return_full=False, **kwargs):\n",
    "    if mock:\n",
    "        res = load('data/misc/sample_response.pkl')\n",
    "    else:\n",
    "        res = openai.Completion.create(\n",
    "            engine=C.engines[engine_i],\n",
    "            prompt=prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            logprobs=logprobs,\n",
    "            **kwargs\n",
    "        )\n",
    "    response = res.choices[0].text\n",
    "    result = (prompt, response)\n",
    "    if return_full: result = (*result, res)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:11:45.166137Z",
     "start_time": "2021-04-16T03:11:43.319325Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\u001b[0m April 11, 21\n",
      "\n",
      "Input: 04/11/21\n",
      "Output: April 11, 21\n",
      "\n",
      "Input: 04/11/21\n",
      "Output: April 11, 21\n",
      "\n",
      "Input: 04/11/21\n",
      "Input: 04/\n"
     ]
    }
   ],
   "source": [
    "prompt, res, full = query_gpt3(load_prompt('short_dates'), return_full=True)\n",
    "print_response(prompt, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:10:54.735293Z",
     "start_time": "2021-04-16T03:10:54.698827Z"
    }
   },
   "outputs": [],
   "source": [
    "full.choices[0].text = f'<MOCK>{full.choices[0].text}</MOCK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:11:16.930723Z",
     "start_time": "2021-04-16T03:11:16.868721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to data/misc/sample_response.pkl.\n"
     ]
    }
   ],
   "source": [
    "save(full, 'data/misc/sample_response.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:10:11.799272Z",
     "start_time": "2021-04-16T03:10:11.735465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/misc/sample_response.pkl.\n"
     ]
    }
   ],
   "source": [
    "full = load('data/misc/sample_response.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:11:54.565399Z",
     "start_time": "2021-04-16T03:11:54.531190Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/misc/sample_response.pkl.\n"
     ]
    }
   ],
   "source": [
    "prompt, text, full = query_gpt3(load_prompt('short_dates'), mock=True,\n",
    "                                return_full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:11:55.781408Z",
     "start_time": "2021-04-16T03:11:55.710819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\u001b[0m<MOCK> 4/11/21\n",
      "\n",
      "Input: 01/20/2017\n",
      "Output: January 20, 2017\n",
      "\n",
      "Input: 2017\n",
      "Output: 2017\n",
      "\n",
      "Input: 07/01/2017\n",
      "Output: 07/01/2017\n",
      "\n",
      "Input</MOCK>\n"
     ]
    }
   ],
   "source": [
    "print_response(prompt, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:30:23.242076Z",
     "start_time": "2021-04-14T03:30:21.197440Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1+1=\u001b[0m1+1+1\n"
     ]
    }
   ],
   "source": [
    "prompt, res = query_gpt3(load_prompt('shortest'), max_tokens=5)\n",
    "print_response(prompt, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:32:03.538550Z",
     "start_time": "2021-04-14T03:32:03.411825Z"
    }
   },
   "outputs": [],
   "source": [
    "res = openai.Completion.create(\n",
    "            engine=C.engines[0],\n",
    "            prompt=load_prompt('shortest'),\n",
    "            max_tokens=2,\n",
    "            logprobs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:32:06.526299Z",
     "start_time": "2021-04-14T03:32:06.482234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens', 'token_logprobs', 'top_logprobs', 'text_offset'])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.choices[0].logprobs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:32:07.446182Z",
     "start_time": "2021-04-14T03:32:07.408036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '.']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.choices[0].logprobs.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:32:08.365810Z",
     "start_time": "2021-04-14T03:32:08.314538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.1797945, -0.9931051]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.choices[0].logprobs.token_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:32:09.520590Z",
     "start_time": "2021-04-14T03:32:09.480416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x120c40048> JSON: {\n",
       "  \"0\": -2.1797945,\n",
       "  \"1\": -1.97355,\n",
       "  \"2\": -1.8083305\n",
       "},\n",
       " <OpenAIObject at 0x120c400a0> JSON: {\n",
       "  \"\\n\": -2.8806696,\n",
       "  \",\": -2.796979,\n",
       "  \".\": -0.9931051\n",
       "}]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list has length=num_tokens. Each item has length=logprobs (func arg).\n",
    "res.choices[0].logprobs.top_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:17:52.281496Z",
     "start_time": "2021-04-14T03:17:52.194480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.choices[0].logprobs.text_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:45:54.180391Z",
     "start_time": "2021-04-14T03:45:52.706100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " April\n",
      " 11\n",
      ",\n",
      " 2021\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Output\n",
      ":\n",
      " August\n",
      " 31\n",
      ",\n",
      " 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "__\n",
      "Tw\n",
      "ice\n",
      "_\n",
      "\n",
      "\n",
      "Notes\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jim\n",
      " Trin\n"
     ]
    }
   ],
   "source": [
    "for chunk in openai.Completion.create(\n",
    "    engine=C.engines[0],\n",
    "    prompt=load_prompt('short_dates'),\n",
    "    max_tokens=25,\n",
    "    stream=True\n",
    "):\n",
    "    print(chunk.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T03:46:36.152028Z",
     "start_time": "2021-04-14T03:46:35.108540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-2oQm3qC5HBXJ7DX5xqtemiHq0TS7v at 0x120c3a7d8> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" April 11, 2100\\n\\nThis product incorporates the fundamentals of the 3-Magnet principle and mechanical resonation. However\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1618371995,\n",
       "  \"id\": \"cmpl-2oQm3qC5HBXJ7DX5xqtemiHq0TS7v\",\n",
       "  \"model\": \"ada:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(\n",
    "    engine=C.engines[0],\n",
    "    prompt=load_prompt('short_dates'),\n",
    "    max_tokens=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:34:08.967656Z",
     "start_time": "2021-04-11T19:34:06.298502Z"
    }
   },
   "outputs": [],
   "source": [
    "res = openai.Completion.create(engine=C.engines[0],\n",
    "                               prompt=prompt, temperature=.4,\n",
    "                               max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:37:25.719649Z",
     "start_time": "2021-04-11T19:37:25.656105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " make a cake:\n",
      "1. Cut the Cake.\n",
      "2. Roll it out.\n",
      "3. Cut out the Design.\n",
      "4. Cut out the Cake.\n",
      "5. Roll it out again.\n",
      "6. Cut out the design.\n",
      "7. Roll it out again.\n",
      "\n",
      "How to make a cake:\n",
      "1. Cut the Cake.\n",
      "2. Roll it out.\n",
      "3. Cut out the Design.\n",
      "4. Roll it out again.\n",
      "5. Cut\n"
     ]
    }
   ],
   "source": [
    "text = res['choices'][0]['text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:37:59.018905Z",
     "start_time": "2021-04-11T19:37:52.915099Z"
    }
   },
   "outputs": [],
   "source": [
    "res = openai.Completion.create(engine=C.engines[-1],\n",
    "                               prompt=prompt, temperature=.4,\n",
    "                               max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:38:00.151460Z",
     "start_time": "2021-04-11T19:38:00.082805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " make a cake:\n",
      "1. Preheat the Oven.\n",
      "2. Add the Ingredients in the Right Order.\n",
      "3. Bake in the Oven.\n",
      "4. Wait until it's done.\n",
      "5. Serve.\n",
      "\n",
      "How to make a pizza:\n",
      "1. Preheat the Oven.\n",
      "2. Add the Ingredients in the Right Order.\n",
      "3. Bake in the Oven.\n",
      "4. Wait until it's done.\n",
      "5. Serve.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = res['choices'][0]['text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:43:22.326736Z",
     "start_time": "2021-04-11T19:43:20.905538Z"
    }
   },
   "outputs": [],
   "source": [
    "res_dates = openai.Completion.create(\n",
    "    engine=C.engines[0],\n",
    "    prompt=load_prompt('short_dates'),\n",
    "    temperature=.4,\n",
    "    max_tokens=30,\n",
    "    logprobs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:43:54.845084Z",
     "start_time": "2021-04-11T19:43:54.765543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\n",
      " November 11, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output: November 11, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(load_prompt('short_dates'))\n",
    "print(res_dates['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\u001b[0mInput: 04/11/21\n",
      "Output: November 11, 2017\n",
      "\n",
      "Input: 04/11/21\n"
     ]
    }
   ],
   "source": [
    "# Moved to test paperspace but don't want to save API credits.\n",
    "prompt = load_prompt('short_dates')\n",
    "res = \"\"\"Input: 04/11/21\n",
    "Output: November 11, 2017\n",
    "\n",
    "Input: 04/11/21\"\"\"\n",
    "print_response(prompt, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:39:33.785002Z",
     "start_time": "2021-04-16T03:39:33.749001Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_gpt3_stream(prompt, engine_i=0, temperature=0.7, max_tokens=50,\n",
    "                      logprobs=None, mock=False, return_full=False,\n",
    "                      **kwargs):\n",
    "    \"\"\"Generator version of query_gpt3. Parameters are the same, but return\n",
    "    values are not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt: str\n",
    "    engine_i: int\n",
    "        Corresponds to engines defined in config, where 0 is the cheapest, 3 is\n",
    "        the most expensive, etc.\n",
    "    temperature: float\n",
    "        Between 0 and 1. 0-0.4 is good for straightforward informational\n",
    "        queries (e.g. reformatting, writing business emails) while 0.7-1 is\n",
    "        good for more creative works.\n",
    "    max_tokens: int\n",
    "    logprobs: int or None\n",
    "        Get log probabilities for top n candidates at each time step.\n",
    "    mock: bool\n",
    "        If True, return a saved sample response instead of hitting the API\n",
    "        (saves tokens). Note that your other gpt3 kwargs\n",
    "        (max_tokens, logprobs, kwargs) will be ignored.\n",
    "        return_full will be respected since it affects the number of items\n",
    "        returned - it's not a kwarg passed to the actual query function. The\n",
    "        first chunk's text will be prefixed by '<MOCK>' and the last chunk's\n",
    "        text will be suffixed by '</MOCK>' to make it easier to notice if I'm\n",
    "        unintentionally using the wrong mode.\n",
    "    return_full: bool\n",
    "        If True, yield a third item which is the full response object.\n",
    "        Otherwise we just yield the prompt and response text. (We keep the\n",
    "        parameter name the same as in query_gpt3 to maintain a consistent\n",
    "        interface, but technically values are yielded rather than returned.)\n",
    "    kwargs: any\n",
    "        Additional kwargs to pass to gpt3.\n",
    "        Ex: presence_penalty, frequency_penalty (both floats in [0, 1]).\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    str or tuple: First item is response text (str). If return_full is True,\n",
    "    a second item consisting of the whole response object is yielded as well.\n",
    "    Because we're streaming results here, we yield 1 token (I think - if not,\n",
    "    a very small chunk of text) at a time rather than returning everything at\n",
    "    once.\n",
    "    \"\"\"\n",
    "    if mock:\n",
    "        stream_res = load('data/misc/sample_stream_response.pkl')\n",
    "    else:\n",
    "        stream_res = openai.Completion.create(\n",
    "            engine=C.engines[engine_i],\n",
    "            prompt=prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            logprobs=logprobs,\n",
    "            stream=True,\n",
    "            **kwargs\n",
    "        )\n",
    "    for chunk in stream_res:\n",
    "        text = chunk.choices[0].text\n",
    "        yield (text, chunk) if return_full else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:57:10.282393Z",
     "start_time": "2021-04-15T03:57:10.234333Z"
    }
   },
   "outputs": [],
   "source": [
    "dates = load_prompt('short_dates')\n",
    "shortest = load_prompt('shortest')\n",
    "how_to = load_prompt('how_to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:14:27.288122Z",
     "start_time": "2021-04-16T03:14:27.250072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/misc/sample_stream_response.pkl.\n",
      "0\n",
      " make\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "1\n",
      " a\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "2\n",
      " cake\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "3\n",
      ":\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for i, (text, chunk) in enumerate(query_gpt3_stream(\n",
    "        how_to, return_full=True, logprobs=2, max_tokens=5, mock=True)):\n",
    "    print(i)\n",
    "    print(text)\n",
    "    chunks.append(chunk)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:15:59.128714Z",
     "start_time": "2021-04-16T03:15:59.095411Z"
    }
   },
   "outputs": [],
   "source": [
    "chunks[0].choices[0].text = f'<PROMPT>{chunks[0].choices[0].text}'\n",
    "chunks[-1].choices[0].text = f'{chunks[-1].choices[0].text}</PROMPT>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:16:17.675398Z",
     "start_time": "2021-04-16T03:16:17.641927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to data/misc/sample_stream_response.pkl.\n"
     ]
    }
   ],
   "source": [
    "save(chunks, 'data/misc/sample_stream_response.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:33:27.405338Z",
     "start_time": "2021-04-15T03:33:27.358895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[openai.openai_object.OpenAIObject, openai.openai_object.OpenAIObject]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:33:28.425816Z",
     "start_time": "2021-04-15T03:33:28.394053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<OpenAIObject at 0x120c8ff10> JSON: {\n",
       "  \" 04\": -2.4684024,\n",
       "  \" April\": -1.7035612,\n",
       "  \" November\": -0.91182953\n",
       "}],\n",
       " [<OpenAIObject at 0x120c82eb8> JSON: {\n",
       "  \" 1\": -2.956241,\n",
       "  \" 10\": -4.0053625,\n",
       "  \" 11\": -0.13010749\n",
       "}]]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chunk.choices[0].logprobs.top_logprobs for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T02:55:44.357095Z",
     "start_time": "2021-04-16T02:55:43.694142Z"
    }
   },
   "outputs": [],
   "source": [
    "openai.Completion.create??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T03:40:59.314508Z",
     "start_time": "2021-04-13T03:40:59.258691Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_content_filter(text):\n",
    "    \"\"\"Wrapper to determine if a piece of text is safe, sensitive, or unsafe.\n",
    "    Details on these categories are here:\n",
    "    https://beta.openai.com/docs/engines/content-filter\n",
    "    This endpoint is free.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple[int, float]: First value is predicted class, where 0 is safe,\n",
    "    1 is sensitive (politics/religion/race/suicide etc.), and 2 is unsafe \n",
    "    (profane/prejudiced/otherwise NSFW).\n",
    "    \"\"\"\n",
    "    res = openai.Completion.create(\n",
    "        engine='content-filter-alpha-c4', \n",
    "        prompt='<|end-of-text|>' + text + '\\n--\\nLabel:',\n",
    "        temperature=0,\n",
    "        max_tokens=1,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        logprobs=10\n",
    "    )\n",
    "    label = res.choices[0].text\n",
    "    cls2logp = {x: res.choices[0].logprobs.top_logprobs[0]\n",
    "                      .get(x, float('-inf'))\n",
    "                for x in ['0', '1', '2']}\n",
    "    logp = cls2logp.pop(label)\n",
    "    # If model is not confident in prediction of 2, choose the next most \n",
    "    # likely class. See https://beta.openai.com/docs/engines/content-filter.\n",
    "    if label == '2' and logp < -.355 and cls2logp:\n",
    "        label, logp = max(cls2logp.items(), key=lambda x: x[-1])\n",
    "    return int(label), np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T03:11:46.084316Z",
     "start_time": "2021-04-13T03:11:43.906438Z"
    }
   },
   "outputs": [],
   "source": [
    "sensitive_txt = 'I\\'m going to kill myself.'\n",
    "sensitive_resp = openai.Completion.create(\n",
    "    engine='content-filter-alpha-c4', \n",
    "    prompt='<|end-of-text|>' + sensitive_txt + '\\n--\\nLabel:',\n",
    "    temperature=0,\n",
    "    max_tokens=1,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    logprobs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T03:38:40.728407Z",
     "start_time": "2021-04-13T03:38:39.422271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.6920572815388716)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_content_filter(sensitive_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombine stream and non-stream functions\n",
    "\n",
    "Realized we can have stream function return a generator rather than yielding values, allowing us to recover a single interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:35:53.524935Z",
     "start_time": "2021-04-16T03:35:53.487970Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:37:31.247517Z",
     "start_time": "2021-04-16T03:37:31.217454Z"
    }
   },
   "outputs": [],
   "source": [
    "def slow(i):\n",
    "    time.sleep(1)\n",
    "    print('call slow', i)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:37:34.546074Z",
     "start_time": "2021-04-16T03:37:31.492145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call slow 0\n",
      "call slow 1\n",
      "call slow 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[slow(i) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:37:34.625425Z",
     "start_time": "2021-04-16T03:37:34.552504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x11732a930>"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(slow(i) for i in range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:37:34.693538Z",
     "start_time": "2021-04-16T03:37:34.628601Z"
    }
   },
   "outputs": [],
   "source": [
    "g1 = (slow(i) for i in range(3))\n",
    "g2 = (slow(c) for c in 'abc')\n",
    "z = zip(g1, g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:37:44.795101Z",
     "start_time": "2021-04-16T03:37:38.737929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call slow 0\n",
      "call slow a\n",
      "0 a\n",
      "call slow 1\n",
      "call slow b\n",
      "1 b\n",
      "call slow 2\n",
      "call slow c\n",
      "2 c\n"
     ]
    }
   ],
   "source": [
    "for x, y in z:\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:48:52.613155Z",
     "start_time": "2021-04-16T03:48:52.566546Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_gpt3_proto(prompt, engine_i=0, temperature=0.7, max_tokens=50,\n",
    "                     logprobs=None, stream=False, mock=False,\n",
    "                     return_full=False, **kwargs):\n",
    "    \"\"\"Convenience function to query gpt3.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt: str\n",
    "    engine_i: int\n",
    "        Corresponds to engines defined in config, where 0 is the cheapest, 3 \n",
    "        is the most expensive, etc.\n",
    "    temperature: float\n",
    "        Between 0 and 1. 0-0.4 is good for straightforward informational\n",
    "        queries (e.g. reformatting, writing business emails) while 0.7-1 is\n",
    "        good for more creative works.\n",
    "    max_tokens: int\n",
    "        Sets max response length. One token is ~.75 words.\n",
    "    logprobs: int or None\n",
    "        Get log probabilities for top n candidates at each time step.\n",
    "    stream: bool\n",
    "        If True, return an iterator instead of a str/tuple. See the returns\n",
    "        section as the output is slightly different. I believe each chunk \n",
    "        returns one token when stream is True.\n",
    "    mock: bool\n",
    "        If True, return a saved sample response instead of hitting the API \n",
    "        in order to save tokens. Note that your other gpt3 kwargs \n",
    "        (max_tokens, logprobs, kwargs) will be ignored. return_full will be\n",
    "        respected since it affects the number of items returned - it's not a \n",
    "        kwarg passed to the actual query function. Text is surrounded by \n",
    "        <MOCK></MOCK> tags to make it obvious when mock is True (it's easy to\n",
    "        forget to change the value of mock when switching back and forth).\n",
    "    return_full: bool\n",
    "        If True, return a third item which is the full response object.\n",
    "        Otherwise we just return the prompt and response text.\n",
    "    kwargs: any\n",
    "        Additional kwargs to pass to gpt3.\n",
    "        Ex: presence_penalty, frequency_penalty (both floats in [0, 1]).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple or iterator: When stream=False, we return a tuple where the first\n",
    "    item is the prompt (str) and the second is the response text(str). If\n",
    "    return_full is True, a third item consisting of the whole response object\n",
    "    is returned as well. When stream=True, we return an iterator where each\n",
    "    step contains a single token. This will either be the text response alone\n",
    "    (str) or a tuple of (text, response) if return_full is True. Unlike in\n",
    "    non-streaming mode, we don't return the prompt - that seems less \n",
    "    appropriate for many time steps.\n",
    "    \"\"\"\n",
    "    if mock:\n",
    "        res = load(C.mock_stream_paths[stream])\n",
    "    else:\n",
    "        res = openai.Completion.create(\n",
    "            engine=C.engines[engine_i],\n",
    "            prompt=prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            logprobs=logprobs,\n",
    "            stream=stream,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    # Extract text and return.\n",
    "    if stream:\n",
    "        texts = (chunk.choices[0].text for chunk in res)\n",
    "        return zip(texts, res) if return_full else texts\n",
    "    else:\n",
    "        output = (prompt, res.choices[0].text, res)\n",
    "        return output if return_full else output[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:39:45.241353Z",
     "start_time": "2021-04-16T03:39:45.192872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/misc/sample_response.pkl.\n",
      "\u001b[1mInput: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\u001b[0m<MOCK> 4/11/21\n",
      "\n",
      "Input: 01/20/2017\n",
      "Output: January 20, 2017\n",
      "\n",
      "Input: 2017\n",
      "Output: 2017\n",
      "\n",
      "Input: 07/01/2017\n",
      "Output: 07/01/2017\n",
      "\n",
      "Input</MOCK>\n"
     ]
    }
   ],
   "source": [
    "prompt, response = query_gpt3_proto(dates, mock=True)\n",
    "print_response(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:39:46.592881Z",
     "start_time": "2021-04-16T03:39:46.557610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/misc/sample_response.pkl.\n",
      "\u001b[1mInput: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\u001b[0m<MOCK> 4/11/21\n",
      "\n",
      "Input: 01/20/2017\n",
      "Output: January 20, 2017\n",
      "\n",
      "Input: 2017\n",
      "Output: 2017\n",
      "\n",
      "Input: 07/01/2017\n",
      "Output: 07/01/2017\n",
      "\n",
      "Input</MOCK>\n"
     ]
    }
   ],
   "source": [
    "prompt, response, full = query_gpt3_proto(dates, return_full=True, mock=True)\n",
    "print_response(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:49:06.971575Z",
     "start_time": "2021-04-16T03:49:06.930309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/misc/sample_stream_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object query_gpt3_proto.<locals>.<genexpr> at 0x11732acf0>"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = query_gpt3_proto(dates, stream=True, mock=True)\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:49:09.710560Z",
     "start_time": "2021-04-16T03:49:09.676005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PROMPT> make\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      " a\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      " cake\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ":\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "</PROMPT>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for txt in gen:\n",
    "    print(txt)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:49:12.432432Z",
     "start_time": "2021-04-16T03:49:12.396039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/misc/sample_stream_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<zip at 0x1172e87c8>"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = query_gpt3_proto(dates, stream=True, return_full=True, mock=True)\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:49:12.682828Z",
     "start_time": "2021-04-16T03:49:12.644611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PROMPT> make\n",
      "<class 'openai.openai_object.OpenAIObject'>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      " a\n",
      "<class 'openai.openai_object.OpenAIObject'>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      " cake\n",
      "<class 'openai.openai_object.OpenAIObject'>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ":\n",
      "<class 'openai.openai_object.OpenAIObject'>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "</PROMPT>\n",
      "<class 'openai.openai_object.OpenAIObject'>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for txt, chunk in gen:\n",
    "    print(txt)\n",
    "    print(type(chunk))\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:39:57.925516Z",
     "start_time": "2021-04-16T03:39:57.893070Z"
    }
   },
   "outputs": [],
   "source": [
    "for txt, chunk in gen:\n",
    "    print(txt)\n",
    "    print(type(chunk))\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:40:03.283354Z",
     "start_time": "2021-04-16T03:40:02.045502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\u001b[0m November 11, 2017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt, response = query_gpt3_proto(dates, mock=False, max_tokens=5)\n",
    "print_response(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:40:23.638304Z",
     "start_time": "2021-04-16T03:40:22.706505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput: 3/1/20\n",
      "Output: March 1, 2020\n",
      "\n",
      "Input: 09-04-99\n",
      "Output: September 4, 1999\n",
      "\n",
      "Input: 11/01/2017\n",
      "Output: November 1, 2017\n",
      "\n",
      "Input: 04/11/21\n",
      "Output:\u001b[0m November 11, 2017\n",
      "\n"
     ]
    }
   ],
   "source": [
    " prompt, response, full = query_gpt3_proto(dates, return_full=True, \n",
    "                                           mock=False, max_tokens=5)\n",
    "print_response(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:50:13.867415Z",
     "start_time": "2021-04-16T03:50:13.589315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object query_gpt3_proto.<locals>.<genexpr> at 0x1173371b0>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = query_gpt3_proto(dates, stream=True, mock=False, max_tokens=5)\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:50:15.134785Z",
     "start_time": "2021-04-16T03:50:15.093040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " November\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      " 11\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ",\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      " 21\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for txt in gen:\n",
    "    print(txt)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:50:13.867415Z",
     "start_time": "2021-04-16T03:50:13.589315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x1172f80c8>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = query_gpt3_proto(dates, stream=True, mock=False, max_tokens=5, \n",
    "                       return_full=True)\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T03:50:15.134785Z",
     "start_time": "2021-04-16T03:50:15.093040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " May\n",
      "<class 'openai.openai_object.OpenAIObject'>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ",\n",
      "<class 'openai.openai_object.OpenAIObject'>\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for txt, chunk in gen:\n",
    "    print(txt)\n",
    "    print(type(chunk))\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch\n",
    "\n",
    "Experimented with more complex solutions below but these were ridiculously over-complicated for my intended use case. Just trying stuff out for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T01:42:30.436701Z",
     "start_time": "2021-04-11T01:42:30.362868Z"
    }
   },
   "outputs": [],
   "source": [
    "@total_ordering\n",
    "@auto_repr\n",
    "class Engine:\n",
    "    \n",
    "    def __init__(self, name, index, price):\n",
    "        self.name = name\n",
    "        self.index = index\n",
    "        self.price = price\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return self.index == other.index\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.index < other.index\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T01:42:30.709968Z",
     "start_time": "2021-04-11T01:42:30.662539Z"
    }
   },
   "outputs": [],
   "source": [
    "class Engines:\n",
    "    \n",
    "#     i2eng = ['ada', 'babbage', 'curie', 'davinci']\n",
    "#     eng2price = {'ada': .0008,\n",
    "#                  'babbage': .0012,\n",
    "#                  'curie': .006,\n",
    "#                  'davinci': .06}\n",
    "    \n",
    "    i2eng = ['ada', 'babbage', 'curie', 'davinci']\n",
    "    engines = [Engine(name, i, price) for i, (name, price) in \n",
    "               enumerate(zip(i2eng, [.0008, .0012, .006, .06]))]\n",
    "    \n",
    "    @dispatch(int)\n",
    "    def __getitem__(self, i):\n",
    "        return self.i2eng[i]\n",
    "    \n",
    "    @dispatch((list, tuple))\n",
    "    def __getitem__(self, idx):\n",
    "        return [self[i] for i in idx]\n",
    "    \n",
    "#     @dispatch(int)\n",
    "#     def price(self, i):\n",
    "#         return self.eng2price[self[i]]\n",
    "    \n",
    "#     @dispatch(str)\n",
    "#     def price(self, name):\n",
    "#         return self.eng2price[name]\n",
    "    \n",
    "#     def prices(self):\n",
    "#         return self.eng2price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T01:42:31.192843Z",
     "start_time": "2021-04-11T01:42:31.149671Z"
    }
   },
   "outputs": [],
   "source": [
    "ENGINES = Engines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T01:42:31.784606Z",
     "start_time": "2021-04-11T01:42:31.750587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Engine(name='ada', index=0, price=0.0008),\n",
       " Engine(name='babbage', index=1, price=0.0012),\n",
       " Engine(name='curie', index=2, price=0.006),\n",
       " Engine(name='davinci', index=3, price=0.06)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGINES.engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T01:42:32.489817Z",
     "start_time": "2021-04-11T01:42:32.457383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGINES[0] < ENGINES[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T01:42:33.360029Z",
     "start_time": "2021-04-11T01:42:33.325462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada', 'curie']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGINES[[0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T01:42:34.004301Z",
     "start_time": "2021-04-11T01:42:33.964513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada', 'curie']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGINES[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T01:42:35.052831Z",
     "start_time": "2021-04-11T01:42:35.020273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ada\n",
      "babbage\n",
      "curie\n",
      "davinci\n"
     ]
    }
   ],
   "source": [
    "for e in ENGINES:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
