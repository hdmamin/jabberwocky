{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Experiment with ways to support passing a list of prompts to query method. Some backends don't support this natively, others do, but none automatically would return the format I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:32.218861Z",
     "start_time": "2022-04-09T22:31:32.201839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:34.870720Z",
     "start_time": "2022-04-09T22:31:32.537418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.openai_utils import load_prompt, load_openai_api_key, \\\n",
    "    GPTBackend\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:35.556609Z",
     "start_time": "2022-04-09T22:31:35.525709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: make thread that returns value so we can run a separate query for each thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:37.067944Z",
     "start_time": "2022-04-09T22:31:37.033094Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class ReturningThread(Thread):\n",
    "\n",
    "    @add_docstring(Thread)\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs=None, *, daemon=None):\n",
    "        \"\"\"This is identical to a regular thread except that the join method\n",
    "        returns the value returned by your target function. The\n",
    "        Thread.__init__ docstring is shown below for the sake of convenience.\n",
    "        \"\"\"\n",
    "        super().__init__(group=group, target=target, name=name,\n",
    "                         args=args, kwargs=kwargs, daemon=daemon)\n",
    "        self.result = None\n",
    "\n",
    "    def run(self):\n",
    "        self.result = self._target(*self._args, **self._kwargs)\n",
    "        \n",
    "    def join(self, timeout=None):\n",
    "        super().join(timeout)\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:37.646693Z",
     "start_time": "2022-04-09T22:31:37.619135Z"
    }
   },
   "outputs": [],
   "source": [
    "def foo(x, wait=2):\n",
    "    time.sleep(wait)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:38.615295Z",
     "start_time": "2022-04-09T22:31:38.585416Z"
    }
   },
   "outputs": [],
   "source": [
    "def foo_inv(x, wait=2):\n",
    "    time.sleep(1 / wait)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:38.777291Z",
     "start_time": "2022-04-09T22:31:38.746491Z"
    }
   },
   "outputs": [],
   "source": [
    "def foo_random(x, max_wait=5):\n",
    "    wait = np.random.uniform(low=0, high=max_wait)\n",
    "    print(wait, flush=True)\n",
    "    time.sleep(wait)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:22:18.048518Z",
     "start_time": "2022-04-03T20:22:07.985576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns values but is slow (sync execution).\n",
    "res = [foo(i) for i in range(5)]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:21:52.630627Z",
     "start_time": "2022-04-03T20:21:50.591510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [Thread(target=foo, args=(i,)) for i in range(5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# Regular thread returns None.\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:25:38.697676Z",
     "start_time": "2022-04-03T20:25:36.653887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [ReturningThread(target=foo, args=(i,)) for i in range(5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# ReturningThread returns values!\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:50:45.441326Z",
     "start_time": "2022-04-05T03:50:43.407449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait 1.0\n",
      "wait 0.5\n",
      "wait 0.3333333333333333\n",
      "wait 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [ReturningThread(target=foo_inv, args=(i, i)) for i in range(1, 5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# ReturningThread returns values!\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:54:31.953745Z",
     "start_time": "2022-04-05T03:54:28.204670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7145703512021404\n",
      "0.5710566753268154\n",
      "2.778421106481786\n",
      "0.44917966624138606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [ReturningThread(target=foo_random, args=(i, 5))\n",
    "           for i in range(1, 5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# ReturningThread returns values!\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try integrating into GPTBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:43.076560Z",
     "start_time": "2022-04-03T21:44:43.036655Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: no guarantees these threads return in the right order, though, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:48:54.240825Z",
     "start_time": "2022-04-06T03:48:53.298931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt_banana at 0x11e51f2f0>\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTBackend()\n",
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:48:57.096765Z",
     "start_time": "2022-04-06T03:48:57.066022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt_huggingface at 0x1100820d0>\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('huggingface')\n",
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:44:00.646610Z",
     "start_time": "2022-04-05T02:44:00.612877Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Six million years after the pandemic,',\n",
    "    'The stegosaurus'\n",
    "]\n",
    "kwargs = {'max_tokens': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:34:33.358120Z",
     "start_time": "2022-04-03T20:34:32.812552Z"
    }
   },
   "outputs": [],
   "source": [
    "threads = [ReturningThread(target=gpt.query, args=(prompt,), kwargs=kwargs) \n",
    "           for prompt in prompts]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "res = [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:34:35.086435Z",
     "start_time": "2022-04-03T20:34:35.039987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the world is still in the grip of a global',\n",
       "  {'generated_text': ' the world is still in the grip of a global'}),\n",
       " ('is a large, large, and highly intelligent animal',\n",
       "  {'generated_text': ' is a large, large, and highly intelligent animal'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:36:21.349540Z",
     "start_time": "2022-04-03T20:36:16.550197Z"
    }
   },
   "outputs": [],
   "source": [
    "threads = [ReturningThread(target=gpt.query,\n",
    "                           args=(prompt,), \n",
    "                           kwargs={**kwargs, 'n': 3, \n",
    "                                   'logprobs': 4, 'engine_i': 1}) \n",
    "           for prompt in prompts]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "res = [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:36:39.231035Z",
     "start_time": "2022-04-03T20:36:39.180639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmap(len, *res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:36:48.181801Z",
     "start_time": "2022-04-03T20:36:48.133726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the population of New York City is poised to rise',\n",
       " 'the pandemic strain of influenza spreads and mutates',\n",
       " 'our species is still struggling to deal with the effects']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:05.079974Z",
     "start_time": "2022-04-03T20:37:05.029095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' the population of New York City is poised to rise'},\n",
       " {'generated_text': ' the pandemic strain of influenza spreads and mutates'},\n",
       " {'generated_text': ' our species is still struggling to deal with the effects'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:29.152724Z",
     "start_time": "2022-04-03T20:37:29.105080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is one of the more remarkable prehistoric dinosaurs, and',\n",
       " ', or giant pterosaur from the late',\n",
       " 'fossil, or dinosaur\\nFossil bones of']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:31.718185Z",
     "start_time": "2022-04-03T20:37:31.646264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' is one of the more remarkable prehistoric dinosaurs, and'},\n",
       " {'generated_text': ', or giant pterosaur from the late'},\n",
       " {'generated_text': ' fossil, or dinosaur\\nFossil bones of'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:49:06.209819Z",
     "start_time": "2022-04-06T03:49:06.180009Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt3 at 0x1100829d8>\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('gooseai')\n",
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:38:56.466544Z",
     "start_time": "2022-04-03T20:38:54.624773Z"
    }
   },
   "outputs": [],
   "source": [
    "threads = [ReturningThread(target=gpt.query,\n",
    "                           args=(prompt,), \n",
    "                           kwargs={'max_tokens': 8, 'n': 2, \n",
    "                                   'logprobs': 5, 'engine_i': 0}) \n",
    "           for prompt in prompts]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "res = [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:40:56.897362Z",
     "start_time": "2022-04-03T20:40:56.848457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:39:06.290772Z",
     "start_time": "2022-04-03T20:39:06.258358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the world is still in the grip of',\n",
       " 'scientists still do not know whether humans are']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:39:12.866153Z",
     "start_time": "2022-04-03T20:39:12.834078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is a fossilized dinosaur named by the',\n",
       " 'is a famous carnivorous dinosaur from the']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:41:15.524928Z",
     "start_time": "2022-04-03T20:41:15.477253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[0])#[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:52:30.767721Z",
     "start_time": "2022-04-03T20:52:30.718958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the world is still in the grip of',\n",
       "  'scientists still do not know whether humans are'],\n",
       " ['is a fossilized dinosaur named by the',\n",
       "  'is a famous carnivorous dinosaur from the'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, resps = list(zip(*res))\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:53:26.283690Z",
     "start_time": "2022-04-03T20:53:26.244193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' the', ' world', ' is', ' still', ' in', ' the', ' grip', ' of'],\n",
       " [' scientists',\n",
       "  ' still',\n",
       "  ' do',\n",
       "  ' not',\n",
       "  ' know',\n",
       "  ' whether',\n",
       "  ' humans',\n",
       "  ' are']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resps[i][j] corresponds to prompt i, completion j.\n",
    "[completion['logprobs'].tokens for completion in resps[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:53:35.756499Z",
     "start_time": "2022-04-03T20:53:35.701049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' is', ' a', ' fossil', 'ized', ' dinosaur', ' named', ' by', ' the'],\n",
       " [' is', ' a', ' famous', ' carniv', 'orous', ' dinosaur', ' from', ' the']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[completion['logprobs'].tokens for completion in resps[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:49:10.210774Z",
     "start_time": "2022-04-06T03:49:10.177457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt3 at 0x1100829d8>\n"
     ]
    }
   ],
   "source": [
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:56:40.049508Z",
     "start_time": "2022-04-03T20:56:39.320533Z"
    }
   },
   "outputs": [],
   "source": [
    "threads2 = [ReturningThread(target=gpt.query,\n",
    "                           args=(prompt,), \n",
    "                           kwargs={'max_tokens': 8, 'n': 1, \n",
    "                                   'logprobs': 5, 'engine_i': 0}) \n",
    "            for prompt in prompts]\n",
    "for thread in threads2:\n",
    "    thread.start()\n",
    "res2 = [thread.join() for thread in threads2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:57:26.768849Z",
     "start_time": "2022-04-03T20:57:26.725952Z"
    }
   },
   "outputs": [],
   "source": [
    "texts2, resps2 = list(zip(*res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:57:29.637473Z",
     "start_time": "2022-04-03T20:57:29.585330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the future of the world’s', 'The stegosaurus (Ste')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:59:01.385365Z",
     "start_time": "2022-04-03T20:59:01.343301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'length',\n",
       " 'index': 0,\n",
       " 'logprobs': <OpenAIObject at 0x125e93d58> JSON: {\n",
       "   \"text_offset\": [\n",
       "     0,\n",
       "     4,\n",
       "     11,\n",
       "     14,\n",
       "     18,\n",
       "     24,\n",
       "     24,\n",
       "     25\n",
       "   ],\n",
       "   \"token_logprobs\": [\n",
       "     -1.7412109375,\n",
       "     -5.03515625,\n",
       "     -0.361083984375,\n",
       "     -1.7802734375,\n",
       "     -1.8515625,\n",
       "     -1.89453125,\n",
       "     -0.0006651878356933594,\n",
       "     -0.00013065338134765625\n",
       "   ],\n",
       "   \"tokens\": [\n",
       "     \" the\",\n",
       "     \" future\",\n",
       "     \" of\",\n",
       "     \" the\",\n",
       "     \" world\",\n",
       "     \"\\ufffd\",\n",
       "     \"\\ufffd\",\n",
       "     \"s\"\n",
       "   ],\n",
       "   \"top_logprobs\": [\n",
       "     {\n",
       "       \" a\": -2.90234375,\n",
       "       \" it\": -3.947265625,\n",
       "       \" scientists\": -4.09375,\n",
       "       \" the\": -1.7412109375,\n",
       "       \" we\": -2.748046875\n",
       "     },\n",
       "     {\n",
       "       \" city\": -4.51171875,\n",
       "       \" human\": -4.06640625,\n",
       "       \" pand\": -4.2109375,\n",
       "       \" virus\": -2.9921875,\n",
       "       \" world\": -1.6845703125\n",
       "     },\n",
       "     {\n",
       "       \" is\": -2.072265625,\n",
       "       \" looks\": -3.4296875,\n",
       "       \" of\": -0.361083984375,\n",
       "       \" remains\": -3.44921875,\n",
       "       \" still\": -4.171875\n",
       "     },\n",
       "     {\n",
       "       \" CO\": -3.35546875,\n",
       "       \" global\": -3.57421875,\n",
       "       \" infectious\": -3.62890625,\n",
       "       \" pand\": -3.509765625,\n",
       "       \" the\": -1.7802734375\n",
       "     },\n",
       "     {\n",
       "       \" global\": -3.478515625,\n",
       "       \" human\": -2.720703125,\n",
       "       \" pand\": -3.0625,\n",
       "       \" virus\": -2.365234375,\n",
       "       \" world\": -1.8515625\n",
       "     },\n",
       "     {\n",
       "       \" is\": -1.21484375,\n",
       "       \" looks\": -3.654296875,\n",
       "       \" remains\": -3.19140625,\n",
       "       \"'s\": -1.814453125,\n",
       "       \"\\ufffd\": -1.89453125\n",
       "     },\n",
       "     {\n",
       "       \"\\ufffd\": -10.5546875\n",
       "     },\n",
       "     {\n",
       "       \" s\": -11.3671875,\n",
       "       \"bytes:'\\\\n'\": -11.8671875,\n",
       "       \"ll\": -12.3125,\n",
       "       \"s\": -0.00013065338134765625,\n",
       "       \"\\ufffd\": -11.3125\n",
       "     }\n",
       "   ]\n",
       " },\n",
       " 'text': ' the future of the world’s',\n",
       " 'token_index': 0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because only 1 completion per prompt, resps is a dict instead of a list of \n",
    "# dicts.\n",
    "resps2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:44:25.174986Z",
     "start_time": "2022-04-05T02:44:24.549929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    hf_res = gpt.query('I want', engine_i=1, max_tokens=5, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:44:26.068677Z",
     "start_time": "2022-04-05T02:44:26.034625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results(text=['to give you one big', 'to show you some pictures'], full=[{'generated_text': ' to give you one big'}, {'generated_text': ' to show you some pictures'}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results(text=hf_res[0], full=hf_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better interface?\n",
    "# texts, full_resps = gpt.query([p1, p2, p3], n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test streaming mode\n",
    "\n",
    "Need a better understanding of what using streaming mode is like before I decide about streaming interface for np or nc > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:52.683198Z",
     "start_time": "2022-04-09T22:31:52.654076Z"
    }
   },
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "from jabberwocky.openai_utils import query_gpt3, query_gpt_huggingface, \\\n",
    "    query_gpt_banana, query_gpt_j, query_gpt_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:52.937264Z",
     "start_time": "2022-04-09T22:31:52.905322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Was toying with idea of adding this to gpt.query warnings to make the \n",
    "# messages unique, in the hope that this would ensure they're always shown\n",
    "# rather than just once. But a. I'm not sure if that's how they define \n",
    "# duplicates, and b. I'm seeing code defined in nb seems to always show \n",
    "# warnings, not just once, so I'm not sure what to make of that. Still \n",
    "# eventually want to write a func like this (maybe moreso for creating new\n",
    "# file paths when encountering collisions) but that should have a more limited\n",
    "# set of possible characters.\n",
    "def random_str(length, lower=True):\n",
    "    rand = b64encode(os.urandom(length)).decode()[:length]\n",
    "    return rand.lower() if lower else rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:00.135475Z",
     "start_time": "2022-04-05T02:45:00.081330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 d\n",
      "2 1g\n",
      "3 p4u\n",
      "4 dpvk\n",
      "5 ijbgo\n",
      "6 a1hztl\n",
      "7 7qzb4lz\n",
      "8 cxcnot6l\n",
      "9 jthktr7ne\n",
      "10 lueazbofkx\n",
      "11 brpiuhixuva\n",
      "12 /j6h2vyqih8m\n",
      "13 rnv/a+azvmlsr\n",
      "14 row/b8sipg+zfr\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    rand = random_str(i)\n",
    "    print(i, rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:03.668606Z",
     "start_time": "2022-04-05T02:45:03.636388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' \\x97\\xc1\\xc3\\x0eB29\\x9b\\xf7'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.urandom(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:16:53.362657Z",
     "start_time": "2022-04-05T03:16:53.318087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "Switching  backend back to \"huggingface\".\n",
      "I  {'index': 0, 'finish_reason': None}\n",
      "want  {'index': 0, 'finish_reason': None}\n",
      "to  {'index': 0, 'finish_reason': None}\n",
      "go  {'index': 0, 'finish_reason': None}\n",
      "to  {'index': 0, 'finish_reason': None}\n",
      "there.  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    repeat_res = gpt.query('I want to go to there.',\n",
    "                           max_tokens=5, stream=True)\n",
    "\n",
    "for txt_, full_ in repeat_res:\n",
    "    print(txt_, full_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:16:43.476606Z",
     "start_time": "2022-04-05T03:16:43.442206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "Switching  backend back to \"huggingface\".\n",
      "I  {'index': 0, 'finish_reason': None}\n",
      "want  {'index': 0, 'finish_reason': None}\n",
      "to  {'index': 0, 'finish_reason': None}\n",
      "go  {'index': 0, 'finish_reason': None}\n",
      "to  {'index': 0, 'finish_reason': None}\n",
      "there.  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "I  {'index': 1, 'finish_reason': None}\n",
      "want  {'index': 1, 'finish_reason': None}\n",
      "to  {'index': 1, 'finish_reason': None}\n",
      "go  {'index': 1, 'finish_reason': None}\n",
      "to  {'index': 1, 'finish_reason': None}\n",
      "there.  {'index': 1, 'finish_reason': 'dummy'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    repeat_res = gpt.query('I want to go to there.',\n",
    "                           max_tokens=5, stream=True, n=2)\n",
    "\n",
    "for txt_, full_ in repeat_res:\n",
    "    print(txt_, full_)\n",
    "    if full_['finish_reason']:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:17:21.347273Z",
     "start_time": "2022-04-05T03:17:18.227244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "\n",
      "\n",
      "I'm  {'id': 'aa2f677a-bc98-4e76-b3e7-ab8d6e75302f', 'message': 'success', 'created': 1649128641, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': \"\\n\\nI'm a\", 'input': 'Who are you?'}], 'index': 0, 'finish_reason': None}\n",
      "a  {'id': 'aa2f677a-bc98-4e76-b3e7-ab8d6e75302f', 'message': 'success', 'created': 1649128641, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': \"\\n\\nI'm a\", 'input': 'Who are you?'}], 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('banana'):\n",
    "    for txt_, full_ in gpt.query('Who are you?',\n",
    "                                 max_tokens=5, stream=True):\n",
    "        print(txt_, full_)\n",
    "        if full_['finish_reason']:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:18:15.585713Z",
     "start_time": "2022-04-05T03:18:14.702742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:278: UserWarning: query_gpt_huggingface received unused kwargs {'stream': True}.\n",
      "  warnings.warn('query_gpt_huggingface received unused kwargs '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I  {'generated_text': '\\n\\nI am a', 'index': 0, 'finish_reason': None}\n",
      "am  {'generated_text': '\\n\\nI am a', 'index': 0, 'finish_reason': None}\n",
      "a  {'generated_text': '\\n\\nI am a', 'index': 0, 'finish_reason': 'dummy'}\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    for txt_, full_ in gpt.query('Who are you?',\n",
    "                                 max_tokens=5, stream=True):\n",
    "        print(txt_, full_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:18:47.204416Z",
     "start_time": "2022-04-05T03:18:46.701482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "\n",
      "\n",
      "I'm  {'generated_text': \"\\n\\nI'm going\", 'index': 0, 'finish_reason': None}\n",
      "going  {'generated_text': \"\\n\\nI'm going\", 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "Are  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "you  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "being  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "honest  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "or  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    for txt_, full_ in gpt.query('Who are you?',\n",
    "                                 max_tokens=5, stream=True, n=2, engine_i=1):\n",
    "        print(txt_, full_)\n",
    "        if full_['finish_reason']:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:37:55.051099Z",
     "start_time": "2022-04-03T21:37:54.998748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason'])]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.choices[0].keys() for row in load(C.mock_stream_paths[True])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with streaming text AND dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:14.112834Z",
     "start_time": "2022-04-05T02:45:14.074292Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:14.453818Z",
     "start_time": "2022-04-05T02:45:14.416411Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream_words(text):\n",
    "    \"\"\"Like stream_chars but splits on spaces. Realized stream_chars was a bad\n",
    "    idea because we risk giving SPEAKER turns like\n",
    "    \"This is over. W\" and \"hat are you doing next?\", neither of which would be\n",
    "    pronounced as intended. We yield with a space for consistency with the\n",
    "    other streaming interfaces which require no further postprocessing.\n",
    "    \"\"\"\n",
    "    for word in text.split(' '):\n",
    "        yield word + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:17.783386Z",
     "start_time": "2022-04-05T02:45:17.739551Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream_response(text:str, full:dict):\n",
    "    yield from zip(stream_words(text), cycle([full]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:18.171856Z",
     "start_time": "2022-04-05T02:53:18.121860Z"
    }
   },
   "outputs": [],
   "source": [
    "def containerize(*args):\n",
    "    res = []\n",
    "    for arg in args:\n",
    "        if listlike(arg):\n",
    "            res.append(arg)\n",
    "        else:\n",
    "            res.append([arg])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:18.336766Z",
     "start_time": "2022-04-05T02:53:18.304906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: this is probably massively over-engineered for mock streaming, but \n",
    "# I'll need to do something like this if I want to support real streaming \n",
    "# where nc and/or np > 1 so it was probably useful to work through this logic\n",
    "# anyway.\n",
    "def stream_multi_response(texts:list, fulls:list):\n",
    "    texts, fulls = containerize(texts, fulls)\n",
    "    for i, (text, full) in enumerate(zip(texts, fulls)):\n",
    "        queue = deque()\n",
    "        gen = stream_response(text, \n",
    "                              {**full, 'index': i, 'finish_reason': None})\n",
    "        done = False\n",
    "        # Yield items while checking if we're at the last item so we can mark\n",
    "        # it with a finish_reason. This lets us know when one completion ends.\n",
    "        while True:\n",
    "            try:\n",
    "                tok, tok_full = next(gen)\n",
    "                queue.append((tok, tok_full))\n",
    "            except StopIteration:\n",
    "                done = True\n",
    "            \n",
    "            while len(queue) > 1:\n",
    "                tok, tok_full = queue.popleft()\n",
    "                yield tok, tok_full\n",
    "            if done: break\n",
    "        tok, tok_full = queue.popleft()\n",
    "        tok_full['finish_reason'] = 'dummy'    \n",
    "        yield tok, tok_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:19.330156Z",
     "start_time": "2022-04-05T02:53:19.297219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abc'], [{'text': 'def'}]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containerize('abc', {'text': 'def'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:19.482525Z",
     "start_time": "2022-04-05T02:53:19.448745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abc'], [{'text': 'def'}]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containerize(['abc'], [{'text': 'def'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:19.876063Z",
     "start_time": "2022-04-05T02:53:19.831675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abc', 'hij'], [{'text': 'def'}, {'text': 'aka'}]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containerize(['abc', 'hij'], [{'text': 'def'}, {'text': 'aka'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:22.112368Z",
     "start_time": "2022-04-05T02:53:22.080809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Santa '\n",
      "'is '\n",
      "'coming '\n",
      "'to '\n",
      "'town. '\n"
     ]
    }
   ],
   "source": [
    "txt = 'Santa is coming to town.'\n",
    "for tok in stream_words(txt):\n",
    "    print(repr(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:25.043479Z",
     "start_time": "2022-04-05T02:53:25.008566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Santa ' {}\n",
      "'is ' {}\n",
      "'coming ' {}\n",
      "'to ' {}\n",
      "'town. ' {}\n"
     ]
    }
   ],
   "source": [
    "for tok, full in stream_response(txt, {}):\n",
    "    print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:06:01.902403Z",
     "start_time": "2022-04-03T22:06:01.416816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1\n",
    "with gpt('huggingface'):\n",
    "    hf_res = gpt.query('I want', engine_i=1, max_tokens=5, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:31.922090Z",
     "start_time": "2022-04-05T02:53:31.891452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['to give you one big', 'to show you some pictures'],\n",
       " [{'generated_text': ' to give you one big'},\n",
       "  {'generated_text': ' to show you some pictures'}])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:35.170612Z",
     "start_time": "2022-04-05T02:53:35.136715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>  to  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  give  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  you  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  one  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  big  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': 'dummy'}\n",
      ">>>  to  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  show  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  you  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  some  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  pictures  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': 'dummy'}\n"
     ]
    }
   ],
   "source": [
    "for tok, full in stream_multi_response(*hf_res):\n",
    "    print('>>> ', tok, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:38.019901Z",
     "start_time": "2022-04-05T02:53:37.989912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>  so  {'response': 'so', 'index': 0, 'finish_reason': 'dummy'}\n"
     ]
    }
   ],
   "source": [
    "# nc = 1, already containerized.\n",
    "for tok, full in stream_multi_response(['so'], [{'response': 'so'}]):\n",
    "    print('>>> ', tok, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:54:12.600410Z",
     "start_time": "2022-04-05T02:54:12.549155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>  so  {'response': 'so', 'index': 0, 'finish_reason': 'dummy'}\n"
     ]
    }
   ],
   "source": [
    "# nc = 1, not yet containerized.\n",
    "for tok, full in stream_multi_response('so', {'response': 'so'}):\n",
    "    print('>>> ', tok, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:39.638234Z",
     "start_time": "2022-04-05T02:53:39.608034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Empty response.\n",
    "for tok, full in stream_multi_response([], []):\n",
    "    print('>>> ', tok, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T20:59:17.669052Z",
     "start_time": "2022-04-09T20:59:17.127807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1, stream=True\n",
    "txt = 'Santa is coming to town.'\n",
    "with gpt('gooseai'):\n",
    "    goose_res = openai.Completion.create(\n",
    "        prompt=txt,\n",
    "        engine=GPTBackend.engine(0),\n",
    "        max_tokens=5,\n",
    "        logprobs=3,\n",
    "        n=2,\n",
    "        stream=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:46:34.780227Z",
     "start_time": "2022-04-09T21:46:33.997765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "Switching  backend back to \"banana\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1, stream=True\n",
    "txts = ['Yesterday was', 'How many']\n",
    "with gpt('gooseai'):\n",
    "    goose_res_multi = openai.Completion.create(\n",
    "        prompt=txts,\n",
    "        engine=GPTBackend.engine(0),\n",
    "        max_tokens=5,\n",
    "        logprobs=3,\n",
    "        n=2,\n",
    "        stream=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:46:42.849635Z",
     "start_time": "2022-04-09T21:46:42.815359Z"
    }
   },
   "outputs": [],
   "source": [
    "_goose_res_multi = list(goose_res_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:26.675931Z",
     "start_time": "2022-04-03T22:53:25.148802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"openai\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1, stream=True\n",
    "with gpt('openai'):\n",
    "    open_res = openai.Completion.create(\n",
    "        prompt=txt,\n",
    "        engine=GPTBackend.engine(0),\n",
    "        max_tokens=5,\n",
    "        logprobs=3,\n",
    "        n=2,\n",
    "        stream=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T20:59:38.229945Z",
     "start_time": "2022-04-09T20:59:38.184369Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          0\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -3.74609375\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" This\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" And\": -2.826171875,\n",
      "            \" The\": -3.029296875,\n",
      "            \"bytes:'\\\\n'\": -1.3095703125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" This\",\n",
      "      \"token_index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          5\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -1.263671875\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" year\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" is\": -2.001953125,\n",
      "            \" time\": -2.638671875,\n",
      "            \" year\": -1.263671875\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" year\",\n",
      "      \"token_index\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          10\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -11.7734375\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Lisa\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"'s\": -3.095703125,\n",
      "            \",\": -0.7060546875,\n",
      "            \"\\ufffd\": -2.1328125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Lisa\",\n",
      "      \"token_index\": 2\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          15\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -3.330078125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \",\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" and\": -2.134765625,\n",
      "            \" is\": -3.26953125,\n",
      "            \",\": -3.330078125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \",\",\n",
      "      \"token_index\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          16\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -6.7890625\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Sara\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" a\": -2.7890625,\n",
      "            \" our\": -3.2421875,\n",
      "            \" the\": -2.599609375\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Sara\",\n",
      "      \"token_index\": 4\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          0\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -13.8515625\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Lt\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" And\": -2.826171875,\n",
      "            \" The\": -3.029296875,\n",
      "            \"bytes:'\\\\n'\": -1.3095703125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Lt\",\n",
      "      \"token_index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          3\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -0.15576171875\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \".\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" Col\": -4.10546875,\n",
      "            \" Colonel\": -5.61328125,\n",
      "            \".\": -0.15576171875\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \".\",\n",
      "      \"token_index\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          4\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -10.28125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Tay\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" Col\": -2.677734375,\n",
      "            \" Colonel\": -2.896484375,\n",
      "            \" Commander\": -2.751953125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Tay\",\n",
      "      \"token_index\": 2\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          8\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -6.1796875\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \"ana\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"e\": -2.953125,\n",
      "            \"ler\": -1.7470703125,\n",
      "            \"st\": -2.91015625\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \"ana\",\n",
      "      \"token_index\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          11\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -4.9453125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" B\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" and\": -4.0234375,\n",
      "            \" is\": -3.548828125,\n",
      "            \",\": -3.458984375\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" B\",\n",
      "      \"token_index\": 4\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_goose_res = []\n",
    "for obj in goose_res:\n",
    "    print(obj)\n",
    "    _goose_res.append(obj)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:42.976851Z",
     "start_time": "2022-04-03T22:53:42.924028Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          24\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -8.621929\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Feeling\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"\\n\": -2.4186804,\n",
      "            \" I\": -2.6238666,\n",
      "            \" She\": -2.4244554\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Feeling\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          32\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -5.186215\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" her\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" a\": -2.1202018,\n",
      "            \" like\": -2.6433406,\n",
      "            \" the\": -2.8912876\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" her\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          24\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.6238666\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" I\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"\\n\": -2.4186804,\n",
      "            \" I\": -2.6238666,\n",
      "            \" She\": -2.4244554\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" I\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          26\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.2295423\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \"'m\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" don\": -2.8276584,\n",
      "            \"'m\": -2.2295423,\n",
      "            \"bytes:\\\\xe2\\\\x80\": -2.7309577\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \"'m\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          28\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.6193585\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" sure\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" going\": -1.7995286,\n",
      "            \" not\": -2.3888402,\n",
      "            \" sure\": -2.6193585\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" sure\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          36\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -3.293669\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" presence\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" up\": -2.2750113,\n",
      "            \" way\": -2.5486472,\n",
      "            \",\": -3.0275166\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" presence\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          33\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -1.7617589\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" you\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" he\": -2.186368,\n",
      "            \" she\": -1.4318895,\n",
      "            \" you\": -1.7617589\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" you\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          45\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -1.0045006\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \",\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" in\": -2.5863404,\n",
      "            \" is\": -2.647669,\n",
      "            \",\": -1.0045006\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \",\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          46\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -8.532746\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Des\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" I\": -1.8522211,\n",
      "            \" he\": -2.4737713,\n",
      "            \" the\": -2.7948432\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Des\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          37\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.7016406\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" can\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"'ll\": -1.5801843,\n",
      "            \"'re\": -2.1810775,\n",
      "            \"'ve\": -1.8903823\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" can\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_open_res = []\n",
    "for obj in open_res:\n",
    "    print(obj)\n",
    "    _open_res.append(obj)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:47:35.605569Z",
     "start_time": "2022-04-09T21:47:35.575491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 20)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_goose_res), len(_open_res), len(_goose_res_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:47:38.062817Z",
     "start_time": "2022-04-09T21:47:38.027917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([' This'], None),\n",
       " ([' year'], None),\n",
       " ([' Lisa'], None),\n",
       " ([','], None),\n",
       " ([' Sara'], 'length'),\n",
       " ([' Lt'], None),\n",
       " (['.'], None),\n",
       " ([' Tay'], None),\n",
       " (['ana'], None),\n",
       " ([' B'], 'length')]"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(row.choices[0].logprobs.tokens, row.choices[0].finish_reason) \n",
    " for row in _goose_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:48:10.643859Z",
     "start_time": "2022-04-09T21:48:10.611827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([' one'], None, 0),\n",
       " ([' of'], None, 0),\n",
       " ([' the'], None, 0),\n",
       " ([' most'], None, 0),\n",
       " ([' important'], 'length', 0),\n",
       " ([' of'], None, 2),\n",
       " ([' you'], None, 2),\n",
       " ([' have'], None, 2),\n",
       " ([' heard'], None, 2),\n",
       " ([' or'], 'length', 2),\n",
       " ([' thoughts'], None, 3),\n",
       " ([' go'], None, 3),\n",
       " ([' through'], None, 3),\n",
       " ([' your'], None, 3),\n",
       " ([' head'], 'length', 3),\n",
       " ([' the'], None, 1),\n",
       " ([' end'], None, 1),\n",
       " ([' of'], None, 1),\n",
       " ([' fashion'], None, 1),\n",
       " ([' week'], 'length', 1)]"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(row.choices[0].logprobs.tokens, \n",
    "  row.choices[0].finish_reason, \n",
    "  row.choices[0].index) \n",
    " for row in _goose_res_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:56.046309Z",
     "start_time": "2022-04-03T22:53:55.996554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([' Feeling'], None),\n",
       " ([' her'], None),\n",
       " ([' I'], None),\n",
       " ([\"'m\"], None),\n",
       " ([' sure'], None),\n",
       " ([' presence'], None),\n",
       " ([' you'], None),\n",
       " ([','], None),\n",
       " ([' Des'], 'length'),\n",
       " ([' can'], 'length')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(row.choices[0].logprobs.tokens, row.choices[0].finish_reason) \n",
    " for row in _open_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:54:26.430884Z",
     "start_time": "2022-04-03T22:54:26.374792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Feeling',\n",
       " ' her',\n",
       " ' I',\n",
       " \"'m\",\n",
       " ' sure',\n",
       " ' presence',\n",
       " ' you',\n",
       " ',',\n",
       " ' Des',\n",
       " ' can']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.choices[0].text for row in _open_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:55:09.775032Z",
     "start_time": "2022-04-03T23:55:09.726986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thought we might be able to use id to reconstruct each completion but that\n",
    "# doesn't work.\n",
    "[row['id'] for row in _open_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:57:34.622597Z",
     "start_time": "2022-04-03T23:57:34.587532Z"
    }
   },
   "outputs": [],
   "source": [
    "# index points to which completion each new token belongs to.\n",
    "completions = defaultdict(list)\n",
    "for row in _open_res:\n",
    "    completions[row['choices'][0]['index']].append(row['choices'][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:57:37.089716Z",
     "start_time": "2022-04-03T23:57:37.038623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [' Feeling', ' her', ' presence', ',', ' Des'],\n",
       "             1: [' I', \"'m\", ' sure', ' you', ' can']})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:02:18.244513Z",
     "start_time": "2022-04-04T00:02:18.199128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None\n",
      "0 None\n",
      "0 None\n",
      "0 None\n",
      "0 length\n",
      "1 None\n",
      "1 None\n",
      "1 None\n",
      "1 None\n",
      "1 length\n"
     ]
    }
   ],
   "source": [
    "# index points to which completion each new token belongs to.\n",
    "# completions = defaultdict(list)\n",
    "for row in _goose_res:\n",
    "    print(row['choices'][0]['index'], row['choices'][0]['finish_reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:02:34.306496Z",
     "start_time": "2022-04-04T00:02:34.265497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None\n",
      "0 None\n",
      "1 None\n",
      "1 None\n",
      "1 None\n",
      "0 None\n",
      "1 None\n",
      "0 None\n",
      "0 length\n",
      "1 length\n"
     ]
    }
   ],
   "source": [
    "# index points to which completion each new token belongs to.\n",
    "# completions = defaultdict(list)\n",
    "for row in _open_res:\n",
    "    print(row['choices'][0]['index'], row['choices'][0]['finish_reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:05:31.960786Z",
     "start_time": "2022-04-04T00:05:31.568056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Switching openai backend to \"repeat\".\n",
      "stream=False\n",
      " ('Santa is coming to town.', {})\n",
      "\n",
      "stream=True\n",
      "'Santa ' {}\n",
      "'is ' {}\n",
      "'coming ' {}\n",
      "'to ' {}\n",
      "'town. ' {}\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:923: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn('strip_output=True is not supported in stream '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:928: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'Streaming mode does not support manual truncation of '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:391: UserWarning: Unused kwargs {'stream': True} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    print('stream=False\\n', gpt.query(txt))\n",
    "    print('\\nstream=True')\n",
    "    for tok, full in gpt.query(txt, stream=True):\n",
    "        print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:05:48.562505Z",
     "start_time": "2022-04-04T00:05:48.530971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "stream=False\n",
      " (['Santa is coming to town.', 'Santa is coming to town.', 'Santa is coming to town.'], [{}, {}, {}])\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    print('stream=False\\n', gpt.query(txt, n=3))\n",
    "#     print('\\nstream=True')\n",
    "#     for tok, full in gpt.query(txt, stream=True):\n",
    "#         print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:42:45.885181Z",
     "start_time": "2022-04-03T23:42:44.090140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Switching openai backend to \"huggingface\".\n",
      "stream=False\n",
      " ('The city is', {'generated_text': '\\n\\nThe city is'})\n",
      "\n",
      "stream=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:923: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn('strip_output=True is not supported in stream '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:928: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'Streaming mode does not support manual truncation of '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:275: UserWarning: query_gpt_huggingface received unused kwargs {'stream': True}.\n",
      "  warnings.warn('query_gpt_huggingface received unused kwargs '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n\\nThe ' {'generated_text': '\\n\\nThe city is'}\n",
      "'city ' {'generated_text': '\\n\\nThe city is'}\n",
      "'is ' {'generated_text': '\\n\\nThe city is'}\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    tmp = gpt.query(txt, max_tokens=5)\n",
    "    print('stream=False\\n', tmp)\n",
    "    print('\\nstream=True')\n",
    "    for tok, full in gpt.query(txt, stream=True, max_tokens=5):\n",
    "        print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:30:53.742399Z",
     "start_time": "2022-04-03T23:30:53.668410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '\\n\\nThe city is'} The \n",
      "{'generated_text': '\\n\\nThe city is'} city \n",
      "{'generated_text': '\\n\\nThe city is'} is \n"
     ]
    }
   ],
   "source": [
    "for tok, full in stream_response(tmp[0], tmp[1]):\n",
    "    print(full, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:27:57.764767Z",
     "start_time": "2022-04-03T23:27:57.714951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The ', {'generated_text': '\\n\\nThe city is'})\n",
      "('city ', {'generated_text': '\\n\\nThe city is'})\n",
      "('is ', {'generated_text': '\\n\\nThe city is'})\n"
     ]
    }
   ],
   "source": [
    "for row in stream_response(*tmp):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:09:04.872102Z",
     "start_time": "2022-04-04T00:09:04.417421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['to give you one big', 'to show you some pictures'],\n",
       " [{'generated_text': ' to give you one big'},\n",
       "  {'generated_text': ' to show you some pictures'}])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:07:51.956064Z",
     "start_time": "2022-04-04T00:07:51.881109Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-63fca387ae31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhf_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-dd51190cc58a>\u001b[0m in \u001b[0;36mstream_response\u001b[0;34m(text, full)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstream_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-c66818f51168>\u001b[0m in \u001b[0;36mstream_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mother\u001b[0m \u001b[0mstreaming\u001b[0m \u001b[0minterfaces\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mrequire\u001b[0m \u001b[0mno\u001b[0m \u001b[0mfurther\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "for row in stream_response(*hf_res):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try using ReturningThread to prototype handling multiple prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:32:11.289283Z",
     "start_time": "2022-04-09T22:32:11.259273Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import multiprocessing\n",
    "from threading import Lock\n",
    "\n",
    "from jabberwocky.utils import with_signature, JsonlinesLogger, load_api_key,\\\n",
    "    strip, squeeze, JsonlinesFormatter, touch, stream_multi_response\n",
    "from jabberwocky.openai_utils import truncate_at_first_stop, \\\n",
    "    MockFunctionException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:38:26.141106Z",
     "start_time": "2022-04-09T22:38:26.111908Z"
    }
   },
   "outputs": [],
   "source": [
    "def thread_starmap(func, kwargs_list=None):\n",
    "    kwargs_list = kwargs_list or [{}]\n",
    "    threads = [ReturningThread(target=func, kwargs=kwargs)\n",
    "               for kwargs in tolist(kwargs_list)]\n",
    "    for thread in threads: thread.start()\n",
    "    return [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:51:26.678470Z",
     "start_time": "2022-04-09T22:51:26.627444Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPTBackend:\n",
    "    \"\"\"\n",
    "    Examples\n",
    "    --------\n",
    "    gpt = GPTBackend()\n",
    "\n",
    "    # Default backend is openai.\n",
    "    openai_res = gpt.query(**kwargs)\n",
    "\n",
    "    with gpt('gooseai'):\n",
    "        # Now we're using the gooseai backend.\n",
    "        gooseai_res = gpt.query(**kwargs)\n",
    "\n",
    "    # Now we're back to using openai.\n",
    "    openai_res_2 = gpt.query(**kwargs)\n",
    "\n",
    "    # Now we'll switch to gooseai and changes will persist since we're not\n",
    "    # using a context manager.\n",
    "    gpt.switch('gooseai')\n",
    "    gooseai_res_2 = gpt.query(**kwargs)\n",
    "    \"\"\"\n",
    "\n",
    "    logger = JsonlinesLogger(\n",
    "        f'./data/logs/{datetime.today().strftime(\"%Y.%m.%d\")}.jsonlines'\n",
    "    )\n",
    "    lock = Lock()\n",
    "\n",
    "    # Only include backends here that actually should change the\n",
    "    # openai.api_base value (these will probably be backends that require no\n",
    "    # or minimal mock_funcs).\n",
    "    name2base = {\n",
    "        'openai': 'https://api.openai.com',\n",
    "        'gooseai': 'https://api.goose.ai/v1',\n",
    "    }\n",
    "\n",
    "    # Order matters: keep openai first so name2key initialization works.\n",
    "    name2func = {\n",
    "        'openai': query_gpt3,\n",
    "        'gooseai': query_gpt3,\n",
    "        'huggingface': query_gpt_huggingface,\n",
    "        'hobby': query_gpt_j,\n",
    "        'repeat': query_gpt_repeat,\n",
    "        'banana': query_gpt_banana\n",
    "    }\n",
    "\n",
    "    # Names of backends that perform stop word truncation how we want (i.e.\n",
    "    # allow us to specify stop phrases AND truncate before the phrase rather\n",
    "    # than after, if we encounter one).\n",
    "    skip_trunc = {'openai'}\n",
    "\n",
    "    name2key = {}\n",
    "    for name in name2func:\n",
    "        if name in {'hobby', 'repeat'}:\n",
    "            name2key[name] = f'<{name.upper()} BACKEND: FAKE API KEY>'\n",
    "        else:\n",
    "            name2key[name] = load_api_key(name)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.new_name = ''\n",
    "        self.old_name = ''\n",
    "        self.old_key = ''\n",
    "\n",
    "    def __call__(self, name):\n",
    "        \"\"\"__enter__ can't take arguments so we need to specify this here.\n",
    "        Notice that name is auto-lowercased and spaces are removed.\n",
    "        \"\"\"\n",
    "        new_name = name.lower().replace(' ', '')\n",
    "        if new_name not in self.name2func:\n",
    "            raise ValueError(f'Invalid name {name}. Valid options are: '\n",
    "                             f'{list(self.name2func)}')\n",
    "\n",
    "        self.new_name = new_name\n",
    "        self.old_name = self.current()\n",
    "        return self\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Change backend to the one specified in __call__, which is\n",
    "        automatically called first when using `with` syntax.\n",
    "        \"\"\"\n",
    "        print(f'Switching openai backend to \"{self.new_name}\".')\n",
    "        # Store an attribute on openai itself to reduce risk of bugs caused by\n",
    "        # GPTBackend being deleted or recreated. Previously used a\n",
    "        # self.base2name mapping to retrieve the current name but that doesn't\n",
    "        # work when multiple names use the same base (e.g. huggingface and\n",
    "        # hobby API backends can't be identified just by their base with\n",
    "        # this implementation).\n",
    "        openai.curr_name = self.new_name\n",
    "        self.old_key, openai.api_key = openai.api_key, \\\n",
    "            self.name2key[self.new_name]\n",
    "        if self.new_name in self.name2base:\n",
    "            openai.api_base = self.name2base[self.new_name]\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, traceback):\n",
    "        \"\"\"Revert to previously used backend on contextmanager exit.\"\"\"\n",
    "        print(f'Switching  backend back to \"{self.old_name}\".')\n",
    "        openai.api_key = self.old_key\n",
    "        if self.old_name in self.name2base:\n",
    "            openai.api_base = self.name2base[self.old_name]\n",
    "        openai.curr_name = self.old_name\n",
    "        self.clear()\n",
    "\n",
    "    @classmethod\n",
    "    def ls(cls):\n",
    "        \"\"\"Print current state of the backend: api_base, api_key, and \n",
    "        mock_func. Mostly useful for debugging and sanity checks.\n",
    "        \"\"\"\n",
    "        print('\\nBase:', openai.api_base)\n",
    "        print('Query func:', cls._get_query_func())\n",
    "\n",
    "    @classmethod\n",
    "    def backends(cls):\n",
    "        \"\"\"List all valid backend names. We could always access these via a\n",
    "        class attribute but this name is easier to remember.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[str]\n",
    "        \"\"\"\n",
    "        return list(cls.name2func)\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Reset instance variables tracking that were used to restore\n",
    "        previous backend.\n",
    "        \"\"\"\n",
    "        self.old_key = self.old_name = self.new_name = ''\n",
    "\n",
    "    def switch(self, name):\n",
    "        \"\"\"Switch backend and make changes persist, unlike in context manager\n",
    "        where we reset them on exit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name: str\n",
    "            One of (openai, gooseai).\n",
    "        \"\"\"\n",
    "        self(name=name).__enter__()\n",
    "        self.clear()\n",
    "\n",
    "    @staticmethod\n",
    "    def current():\n",
    "        \"\"\"Get current backend name, e.g. \"gooseai\". If we've ever switched\n",
    "        backend with GPTBackend, openai.curr_name\n",
    "        should exist. If not, the backend should be the default.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "        \"\"\"\n",
    "        return getattr(openai, 'curr_name', 'openai')\n",
    "\n",
    "    @classmethod\n",
    "    def _get_query_func(cls, backend=None):\n",
    "        \"\"\"Return current mock function (callable or None).\"\"\"\n",
    "        return cls.name2func[backend or cls.current()]\n",
    "\n",
    "    @classmethod\n",
    "    def key(cls):\n",
    "        \"\"\"Return current API key. In some cases this is a mock value since\n",
    "        some modes don't have a key.\n",
    "        \"\"\"\n",
    "        # More reliable than checking name2key because the openai attribute\n",
    "        # is what's actually used (at least for openai vs. gooseai -\n",
    "        # huggingface mock_func technically uses a global).\n",
    "        return openai.api_key\n",
    "\n",
    "    @classmethod\n",
    "    def engine(cls, engine_i, backend=None):\n",
    "        \"\"\"Get appropriate engine name depending on current api backend and\n",
    "        selected engine_i.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        engine_i: int\n",
    "            Number from 0-3 (inclusive) specifying which model to use. The two\n",
    "            backends *should* perform similar for values of 0-2, but openai's\n",
    "            3 (davinci, 175 billion parameters) is a much bigger model than\n",
    "            gooseai's 3 (NEO-X, 20 billion parameters). Mostly used in\n",
    "            query_gpt3().\n",
    "        backend: str or None\n",
    "            If provided, should be the name of a backend (e.g. 'huggingface'\n",
    "            or any of the keys in GPTBackend.backends()).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str: Name of an engine, e.g. \"davinci\" if we're in openai mode or\n",
    "        \"gpt-neo-20b\" if we're in gooseai mode.\n",
    "        \"\"\"\n",
    "        engines = C.backend_engines[backend or cls.current()]\n",
    "\n",
    "        # Adds better error message if user passes in a number too big for the\n",
    "        # current backend.\n",
    "        try:\n",
    "            return engines[engine_i]\n",
    "        except IndexError:\n",
    "            raise ValueError(f'Encountered invalid engine_i value: {engine_i}.'\n",
    "                             f'Should be one of {list(range(len(engines)))} '\n",
    "                             f'when using backend {cls.current()}.')\n",
    "\n",
    "    # Decorator order matters - doesn't work if we flip these.\n",
    "    @classmethod\n",
    "    @with_signature(query_gpt3)\n",
    "    @add_docstring(query_gpt3)\n",
    "    def query(cls, prompt, strip_output=True, log=True, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prompt\n",
    "        strip_output\n",
    "        log: bool or str\n",
    "            If True, the logfile defaults to a path like\n",
    "            './data/logs/2022.04.07.jsonlines' (current year, month, day).\n",
    "            If str, use that as the log path. If False or None, do not log.\n",
    "        kwargs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[str, dict]\n",
    "        \"\"\"\n",
    "        if not isinstance(prompt, str):\n",
    "            raise NotImplementedError(\n",
    "                f'Prompt must be str, not {type(prompt).__name__}.'\n",
    "            )\n",
    "\n",
    "        # Keep trunc_full definition here so we can provide warnings if user\n",
    "        # is in stream mode.\n",
    "        query_func = cls._get_query_func()\n",
    "        trunc_full = cls.current() not in cls.skip_trunc\n",
    "        stream = kwargs.get('stream', False)\n",
    "        if stream:\n",
    "            if strip_output:\n",
    "                warnings.warn('strip_output=True is not supported in stream '\n",
    "                              'mode. Automatically setting it to False.')\n",
    "                strip_output = False\n",
    "            if trunc_full:\n",
    "                warnings.warn(\n",
    "                    'Streaming mode does not support manual truncation of '\n",
    "                    'stop phrases and your current backend has limited '\n",
    "                    'support for truncation.'\n",
    "                )\n",
    "\n",
    "        # V2 library no longer supports user passing in mock_func. We want to\n",
    "        # remove this from the kwargs we pass to our actual function.\n",
    "        kwargs_func = kwargs.pop('mock_func', None)\n",
    "        if kwargs_func:\n",
    "            raise ValueError(\n",
    "                f'Encountered unexpected mock_func {kwargs_func} with this '\n",
    "                'interface. This was part of the v1 library but is no longer '\n",
    "                'supported.'\n",
    "            )\n",
    "\n",
    "        start_i = kwargs.pop('start_i', 0)\n",
    "        n = kwargs.get('n', 1)\n",
    "        kwargs['prompt'] = prompt\n",
    "        cls._log_query_kwargs(log=log, query_func=query_func, **kwargs)\n",
    "        func_params = params(query_func)\n",
    "        \n",
    "        # Possibly easier for caller to check for errors this way? Mostly a\n",
    "        # holdover from v1 library design, but I'm not 100% sure if the\n",
    "        # benefits still hold given the new design.\n",
    "        try:\n",
    "#             text, full_response = query_func(**kwargs)\n",
    "            if n > 1 and n not in func_params:\n",
    "                del kwargs['n']\n",
    "                # If current query function doesn't natively support multiple\n",
    "                # completions, we can make multiple threaded requests. Need\n",
    "                # to unzip afterwards to regain the (texts, full_responses)\n",
    "                # structure.\n",
    "                response = thread_starmap(query_func, \n",
    "                                          [kwargs for _ in range(n)])\n",
    "                response = list(zip(*response))\n",
    "            else:\n",
    "                response = query_func(**kwargs)\n",
    "        except Exception as e:\n",
    "            raise MockFunctionException(str(e)) from None\n",
    "        if stream:\n",
    "#             if 'stream' in params(query_func):\n",
    "#                 return text, full_response\n",
    "#             # TODO: this isn't yet compatible w/ backends w/ native streaming\n",
    "#             # functionality. Think it should be simple to tweak though since\n",
    "#             # they provide 99% of what I want.\n",
    "#             return stream_multi_response(text, full_response, start_i=start_i)\n",
    "\n",
    "            if 'stream' in func_params:\n",
    "                return response\n",
    "            return stream_multi_response(*response, start_i=start_i)\n",
    "\n",
    "        text, full_response = response\n",
    "        print('text:', text, flush=True)\n",
    "        print('full:', full_response, flush=True)\n",
    "        # Manually check for stop phrases because most backends either don't\n",
    "        # or truncate AFTER the stop phrase which is rarely what we want.\n",
    "        stop = kwargs.get('stop', [])\n",
    "        clean_text = []\n",
    "        # tolist doesn't know how to handle dicts so we check explicitly.\n",
    "        if not listlike(text):\n",
    "            text = tolist(text)\n",
    "            full_response = [full_response]\n",
    "        for text_, resp_ in zip(text, full_response):\n",
    "            text_ = truncate_at_first_stop(\n",
    "                text_,\n",
    "                stop_phrases=stop,\n",
    "                finish_reason=resp_.get('finish_reason', ''),\n",
    "                trunc_full=trunc_full,\n",
    "                trunc_partial=True\n",
    "            )\n",
    "            clean_text.append(strip(text_, strip_output))\n",
    "\n",
    "        return squeeze(clean_text, full_response, n=n)\n",
    "\n",
    "    @classmethod\n",
    "    def _log_query_kwargs(cls, log, query_func=None, **kwargs):\n",
    "        \"\"\"Log kwargs for troubleshooting purposes.\"\"\"\n",
    "        if log:\n",
    "            # Meta key is used to store any info we want to log but that should\n",
    "            # not be passed to the actual query_gpt3 call.\n",
    "            kwargs['meta'] = {\n",
    "                'backend_name': cls.current(),\n",
    "                'query_func': func_name(query_func) if query_func else None\n",
    "            }\n",
    "            with cls.lock:\n",
    "                if not isinstance(log, (str, Path)):\n",
    "                    log = cls.logger.path\n",
    "            \n",
    "            # If log file was deleted, we must recreate it AND use \n",
    "            # change_path to reopen the file object.\n",
    "                if not os.path.exists(log):\n",
    "                    touch(log)\n",
    "                    cls.logger.path = None\n",
    "                if log != cls.logger.path:\n",
    "                    cls.logger.change_path(log)\n",
    "            cls.logger.info(kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{func_name(self)} <current_name: {self.current()}>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:51:27.350488Z",
     "start_time": "2022-04-09T22:51:27.318153Z"
    }
   },
   "outputs": [],
   "source": [
    "@with_signature(query_gpt3)\n",
    "@add_docstring(query_gpt3)\n",
    "def query_batch(prompt, strip_output=True, log=True, **kwargs):\n",
    "    kwargs.update(strip_output=strip_output, log=log)\n",
    "    print('gpt', gpt)\n",
    "    # Setting start_i to i*n ensures that the 'index' returned in streamed\n",
    "    # responses is different for each prompt's completion(s). Otherwise, \n",
    "    # because each query is run separately, each prompt's completion(s) would\n",
    "    # start at 0.\n",
    "    n = kwargs.get('n', 1)\n",
    "    threads = [\n",
    "        ReturningThread(target=GPTBackend.query,\n",
    "                        args=(p,),\n",
    "                        kwargs={**kwargs, 'start_i': i * n})\n",
    "        for i, p in enumerate(prompt)\n",
    "    ]\n",
    "    for thread in threads: thread.start()\n",
    "    res = [thread.join() for thread in threads]\n",
    "    if kwargs.get('stream', False):\n",
    "        return chain(*res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:51:27.594847Z",
     "start_time": "2022-04-09T22:51:27.546972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "gpt GPTBackend <current_name: repeat>\n",
      "text: I AM SO\n",
      "text: TOMORROW IS\n",
      "full: {}\n",
      "full:text: THE COLOR\n",
      " {}\n",
      "full: {}\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'I am so',\n",
    "    'Tomorrow is',\n",
    "    'The color'\n",
    "]\n",
    "gpt = GPTBackend()\n",
    "gpt.switch('repeat')\n",
    "batch_res = query_batch(prompts, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:51:27.759224Z",
     "start_time": "2022-04-09T22:51:27.726467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I AM SO', {}), ('TOMORROW IS', {}), ('THE COLOR', {})]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:51:28.276462Z",
     "start_time": "2022-04-09T22:51:28.227760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt GPTBackend <current_name: repeat>\n",
      "{'prompt': 'I am so', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'prompt': 'Tomorrow is', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "text: I AM SO\n",
      "text:full: {}\n",
      " TOMORROW IS\n",
      "{'prompt': 'The color', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "full:text: {}\n",
      " THE COLOR\n",
      "full: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I AM SO', {}), ('TOMORROW IS', {}), ('THE COLOR', {})]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_batch(prompts, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:51:31.827433Z",
     "start_time": "2022-04-09T22:51:31.668126Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm {GPTBackend.logger.path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:51:31.997203Z",
     "start_time": "2022-04-09T22:51:31.834347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /Users/hmamin/jabberwocky/data/logs/2022.04.09.jsonlines: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat {GPTBackend.logger.path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:51:33.205920Z",
     "start_time": "2022-04-09T22:51:33.149593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt GPTBackend <current_name: repeat>\n",
      "{'n': 3, 'prompt': 'I am so', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 3, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 3, 'prompt': 'The color', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "text: ('I AM SO', 'I AM SO', 'I AM SO')\n",
      "text: ('TOMORROW IS', 'TOMORROW IS', 'TOMORROW IS')\n",
      "full:full: ({}, {}, {})\n",
      "text: ('THE COLOR', 'THE COLOR', 'THE COLOR')\n",
      "full: ({}, {}, {})\n",
      " ({}, {}, {})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['I AM SO', 'I AM SO', 'I AM SO'], ({}, {}, {})),\n",
       " (['TOMORROW IS', 'TOMORROW IS', 'TOMORROW IS'], ({}, {}, {})),\n",
       " (['THE COLOR', 'THE COLOR', 'THE COLOR'], ({}, {}, {}))]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_batch(prompts, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:58:32.665540Z",
     "start_time": "2022-04-09T22:58:01.616505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt GPTBackend <current_name: banana>\n",
      "{'n': 2, 'prompt': 'I am so', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'prompt': 'The color', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-8ad5a252077c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pythonhm/htools/htools/meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1936\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1937\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-04ac1290e01b>\u001b[0m in \u001b[0;36mquery_batch\u001b[0;34m(prompt, strip_output, log, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ]\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-04ac1290e01b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ]\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-17448d5daffd>\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query_batch(prompts, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:20:06.876563Z",
     "start_time": "2022-04-09T21:20:06.830451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt GPTBackend <current_name: repeat>\n",
      "{'n': 1, 'stream': True, 'prompt': 'I am so', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 1, 'stream': True, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:232: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:237: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "INFO:JsonlinesLogger:{'n': 1, 'stream': True, 'prompt': 'I am so', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "INFO:JsonlinesLogger:{'n': 1, 'stream': True, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 1, 'stream': True, 'prompt': 'The color', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:251: UserWarning: Unused kwargs {'stream': True} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "INFO:JsonlinesLogger:{'n': 1, 'stream': True, 'prompt': 'The color', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  {'index': 0, 'finish_reason': None}\n",
      "AM  {'index': 0, 'finish_reason': None}\n",
      "SO  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "TOMORROW  {'index': 1, 'finish_reason': None}\n",
      "IS  {'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "THE  {'index': 2, 'finish_reason': None}\n",
      "COLOR  {'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tok_, full_ in query_batch(prompts, n=1, stream=True):\n",
    "    print(tok_, full_)\n",
    "    if full_['finish_reason']: \n",
    "        print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:20:58.776746Z",
     "start_time": "2022-04-09T21:20:58.740665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt GPTBackend <current_name: repeat>\n",
      "I  {'index': 0, 'finish_reason': None}\n",
      "AM  {'index': 0, 'finish_reason': None}\n",
      "SO  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "I  {'index': 1, 'finish_reason': None}\n",
      "AM  {'index': 1, 'finish_reason': None}\n",
      "SO  {'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "TOMORROW  {'index': 2, 'finish_reason': None}\n",
      "IS  {'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "TOMORROW  {'index': 3, 'finish_reason': None}\n",
      "IS  {'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "THE  {'index': 4, 'finish_reason': None}\n",
      "COLOR  {'index': 4, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "THE  {'index': 5, 'finish_reason': None}\n",
      "COLOR  {'index': 5, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:232: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:237: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n"
     ]
    }
   ],
   "source": [
    "for tok_, full_ in query_batch(prompts, n=2, log=False, stream=True):\n",
    "    print(tok_, full_)\n",
    "    if full_['finish_reason']: \n",
    "        print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:52:17.344831Z",
     "start_time": "2022-04-09T22:52:15.969648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "gpt GPTBackend <current_name: banana>\n",
      "{'max_tokens': 5, 'prompt': 'I am so', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'max_tokens': 5, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'max_tokens': 5, 'prompt': 'The color', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "text:text:   of the stone will be\n",
      " the day we should all\n",
      "full: {'id': '2cc4dc4e-f274-45fd-aa92-c0a3b88a404b', 'message': 'success', 'created': 1649544737, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of the stone will be', 'input': 'The color'}]}\n",
      "full: {'id': 'd701b44c-06d3-47bd-aa8d-5d60e1aab5a4', 'message': 'success', 'created': 1649544736, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the day we should all', 'input': 'Tomorrow is'}]}\n",
      "text:  sorry.\" \"You have\n",
      "full: {'id': 'be8e07d3-ee96-4376-aaf5-27e8e10e030f', 'message': 'success', 'created': 1649544737, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' sorry.\" \"You have', 'input': 'I am so'}]}\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('banana'):\n",
    "    banana_batch_res = query_batch(prompts, max_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:52:19.575618Z",
     "start_time": "2022-04-09T22:52:19.545342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sorry.\" \"You have',\n",
       "  {'id': 'be8e07d3-ee96-4376-aaf5-27e8e10e030f',\n",
       "   'message': 'success',\n",
       "   'created': 1649544737,\n",
       "   'apiVersion': '26 Nov 2021',\n",
       "   'modelOutputs': [{'output': ' sorry.\" \"You have', 'input': 'I am so'}]}),\n",
       " ('the day we should all',\n",
       "  {'id': 'd701b44c-06d3-47bd-aa8d-5d60e1aab5a4',\n",
       "   'message': 'success',\n",
       "   'created': 1649544736,\n",
       "   'apiVersion': '26 Nov 2021',\n",
       "   'modelOutputs': [{'output': ' the day we should all',\n",
       "     'input': 'Tomorrow is'}]}),\n",
       " ('of the stone will be',\n",
       "  {'id': '2cc4dc4e-f274-45fd-aa92-c0a3b88a404b',\n",
       "   'message': 'success',\n",
       "   'created': 1649544737,\n",
       "   'apiVersion': '26 Nov 2021',\n",
       "   'modelOutputs': [{'output': ' of the stone will be',\n",
       "     'input': 'The color'}]})]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_batch_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:52:27.157637Z",
     "start_time": "2022-04-09T22:52:25.192411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "gpt GPTBackend <current_name: banana>\n",
      "{'n': 2, 'max_tokens': 5, 'stream': True, 'prompt': 'The color', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'max_tokens': 5, 'stream': True, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'max_tokens': 5, 'stream': True, 'prompt': 'I am so', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:237: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:232: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:371: UserWarning: query_gpt_banana received unused kwargs {'stream': True}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  {'index': 0, 'finish_reason': None}\n",
      "happy  {'index': 0, 'finish_reason': None}\n",
      "I  {'index': 0, 'finish_reason': None}\n",
      "found  {'index': 0, 'finish_reason': None}\n",
      "this  {'index': 0, 'finish_reason': None}\n",
      "forum  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 1, 'finish_reason': None}\n",
      "happy  {'index': 1, 'finish_reason': None}\n",
      "I  {'index': 1, 'finish_reason': None}\n",
      "found  {'index': 1, 'finish_reason': None}\n",
      "this  {'index': 1, 'finish_reason': None}\n",
      "article  {'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 2, 'finish_reason': None}\n",
      "the  {'index': 2, 'finish_reason': None}\n",
      "last  {'index': 2, 'finish_reason': None}\n",
      "day  {'index': 2, 'finish_reason': None}\n",
      "of  {'index': 2, 'finish_reason': None}\n",
      "my  {'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 3, 'finish_reason': None}\n",
      "the  {'index': 3, 'finish_reason': None}\n",
      "day  {'index': 3, 'finish_reason': None}\n",
      "of  {'index': 3, 'finish_reason': None}\n",
      "the  {'index': 3, 'finish_reason': None}\n",
      "annual  {'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 4, 'finish_reason': None}\n",
      "of  {'index': 4, 'finish_reason': None}\n",
      "a  {'index': 4, 'finish_reason': None}\n",
      "peacock�  {'index': 4, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 5, 'finish_reason': None}\n",
      "of  {'index': 5, 'finish_reason': None}\n",
      "this  {'index': 5, 'finish_reason': None}\n",
      "fabric  {'index': 5, 'finish_reason': None}\n",
      "is  {'index': 5, 'finish_reason': None}\n",
      "so  {'index': 5, 'finish_reason': 'dummy'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('banana')\n",
    "for tok_, full_ in query_batch(prompts, n=2, max_tokens=5, stream=True):\n",
    "    print(tok_, select(full_, keep=['index', 'finish_reason']))\n",
    "    if full_['finish_reason']: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T20:55:04.958881Z",
     "start_time": "2022-04-09T20:55:04.898385Z"
    }
   },
   "outputs": [],
   "source": [
    "chunks = _open_res\n",
    "texts = (chunk['choices'][0]['text'] for chunk in chunks)\n",
    "chunks = (dict(chunk['choices'][0]) for chunk in chunks)\n",
    "open_res = list(zip(texts, chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T20:55:05.432293Z",
     "start_time": "2022-04-09T20:55:05.387360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Feeling', {'finish_reason': None, 'index': 0, 'logprobs': {'text_offset': [24], 'token_logprobs': [-8.621929], 'tokens': [' Feeling'], 'top_logprobs': [{'\\n': -2.4186804, ' I': -2.6238666, ' She': -2.4244554}]}, 'text': ' Feeling'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' her', {'finish_reason': None, 'index': 0, 'logprobs': {'text_offset': [32], 'token_logprobs': [-5.186215], 'tokens': [' her'], 'top_logprobs': [{' a': -2.1202018, ' like': -2.6433406, ' the': -2.8912876}]}, 'text': ' her'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' I', {'finish_reason': None, 'index': 1, 'logprobs': {'text_offset': [24], 'token_logprobs': [-2.6238666], 'tokens': [' I'], 'top_logprobs': [{'\\n': -2.4186804, ' I': -2.6238666, ' She': -2.4244554}]}, 'text': ' I'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(\"'m\", {'finish_reason': None, 'index': 1, 'logprobs': {'text_offset': [26], 'token_logprobs': [-2.2295423], 'tokens': [\"'m\"], 'top_logprobs': [{' don': -2.8276584, \"'m\": -2.2295423, 'bytes:\\\\xe2\\\\x80': -2.7309577}]}, 'text': \"'m\"})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' sure', {'finish_reason': None, 'index': 1, 'logprobs': {'text_offset': [28], 'token_logprobs': [-2.6193585], 'tokens': [' sure'], 'top_logprobs': [{' going': -1.7995286, ' not': -2.3888402, ' sure': -2.6193585}]}, 'text': ' sure'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' presence', {'finish_reason': None, 'index': 0, 'logprobs': {'text_offset': [36], 'token_logprobs': [-3.293669], 'tokens': [' presence'], 'top_logprobs': [{' up': -2.2750113, ' way': -2.5486472, ',': -3.0275166}]}, 'text': ' presence'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' you', {'finish_reason': None, 'index': 1, 'logprobs': {'text_offset': [33], 'token_logprobs': [-1.7617589], 'tokens': [' you'], 'top_logprobs': [{' he': -2.186368, ' she': -1.4318895, ' you': -1.7617589}]}, 'text': ' you'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(',', {'finish_reason': None, 'index': 0, 'logprobs': {'text_offset': [45], 'token_logprobs': [-1.0045006], 'tokens': [','], 'top_logprobs': [{' in': -2.5863404, ' is': -2.647669, ',': -1.0045006}]}, 'text': ','})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' Des', {'finish_reason': 'length', 'index': 0, 'logprobs': {'text_offset': [46], 'token_logprobs': [-8.532746], 'tokens': [' Des'], 'top_logprobs': [{' I': -1.8522211, ' he': -2.4737713, ' the': -2.7948432}]}, 'text': ' Des'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' can', {'finish_reason': 'length', 'index': 1, 'logprobs': {'text_offset': [37], 'token_logprobs': [-2.7016406], 'tokens': [' can'], 'top_logprobs': [{\"'ll\": -1.5801843, \"'re\": -2.1810775, \"'ve\": -1.8903823}]}, 'text': ' can'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in open_res:\n",
    "    print(row)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:22.757142Z",
     "start_time": "2022-04-09T22:31:22.726046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to data/tmp/_goose_res_multi.pkl.\n",
      "Writing data to data/tmp/_goose_res.pkl.\n",
      "Writing data to data/tmp/_open_res.pkl.\n"
     ]
    }
   ],
   "source": [
    "save(_goose_res_multi, 'data/tmp/_goose_res_multi.pkl')\n",
    "save(_goose_res, 'data/tmp/_goose_res.pkl')\n",
    "save(_open_res, 'data/tmp/_open_res.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
