{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Experiment with ways to support passing a list of prompts to query method. Some backends don't support this natively, others do, but none automatically would return the format I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:04.596114Z",
     "start_time": "2022-04-03T21:44:04.554296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:24:18.742364Z",
     "start_time": "2022-04-04T00:24:18.692405Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.openai_utils import load_prompt, load_openai_api_key, \\\n",
    "    GPTBackend\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:07.044449Z",
     "start_time": "2022-04-03T21:44:06.993938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: make thread that returns value so we can run a separate query for each thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:07.107710Z",
     "start_time": "2022-04-03T21:44:07.060715Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class ReturningThread(Thread):\n",
    "\n",
    "    @add_docstring(Thread)\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs=None, *, daemon=None):\n",
    "        \"\"\"This is identical to a regular thread except that the join method\n",
    "        returns the value returned by your target function. The\n",
    "        Thread.__init__ docstring is shown below for the sake of convenience.\n",
    "        \"\"\"\n",
    "        super().__init__(group=group, target=target, name=name,\n",
    "                         args=args, kwargs=kwargs, daemon=daemon)\n",
    "        self.result = None\n",
    "\n",
    "    def run(self):\n",
    "        self.result = self._target(*self._args, **self._kwargs)\n",
    "\n",
    "    def join(self, timeout=None):\n",
    "        super().join(timeout)\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:07.134949Z",
     "start_time": "2022-04-03T21:44:07.110262Z"
    }
   },
   "outputs": [],
   "source": [
    "def foo(x, wait=2):\n",
    "    time.sleep(wait)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:22:18.048518Z",
     "start_time": "2022-04-03T20:22:07.985576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns values but is slow (sync execution).\n",
    "res = [foo(i) for i in range(5)]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:21:52.630627Z",
     "start_time": "2022-04-03T20:21:50.591510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [Thread(target=foo, args=(i,)) for i in range(5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# Regular thread returns None.\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:25:38.697676Z",
     "start_time": "2022-04-03T20:25:36.653887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [ReturningThread(target=foo, args=(i,)) for i in range(5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# ReturningThread returns values!\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try integrating into GPTBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:43.076560Z",
     "start_time": "2022-04-03T21:44:43.036655Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: no guarantees these threads return in the right order, though, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:43.255226Z",
     "start_time": "2022-04-03T21:44:43.214873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base: https://api.openai.com\n",
      "Key: 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\n",
      "Query func: <function query_gpt3 at 0x1237e7378>\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTBackend()\n",
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:43.646510Z",
     "start_time": "2022-04-03T21:44:43.597458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Key: VMDTuFkyIsfUlSqJOCoQPslfbhNOIYroqF\n",
      "Query func: <function query_gpt_huggingface at 0x1237e7268>\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('huggingface')\n",
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:44.127787Z",
     "start_time": "2022-04-03T21:44:44.092817Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Six million years after the pandemic,',\n",
    "    'The stegosaurus'\n",
    "]\n",
    "kwargs = {'max_tokens': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:34:33.358120Z",
     "start_time": "2022-04-03T20:34:32.812552Z"
    }
   },
   "outputs": [],
   "source": [
    "threads = [ReturningThread(target=gpt.query, args=(prompt,), kwargs=kwargs) \n",
    "           for prompt in prompts]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "res = [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:34:35.086435Z",
     "start_time": "2022-04-03T20:34:35.039987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the world is still in the grip of a global',\n",
       "  {'generated_text': ' the world is still in the grip of a global'}),\n",
       " ('is a large, large, and highly intelligent animal',\n",
       "  {'generated_text': ' is a large, large, and highly intelligent animal'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:36:21.349540Z",
     "start_time": "2022-04-03T20:36:16.550197Z"
    }
   },
   "outputs": [],
   "source": [
    "threads = [ReturningThread(target=gpt.query,\n",
    "                           args=(prompt,), \n",
    "                           kwargs={**kwargs, 'n': 3, \n",
    "                                   'logprobs': 4, 'engine_i': 1}) \n",
    "           for prompt in prompts]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "res = [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:36:39.231035Z",
     "start_time": "2022-04-03T20:36:39.180639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmap(len, *res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:36:48.181801Z",
     "start_time": "2022-04-03T20:36:48.133726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the population of New York City is poised to rise',\n",
       " 'the pandemic strain of influenza spreads and mutates',\n",
       " 'our species is still struggling to deal with the effects']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:05.079974Z",
     "start_time": "2022-04-03T20:37:05.029095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' the population of New York City is poised to rise'},\n",
       " {'generated_text': ' the pandemic strain of influenza spreads and mutates'},\n",
       " {'generated_text': ' our species is still struggling to deal with the effects'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:29.152724Z",
     "start_time": "2022-04-03T20:37:29.105080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is one of the more remarkable prehistoric dinosaurs, and',\n",
       " ', or giant pterosaur from the late',\n",
       " 'fossil, or dinosaur\\nFossil bones of']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:31.718185Z",
     "start_time": "2022-04-03T20:37:31.646264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' is one of the more remarkable prehistoric dinosaurs, and'},\n",
       " {'generated_text': ', or giant pterosaur from the late'},\n",
       " {'generated_text': ' fossil, or dinosaur\\nFossil bones of'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:49.865248Z",
     "start_time": "2022-04-03T20:37:49.805786Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Key: sk-QtTMOXuKKuewX8khBHcoCGhzge9GvOpLxdHCmOjpCqCGNVD4\n",
      "Query func: <function query_gpt3 at 0x125238bf8>\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('gooseai')\n",
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:38:56.466544Z",
     "start_time": "2022-04-03T20:38:54.624773Z"
    }
   },
   "outputs": [],
   "source": [
    "threads = [ReturningThread(target=gpt.query,\n",
    "                           args=(prompt,), \n",
    "                           kwargs={'max_tokens': 8, 'n': 2, \n",
    "                                   'logprobs': 5, 'engine_i': 0}) \n",
    "           for prompt in prompts]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "res = [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:40:56.897362Z",
     "start_time": "2022-04-03T20:40:56.848457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:39:06.290772Z",
     "start_time": "2022-04-03T20:39:06.258358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the world is still in the grip of',\n",
       " 'scientists still do not know whether humans are']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:39:12.866153Z",
     "start_time": "2022-04-03T20:39:12.834078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is a fossilized dinosaur named by the',\n",
       " 'is a famous carnivorous dinosaur from the']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:41:15.524928Z",
     "start_time": "2022-04-03T20:41:15.477253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[0])#[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:52:30.767721Z",
     "start_time": "2022-04-03T20:52:30.718958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the world is still in the grip of',\n",
       "  'scientists still do not know whether humans are'],\n",
       " ['is a fossilized dinosaur named by the',\n",
       "  'is a famous carnivorous dinosaur from the'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, resps = list(zip(*res))\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:53:26.283690Z",
     "start_time": "2022-04-03T20:53:26.244193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' the', ' world', ' is', ' still', ' in', ' the', ' grip', ' of'],\n",
       " [' scientists',\n",
       "  ' still',\n",
       "  ' do',\n",
       "  ' not',\n",
       "  ' know',\n",
       "  ' whether',\n",
       "  ' humans',\n",
       "  ' are']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resps[i][j] corresponds to prompt i, completion j.\n",
    "[completion['logprobs'].tokens for completion in resps[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:53:35.756499Z",
     "start_time": "2022-04-03T20:53:35.701049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' is', ' a', ' fossil', 'ized', ' dinosaur', ' named', ' by', ' the'],\n",
       " [' is', ' a', ' famous', ' carniv', 'orous', ' dinosaur', ' from', ' the']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[completion['logprobs'].tokens for completion in resps[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:56:00.807333Z",
     "start_time": "2022-04-03T20:56:00.775365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Key: sk-QtTMOXuKKuewX8khBHcoCGhzge9GvOpLxdHCmOjpCqCGNVD4\n",
      "Query func: <function query_gpt3 at 0x125238bf8>\n"
     ]
    }
   ],
   "source": [
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:56:40.049508Z",
     "start_time": "2022-04-03T20:56:39.320533Z"
    }
   },
   "outputs": [],
   "source": [
    "threads2 = [ReturningThread(target=gpt.query,\n",
    "                           args=(prompt,), \n",
    "                           kwargs={'max_tokens': 8, 'n': 1, \n",
    "                                   'logprobs': 5, 'engine_i': 0}) \n",
    "            for prompt in prompts]\n",
    "for thread in threads2:\n",
    "    thread.start()\n",
    "res2 = [thread.join() for thread in threads2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:57:26.768849Z",
     "start_time": "2022-04-03T20:57:26.725952Z"
    }
   },
   "outputs": [],
   "source": [
    "texts2, resps2 = list(zip(*res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:57:29.637473Z",
     "start_time": "2022-04-03T20:57:29.585330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the future of the world’s', 'The stegosaurus (Ste')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:59:01.385365Z",
     "start_time": "2022-04-03T20:59:01.343301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'length',\n",
       " 'index': 0,\n",
       " 'logprobs': <OpenAIObject at 0x125e93d58> JSON: {\n",
       "   \"text_offset\": [\n",
       "     0,\n",
       "     4,\n",
       "     11,\n",
       "     14,\n",
       "     18,\n",
       "     24,\n",
       "     24,\n",
       "     25\n",
       "   ],\n",
       "   \"token_logprobs\": [\n",
       "     -1.7412109375,\n",
       "     -5.03515625,\n",
       "     -0.361083984375,\n",
       "     -1.7802734375,\n",
       "     -1.8515625,\n",
       "     -1.89453125,\n",
       "     -0.0006651878356933594,\n",
       "     -0.00013065338134765625\n",
       "   ],\n",
       "   \"tokens\": [\n",
       "     \" the\",\n",
       "     \" future\",\n",
       "     \" of\",\n",
       "     \" the\",\n",
       "     \" world\",\n",
       "     \"\\ufffd\",\n",
       "     \"\\ufffd\",\n",
       "     \"s\"\n",
       "   ],\n",
       "   \"top_logprobs\": [\n",
       "     {\n",
       "       \" a\": -2.90234375,\n",
       "       \" it\": -3.947265625,\n",
       "       \" scientists\": -4.09375,\n",
       "       \" the\": -1.7412109375,\n",
       "       \" we\": -2.748046875\n",
       "     },\n",
       "     {\n",
       "       \" city\": -4.51171875,\n",
       "       \" human\": -4.06640625,\n",
       "       \" pand\": -4.2109375,\n",
       "       \" virus\": -2.9921875,\n",
       "       \" world\": -1.6845703125\n",
       "     },\n",
       "     {\n",
       "       \" is\": -2.072265625,\n",
       "       \" looks\": -3.4296875,\n",
       "       \" of\": -0.361083984375,\n",
       "       \" remains\": -3.44921875,\n",
       "       \" still\": -4.171875\n",
       "     },\n",
       "     {\n",
       "       \" CO\": -3.35546875,\n",
       "       \" global\": -3.57421875,\n",
       "       \" infectious\": -3.62890625,\n",
       "       \" pand\": -3.509765625,\n",
       "       \" the\": -1.7802734375\n",
       "     },\n",
       "     {\n",
       "       \" global\": -3.478515625,\n",
       "       \" human\": -2.720703125,\n",
       "       \" pand\": -3.0625,\n",
       "       \" virus\": -2.365234375,\n",
       "       \" world\": -1.8515625\n",
       "     },\n",
       "     {\n",
       "       \" is\": -1.21484375,\n",
       "       \" looks\": -3.654296875,\n",
       "       \" remains\": -3.19140625,\n",
       "       \"'s\": -1.814453125,\n",
       "       \"\\ufffd\": -1.89453125\n",
       "     },\n",
       "     {\n",
       "       \"\\ufffd\": -10.5546875\n",
       "     },\n",
       "     {\n",
       "       \" s\": -11.3671875,\n",
       "       \"bytes:'\\\\n'\": -11.8671875,\n",
       "       \"ll\": -12.3125,\n",
       "       \"s\": -0.00013065338134765625,\n",
       "       \"\\ufffd\": -11.3125\n",
       "     }\n",
       "   ]\n",
       " },\n",
       " 'text': ' the future of the world’s',\n",
       " 'token_index': 0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because only 1 completion per prompt, resps is a dict instead of a list of \n",
    "# dicts.\n",
    "resps2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:06:01.902403Z",
     "start_time": "2022-04-03T22:06:01.416816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    hf_res = gpt.query('I want', engine_i=1, max_tokens=5, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:07:19.804315Z",
     "start_time": "2022-04-04T00:07:19.753125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results(text=['to give you one big', 'to show you some pictures'], full=[{'generated_text': ' to give you one big'}, {'generated_text': ' to show you some pictures'}])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results(text=hf_res[0], full=hf_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better interface\n",
    "# texts, full_resps = gpt.query([p1, p2, p3], n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test streaming mode\n",
    "\n",
    "Need a better understanding of what using streaming mode is like before I decide about streaming interface for np or nc > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:48:13.557001Z",
     "start_time": "2022-04-03T21:48:13.505444Z"
    }
   },
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "from jabberwocky.openai_utils import query_gpt3, query_gpt_huggingface, \\\n",
    "    query_gpt_banana, query_gpt_j, query_gpt_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:55:44.929746Z",
     "start_time": "2022-04-03T21:55:44.886107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Was toying with idea of adding this to gpt.query warnings to make the \n",
    "# messages unique, in the hope that this would ensure they're always shown\n",
    "# rather than just once. But a. I'm not sure if that's how they define \n",
    "# duplicates, and b. I'm seeing code defined in nb seems to always show \n",
    "# warnings, not just once, so I'm not sure what to make of that. Still \n",
    "# eventually want to write a func like this (maybe moreso for creating new\n",
    "# file paths when encountering collisions) but that should have a more limited\n",
    "# set of possible characters.\n",
    "def random_str(length, lower=True):\n",
    "    rand = b64encode(os.urandom(length)).decode()[:length]\n",
    "    return rand.lower() if lower else rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:52:09.902337Z",
     "start_time": "2022-04-03T21:52:09.851203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 3\n",
      "2 l6\n",
      "3 oym\n",
      "4 jo/0\n",
      "5 xqztr\n",
      "6 4mtzog\n",
      "7 ge5korg\n",
      "8 660d5aiz\n",
      "9 fcdkd1iyz\n",
      "10 z4t+jhlyb5\n",
      "11 shkuswnafmu\n",
      "12 nqgpfuek1/r1\n",
      "13 hwn6vlcf1v54n\n",
      "14 bptgfd0uds2s13\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    rand = random_str(i)\n",
    "    print(i, rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:47:08.510710Z",
     "start_time": "2022-04-03T21:47:08.460882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'P\\xe3\\x87#\\x99i\\xc4\\x88d\\x92'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.urandom(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:50.138844Z",
     "start_time": "2022-04-03T21:44:50.090175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:921: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn('strip_output=True is not supported in stream '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:926: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'Streaming mode does not support manual truncation of '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:391: UserWarning: Unused kwargs {'max_tokens': 5, 'stream': True} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    repeat_res = gpt.query('I want', max_tokens=5, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:57.931085Z",
     "start_time": "2022-04-03T21:44:57.876182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    repeat_res = gpt.query('I want', max_tokens=5, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:26:59.003604Z",
     "start_time": "2022-04-03T21:26:58.955802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I want', {})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:34:33.441850Z",
     "start_time": "2022-04-03T21:34:32.520598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:921: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn(\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:926: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend's has limited support for truncation.\n",
      "  \n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:677: UserWarning: query_gpt_banana received unused kwargs {'stream': True}.\n",
      "  params = {\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching  backend back to \"gooseai\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('banana'):\n",
    "    banana_res = gpt.query('I want', max_tokens=5, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:37:55.051099Z",
     "start_time": "2022-04-03T21:37:54.998748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason'])]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.choices[0].keys() for row in load(C.mock_stream_paths[True])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gpt('gooseai'):\n",
    "    query_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with streaming text AND dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:59:49.524731Z",
     "start_time": "2022-04-03T21:59:49.460425Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:59:49.743074Z",
     "start_time": "2022-04-03T21:59:49.703100Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream_words(text):\n",
    "    \"\"\"Like stream_chars but splits on spaces. Realized stream_chars was a bad\n",
    "    idea because we risk giving SPEAKER turns like\n",
    "    \"This is over. W\" and \"hat are you doing next?\", neither of which would be\n",
    "    pronounced as intended. We yield with a space for consistency with the\n",
    "    other streaming interfaces which require no further postprocessing.\n",
    "    \"\"\"\n",
    "    for word in text.split(' '):\n",
    "        yield word + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:04:26.687623Z",
     "start_time": "2022-04-03T22:04:26.644199Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream_response(text:str, full:dict):\n",
    "    yield from zip(stream_words(text), cycle([full]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T01:08:39.384584Z",
     "start_time": "2022-04-04T01:08:39.339719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: this is probably massively over-engineered for mock streaming, but \n",
    "# I'll need to do something like this if I want to support real streaming \n",
    "# where nc and/or np > 1 so it was probably useful to work through this logic\n",
    "# anyway.\n",
    "def stream_multi_response(texts:list, fulls:list):\n",
    "    for i, (text, full) in enumerate(zip(texts, fulls)):\n",
    "        queue = deque()\n",
    "        gen = stream_response(text, \n",
    "                              {**full, 'index': i, 'finish_reason': None})\n",
    "        done = False\n",
    "        # Yield items while checking if we're at the last item so we can mark\n",
    "        # it with a finish_reason. This lets us know when one completion ends.\n",
    "        while True:\n",
    "            try:\n",
    "                tok, tok_full = next(gen)\n",
    "                queue.append((tok, tok_full))\n",
    "            except StopIteration:\n",
    "                done = True\n",
    "            \n",
    "            while len(queue) > 1:\n",
    "                tok, tok_full = queue.popleft()\n",
    "                yield tok, tok_full\n",
    "            if done: break\n",
    "        tok, tok_full = queue.popleft()\n",
    "        tok_full['finish_reason'] = 'dummy'    \n",
    "        yield tok, tok_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:14:23.331850Z",
     "start_time": "2022-04-04T00:14:23.291088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Santa '\n",
      "'is '\n",
      "'coming '\n",
      "'to '\n",
      "'town. '\n"
     ]
    }
   ],
   "source": [
    "txt = 'Santa is coming to town.'\n",
    "for tok in stream_words(txt):\n",
    "    print(repr(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:04:27.262860Z",
     "start_time": "2022-04-03T22:04:27.226365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Santa ' {}\n",
      "'is ' {}\n",
      "'coming ' {}\n",
      "'to ' {}\n",
      "'town. ' {}\n"
     ]
    }
   ],
   "source": [
    "for tok, full in stream_response(txt, {}):\n",
    "    print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:06:01.902403Z",
     "start_time": "2022-04-03T22:06:01.416816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1\n",
    "with gpt('huggingface'):\n",
    "    hf_res = gpt.query('I want', engine_i=1, max_tokens=5, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:41:42.969933Z",
     "start_time": "2022-04-04T00:41:42.927011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['to give you one big', 'to show you some pictures'],\n",
       " [{'generated_text': ' to give you one big'},\n",
       "  {'generated_text': ' to show you some pictures'}])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T01:07:11.068161Z",
     "start_time": "2022-04-04T01:07:11.032468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>  to  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  give  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  you  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  one  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  big  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': 'dummy'}\n",
      ">>>  to  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  show  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  you  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  some  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  pictures  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': 'dummy'}\n"
     ]
    }
   ],
   "source": [
    "for tok, full in stream_multi_response(*hf_res):\n",
    "    print('>>> ', tok, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:48:32.545520Z",
     "start_time": "2022-04-03T22:48:32.040038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1, stream=True\n",
    "with gpt('gooseai'):\n",
    "    goose_res = openai.Completion.create(\n",
    "        prompt=txt,\n",
    "        engine=GPTBackend.engine(0),\n",
    "        max_tokens=5,\n",
    "        logprobs=3,\n",
    "        n=2,\n",
    "        stream=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:26.675931Z",
     "start_time": "2022-04-03T22:53:25.148802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"openai\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1, stream=True\n",
    "with gpt('openai'):\n",
    "    open_res = openai.Completion.create(\n",
    "        prompt=txt,\n",
    "        engine=GPTBackend.engine(0),\n",
    "        max_tokens=5,\n",
    "        logprobs=3,\n",
    "        n=2,\n",
    "        stream=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:49:59.973627Z",
     "start_time": "2022-04-03T22:49:59.929035Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          0\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -3.80078125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" In\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" And\": -2.826171875,\n",
      "            \" The\": -3.03125,\n",
      "            \"bytes:'\\\\n'\": -1.30859375\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" In\",\n",
      "      \"token_index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"01184be1-0c83-4906-a88c-9b71165a0da0\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          3\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -5.8125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" mid\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" a\": -2.478515625,\n",
      "            \" fact\": -1.5859375,\n",
      "            \" the\": -1.8759765625\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" mid\",\n",
      "      \"token_index\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"01184be1-0c83-4906-a88c-9b71165a0da0\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          7\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -0.10931396484375\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \"-\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" December\": -4.82421875,\n",
      "            \"-\": -0.10931396484375,\n",
      "            \"town\": -4.45703125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \"-\",\n",
      "      \"token_index\": 2\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"01184be1-0c83-4906-a88c-9b71165a0da0\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          8\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.484375\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \"January\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"December\": -1.9091796875,\n",
      "            \"January\": -2.484375,\n",
      "            \"November\": -1.9287109375\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \"January\",\n",
      "      \"token_index\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"01184be1-0c83-4906-a88c-9b71165a0da0\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          15\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -0.260986328125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \",\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" the\": -3.923828125,\n",
      "            \" we\": -4.4140625,\n",
      "            \",\": -0.260986328125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \",\",\n",
      "      \"token_index\": 4\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"01184be1-0c83-4906-a88c-9b71165a0da0\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          0\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -5.015625\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" There\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" And\": -2.826171875,\n",
      "            \" The\": -3.03125,\n",
      "            \"bytes:'\\\\n'\": -1.30859375\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" There\",\n",
      "      \"token_index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"30f54170-32a8-4201-bbf2-81168512c53e\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          6\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.103515625\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" is\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" are\": -1.486328125,\n",
      "            \"'s\": -1.650390625,\n",
      "            \"\\ufffd\": -1.5693359375\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" is\",\n",
      "      \"token_index\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"30f54170-32a8-4201-bbf2-81168512c53e\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          9\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -4.5703125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" not\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" a\": -1.169921875,\n",
      "            \" no\": -1.3955078125,\n",
      "            \" nothing\": -2.51171875\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" not\",\n",
      "      \"token_index\": 2\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"30f54170-32a8-4201-bbf2-81168512c53e\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          13\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -0.80517578125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" a\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" a\": -0.80517578125,\n",
      "            \" much\": -1.2998046875,\n",
      "            \" one\": -2.66015625\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" a\",\n",
      "      \"token_index\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"30f54170-32a8-4201-bbf2-81168512c53e\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          15\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -3.89453125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Santa\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" better\": -2.923828125,\n",
      "            \" day\": -2.685546875,\n",
      "            \" single\": -1.6318359375\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Santa\",\n",
      "      \"token_index\": 4\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026112,\n",
      "  \"id\": \"30f54170-32a8-4201-bbf2-81168512c53e\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_goose_res = []\n",
    "for obj in goose_res:\n",
    "    print(obj)\n",
    "    _goose_res.append(obj)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:42.976851Z",
     "start_time": "2022-04-03T22:53:42.924028Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          24\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -8.621929\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Feeling\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"\\n\": -2.4186804,\n",
      "            \" I\": -2.6238666,\n",
      "            \" She\": -2.4244554\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Feeling\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          32\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -5.186215\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" her\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" a\": -2.1202018,\n",
      "            \" like\": -2.6433406,\n",
      "            \" the\": -2.8912876\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" her\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          24\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.6238666\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" I\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"\\n\": -2.4186804,\n",
      "            \" I\": -2.6238666,\n",
      "            \" She\": -2.4244554\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" I\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          26\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.2295423\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \"'m\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" don\": -2.8276584,\n",
      "            \"'m\": -2.2295423,\n",
      "            \"bytes:\\\\xe2\\\\x80\": -2.7309577\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \"'m\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          28\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.6193585\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" sure\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" going\": -1.7995286,\n",
      "            \" not\": -2.3888402,\n",
      "            \" sure\": -2.6193585\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" sure\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          36\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -3.293669\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" presence\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" up\": -2.2750113,\n",
      "            \" way\": -2.5486472,\n",
      "            \",\": -3.0275166\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" presence\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          33\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -1.7617589\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" you\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" he\": -2.186368,\n",
      "            \" she\": -1.4318895,\n",
      "            \" you\": -1.7617589\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" you\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          45\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -1.0045006\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \",\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" in\": -2.5863404,\n",
      "            \" is\": -2.647669,\n",
      "            \",\": -1.0045006\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \",\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          46\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -8.532746\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Des\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" I\": -1.8522211,\n",
      "            \" he\": -2.4737713,\n",
      "            \" the\": -2.7948432\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Des\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          37\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.7016406\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" can\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"'ll\": -1.5801843,\n",
      "            \"'re\": -2.1810775,\n",
      "            \"'ve\": -1.8903823\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" can\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_open_res = []\n",
    "for obj in open_res:\n",
    "    print(obj)\n",
    "    _open_res.append(obj)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:48.880654Z",
     "start_time": "2022-04-03T22:53:48.844199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_goose_res), len(_open_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:52:49.131144Z",
     "start_time": "2022-04-03T22:52:49.083892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([' In'], None),\n",
       " ([' mid'], None),\n",
       " (['-'], None),\n",
       " (['January'], None),\n",
       " ([','], 'length'),\n",
       " ([' There'], None),\n",
       " ([' is'], None),\n",
       " ([' not'], None),\n",
       " ([' a'], None),\n",
       " ([' Santa'], 'length')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(row.choices[0].logprobs.tokens, row.choices[0].finish_reason) \n",
    " for row in _goose_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:56.046309Z",
     "start_time": "2022-04-03T22:53:55.996554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([' Feeling'], None),\n",
       " ([' her'], None),\n",
       " ([' I'], None),\n",
       " ([\"'m\"], None),\n",
       " ([' sure'], None),\n",
       " ([' presence'], None),\n",
       " ([' you'], None),\n",
       " ([','], None),\n",
       " ([' Des'], 'length'),\n",
       " ([' can'], 'length')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(row.choices[0].logprobs.tokens, row.choices[0].finish_reason) \n",
    " for row in _open_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:54:26.430884Z",
     "start_time": "2022-04-03T22:54:26.374792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Feeling',\n",
       " ' her',\n",
       " ' I',\n",
       " \"'m\",\n",
       " ' sure',\n",
       " ' presence',\n",
       " ' you',\n",
       " ',',\n",
       " ' Des',\n",
       " ' can']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.choices[0].text for row in _open_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:55:09.775032Z",
     "start_time": "2022-04-03T23:55:09.726986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thought we might be able to use id to reconstruct each completion but that\n",
    "# doesn't work.\n",
    "[row['id'] for row in _open_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:57:34.622597Z",
     "start_time": "2022-04-03T23:57:34.587532Z"
    }
   },
   "outputs": [],
   "source": [
    "# index points to which completion each new token belongs to.\n",
    "completions = defaultdict(list)\n",
    "for row in _open_res:\n",
    "    completions[row['choices'][0]['index']].append(row['choices'][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:57:37.089716Z",
     "start_time": "2022-04-03T23:57:37.038623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [' Feeling', ' her', ' presence', ',', ' Des'],\n",
       "             1: [' I', \"'m\", ' sure', ' you', ' can']})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:02:18.244513Z",
     "start_time": "2022-04-04T00:02:18.199128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None\n",
      "0 None\n",
      "0 None\n",
      "0 None\n",
      "0 length\n",
      "1 None\n",
      "1 None\n",
      "1 None\n",
      "1 None\n",
      "1 length\n"
     ]
    }
   ],
   "source": [
    "# index points to which completion each new token belongs to.\n",
    "# completions = defaultdict(list)\n",
    "for row in _goose_res:\n",
    "    print(row['choices'][0]['index'], row['choices'][0]['finish_reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:02:34.306496Z",
     "start_time": "2022-04-04T00:02:34.265497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None\n",
      "0 None\n",
      "1 None\n",
      "1 None\n",
      "1 None\n",
      "0 None\n",
      "1 None\n",
      "0 None\n",
      "0 length\n",
      "1 length\n"
     ]
    }
   ],
   "source": [
    "# index points to which completion each new token belongs to.\n",
    "# completions = defaultdict(list)\n",
    "for row in _open_res:\n",
    "    print(row['choices'][0]['index'], row['choices'][0]['finish_reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:05:31.960786Z",
     "start_time": "2022-04-04T00:05:31.568056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Switching openai backend to \"repeat\".\n",
      "stream=False\n",
      " ('Santa is coming to town.', {})\n",
      "\n",
      "stream=True\n",
      "'Santa ' {}\n",
      "'is ' {}\n",
      "'coming ' {}\n",
      "'to ' {}\n",
      "'town. ' {}\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:923: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn('strip_output=True is not supported in stream '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:928: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'Streaming mode does not support manual truncation of '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:391: UserWarning: Unused kwargs {'stream': True} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    print('stream=False\\n', gpt.query(txt))\n",
    "    print('\\nstream=True')\n",
    "    for tok, full in gpt.query(txt, stream=True):\n",
    "        print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:05:48.562505Z",
     "start_time": "2022-04-04T00:05:48.530971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "stream=False\n",
      " (['Santa is coming to town.', 'Santa is coming to town.', 'Santa is coming to town.'], [{}, {}, {}])\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    print('stream=False\\n', gpt.query(txt, n=3))\n",
    "#     print('\\nstream=True')\n",
    "#     for tok, full in gpt.query(txt, stream=True):\n",
    "#         print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:42:45.885181Z",
     "start_time": "2022-04-03T23:42:44.090140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Switching openai backend to \"huggingface\".\n",
      "stream=False\n",
      " ('The city is', {'generated_text': '\\n\\nThe city is'})\n",
      "\n",
      "stream=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:923: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn('strip_output=True is not supported in stream '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:928: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'Streaming mode does not support manual truncation of '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:275: UserWarning: query_gpt_huggingface received unused kwargs {'stream': True}.\n",
      "  warnings.warn('query_gpt_huggingface received unused kwargs '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n\\nThe ' {'generated_text': '\\n\\nThe city is'}\n",
      "'city ' {'generated_text': '\\n\\nThe city is'}\n",
      "'is ' {'generated_text': '\\n\\nThe city is'}\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    tmp = gpt.query(txt, max_tokens=5)\n",
    "    print('stream=False\\n', tmp)\n",
    "    print('\\nstream=True')\n",
    "    for tok, full in gpt.query(txt, stream=True, max_tokens=5):\n",
    "        print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:30:53.742399Z",
     "start_time": "2022-04-03T23:30:53.668410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '\\n\\nThe city is'} The \n",
      "{'generated_text': '\\n\\nThe city is'} city \n",
      "{'generated_text': '\\n\\nThe city is'} is \n"
     ]
    }
   ],
   "source": [
    "for tok, full in stream_response(tmp[0], tmp[1]):\n",
    "    print(full, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:27:57.764767Z",
     "start_time": "2022-04-03T23:27:57.714951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The ', {'generated_text': '\\n\\nThe city is'})\n",
      "('city ', {'generated_text': '\\n\\nThe city is'})\n",
      "('is ', {'generated_text': '\\n\\nThe city is'})\n"
     ]
    }
   ],
   "source": [
    "for row in stream_response(*tmp):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:09:04.872102Z",
     "start_time": "2022-04-04T00:09:04.417421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['to give you one big', 'to show you some pictures'],\n",
       " [{'generated_text': ' to give you one big'},\n",
       "  {'generated_text': ' to show you some pictures'}])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:07:51.956064Z",
     "start_time": "2022-04-04T00:07:51.881109Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-63fca387ae31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhf_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-dd51190cc58a>\u001b[0m in \u001b[0;36mstream_response\u001b[0;34m(text, full)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstream_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-c66818f51168>\u001b[0m in \u001b[0;36mstream_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mother\u001b[0m \u001b[0mstreaming\u001b[0m \u001b[0minterfaces\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mrequire\u001b[0m \u001b[0mno\u001b[0m \u001b[0mfurther\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "for row in stream_response(*hf_res):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
