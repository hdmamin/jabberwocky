{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Experiment with ways to support passing a list of prompts to query method. Some backends don't support this natively, others do, but none automatically would return the format I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:32.218861Z",
     "start_time": "2022-04-09T22:31:32.201839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T22:03:12.833880Z",
     "start_time": "2022-04-10T22:03:12.754268Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.openai_utils import load_prompt, load_openai_api_key, \\\n",
    "    GPTBackend\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:35.556609Z",
     "start_time": "2022-04-09T22:31:35.525709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: make thread that returns value so we can run a separate query for each thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:37.067944Z",
     "start_time": "2022-04-09T22:31:37.033094Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class ReturningThread(Thread):\n",
    "\n",
    "    @add_docstring(Thread)\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs=None, *, daemon=None):\n",
    "        \"\"\"This is identical to a regular thread except that the join method\n",
    "        returns the value returned by your target function. The\n",
    "        Thread.__init__ docstring is shown below for the sake of convenience.\n",
    "        \"\"\"\n",
    "        super().__init__(group=group, target=target, name=name,\n",
    "                         args=args, kwargs=kwargs, daemon=daemon)\n",
    "        self.result = None\n",
    "\n",
    "    def run(self):\n",
    "        self.result = self._target(*self._args, **self._kwargs)\n",
    "        \n",
    "    def join(self, timeout=None):\n",
    "        super().join(timeout)\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:37.646693Z",
     "start_time": "2022-04-09T22:31:37.619135Z"
    }
   },
   "outputs": [],
   "source": [
    "def foo(x, wait=2):\n",
    "    time.sleep(wait)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:38.615295Z",
     "start_time": "2022-04-09T22:31:38.585416Z"
    }
   },
   "outputs": [],
   "source": [
    "def foo_inv(x, wait=2):\n",
    "    time.sleep(1 / wait)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:38.777291Z",
     "start_time": "2022-04-09T22:31:38.746491Z"
    }
   },
   "outputs": [],
   "source": [
    "def foo_random(x, max_wait=5):\n",
    "    wait = np.random.uniform(low=0, high=max_wait)\n",
    "    print(wait, flush=True)\n",
    "    time.sleep(wait)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:22:18.048518Z",
     "start_time": "2022-04-03T20:22:07.985576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns values but is slow (sync execution).\n",
    "res = [foo(i) for i in range(5)]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:21:52.630627Z",
     "start_time": "2022-04-03T20:21:50.591510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [Thread(target=foo, args=(i,)) for i in range(5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# Regular thread returns None.\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:25:38.697676Z",
     "start_time": "2022-04-03T20:25:36.653887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [ReturningThread(target=foo, args=(i,)) for i in range(5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# ReturningThread returns values!\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:50:45.441326Z",
     "start_time": "2022-04-05T03:50:43.407449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait 1.0\n",
      "wait 0.5\n",
      "wait 0.3333333333333333\n",
      "wait 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [ReturningThread(target=foo_inv, args=(i, i)) for i in range(1, 5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# ReturningThread returns values!\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:54:31.953745Z",
     "start_time": "2022-04-05T03:54:28.204670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7145703512021404\n",
      "0.5710566753268154\n",
      "2.778421106481786\n",
      "0.44917966624138606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [ReturningThread(target=foo_random, args=(i, 5))\n",
    "           for i in range(1, 5)]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# ReturningThread returns values!\n",
    "res = [thread.join() for thread in threads]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try integrating into GPTBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:44:43.076560Z",
     "start_time": "2022-04-03T21:44:43.036655Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: no guarantees these threads return in the right order, though, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:48:54.240825Z",
     "start_time": "2022-04-06T03:48:53.298931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt_banana at 0x11e51f2f0>\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTBackend()\n",
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:48:57.096765Z",
     "start_time": "2022-04-06T03:48:57.066022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "\n",
      "Base: https://api.openai.com\n",
      "Query func: <function query_gpt_huggingface at 0x1100820d0>\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('huggingface')\n",
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:44:00.646610Z",
     "start_time": "2022-04-05T02:44:00.612877Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Six million years after the pandemic,',\n",
    "    'The stegosaurus'\n",
    "]\n",
    "kwargs = {'max_tokens': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:34:33.358120Z",
     "start_time": "2022-04-03T20:34:32.812552Z"
    }
   },
   "outputs": [],
   "source": [
    "threads = [ReturningThread(target=gpt.query, args=(prompt,), kwargs=kwargs) \n",
    "           for prompt in prompts]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "res = [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:34:35.086435Z",
     "start_time": "2022-04-03T20:34:35.039987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the world is still in the grip of a global',\n",
       "  {'generated_text': ' the world is still in the grip of a global'}),\n",
       " ('is a large, large, and highly intelligent animal',\n",
       "  {'generated_text': ' is a large, large, and highly intelligent animal'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:36:21.349540Z",
     "start_time": "2022-04-03T20:36:16.550197Z"
    }
   },
   "outputs": [],
   "source": [
    "threads = [ReturningThread(target=gpt.query,\n",
    "                           args=(prompt,), \n",
    "                           kwargs={**kwargs, 'n': 3, \n",
    "                                   'logprobs': 4, 'engine_i': 1}) \n",
    "           for prompt in prompts]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "res = [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:36:39.231035Z",
     "start_time": "2022-04-03T20:36:39.180639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmap(len, *res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:36:48.181801Z",
     "start_time": "2022-04-03T20:36:48.133726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the population of New York City is poised to rise',\n",
       " 'the pandemic strain of influenza spreads and mutates',\n",
       " 'our species is still struggling to deal with the effects']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:05.079974Z",
     "start_time": "2022-04-03T20:37:05.029095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' the population of New York City is poised to rise'},\n",
       " {'generated_text': ' the pandemic strain of influenza spreads and mutates'},\n",
       " {'generated_text': ' our species is still struggling to deal with the effects'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:29.152724Z",
     "start_time": "2022-04-03T20:37:29.105080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is one of the more remarkable prehistoric dinosaurs, and',\n",
       " ', or giant pterosaur from the late',\n",
       " 'fossil, or dinosaur\\nFossil bones of']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:37:31.718185Z",
     "start_time": "2022-04-03T20:37:31.646264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' is one of the more remarkable prehistoric dinosaurs, and'},\n",
       " {'generated_text': ', or giant pterosaur from the late'},\n",
       " {'generated_text': ' fossil, or dinosaur\\nFossil bones of'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:49:06.209819Z",
     "start_time": "2022-04-06T03:49:06.180009Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt3 at 0x1100829d8>\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('gooseai')\n",
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:38:56.466544Z",
     "start_time": "2022-04-03T20:38:54.624773Z"
    }
   },
   "outputs": [],
   "source": [
    "threads = [ReturningThread(target=gpt.query,\n",
    "                           args=(prompt,), \n",
    "                           kwargs={'max_tokens': 8, 'n': 2, \n",
    "                                   'logprobs': 5, 'engine_i': 0}) \n",
    "           for prompt in prompts]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "res = [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:40:56.897362Z",
     "start_time": "2022-04-03T20:40:56.848457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:39:06.290772Z",
     "start_time": "2022-04-03T20:39:06.258358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the world is still in the grip of',\n",
       " 'scientists still do not know whether humans are']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:39:12.866153Z",
     "start_time": "2022-04-03T20:39:12.834078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is a fossilized dinosaur named by the',\n",
       " 'is a famous carnivorous dinosaur from the']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:41:15.524928Z",
     "start_time": "2022-04-03T20:41:15.477253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[0])#[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:52:30.767721Z",
     "start_time": "2022-04-03T20:52:30.718958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the world is still in the grip of',\n",
       "  'scientists still do not know whether humans are'],\n",
       " ['is a fossilized dinosaur named by the',\n",
       "  'is a famous carnivorous dinosaur from the'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, resps = list(zip(*res))\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:53:26.283690Z",
     "start_time": "2022-04-03T20:53:26.244193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' the', ' world', ' is', ' still', ' in', ' the', ' grip', ' of'],\n",
       " [' scientists',\n",
       "  ' still',\n",
       "  ' do',\n",
       "  ' not',\n",
       "  ' know',\n",
       "  ' whether',\n",
       "  ' humans',\n",
       "  ' are']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resps[i][j] corresponds to prompt i, completion j.\n",
    "[completion['logprobs'].tokens for completion in resps[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:53:35.756499Z",
     "start_time": "2022-04-03T20:53:35.701049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' is', ' a', ' fossil', 'ized', ' dinosaur', ' named', ' by', ' the'],\n",
       " [' is', ' a', ' famous', ' carniv', 'orous', ' dinosaur', ' from', ' the']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[completion['logprobs'].tokens for completion in resps[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T03:49:10.210774Z",
     "start_time": "2022-04-06T03:49:10.177457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base: https://api.goose.ai/v1\n",
      "Query func: <function query_gpt3 at 0x1100829d8>\n"
     ]
    }
   ],
   "source": [
    "gpt.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:56:40.049508Z",
     "start_time": "2022-04-03T20:56:39.320533Z"
    }
   },
   "outputs": [],
   "source": [
    "threads2 = [ReturningThread(target=gpt.query,\n",
    "                           args=(prompt,), \n",
    "                           kwargs={'max_tokens': 8, 'n': 1, \n",
    "                                   'logprobs': 5, 'engine_i': 0}) \n",
    "            for prompt in prompts]\n",
    "for thread in threads2:\n",
    "    thread.start()\n",
    "res2 = [thread.join() for thread in threads2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:57:26.768849Z",
     "start_time": "2022-04-03T20:57:26.725952Z"
    }
   },
   "outputs": [],
   "source": [
    "texts2, resps2 = list(zip(*res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:57:29.637473Z",
     "start_time": "2022-04-03T20:57:29.585330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the future of the world’s', 'The stegosaurus (Ste')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T20:59:01.385365Z",
     "start_time": "2022-04-03T20:59:01.343301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'length',\n",
       " 'index': 0,\n",
       " 'logprobs': <OpenAIObject at 0x125e93d58> JSON: {\n",
       "   \"text_offset\": [\n",
       "     0,\n",
       "     4,\n",
       "     11,\n",
       "     14,\n",
       "     18,\n",
       "     24,\n",
       "     24,\n",
       "     25\n",
       "   ],\n",
       "   \"token_logprobs\": [\n",
       "     -1.7412109375,\n",
       "     -5.03515625,\n",
       "     -0.361083984375,\n",
       "     -1.7802734375,\n",
       "     -1.8515625,\n",
       "     -1.89453125,\n",
       "     -0.0006651878356933594,\n",
       "     -0.00013065338134765625\n",
       "   ],\n",
       "   \"tokens\": [\n",
       "     \" the\",\n",
       "     \" future\",\n",
       "     \" of\",\n",
       "     \" the\",\n",
       "     \" world\",\n",
       "     \"\\ufffd\",\n",
       "     \"\\ufffd\",\n",
       "     \"s\"\n",
       "   ],\n",
       "   \"top_logprobs\": [\n",
       "     {\n",
       "       \" a\": -2.90234375,\n",
       "       \" it\": -3.947265625,\n",
       "       \" scientists\": -4.09375,\n",
       "       \" the\": -1.7412109375,\n",
       "       \" we\": -2.748046875\n",
       "     },\n",
       "     {\n",
       "       \" city\": -4.51171875,\n",
       "       \" human\": -4.06640625,\n",
       "       \" pand\": -4.2109375,\n",
       "       \" virus\": -2.9921875,\n",
       "       \" world\": -1.6845703125\n",
       "     },\n",
       "     {\n",
       "       \" is\": -2.072265625,\n",
       "       \" looks\": -3.4296875,\n",
       "       \" of\": -0.361083984375,\n",
       "       \" remains\": -3.44921875,\n",
       "       \" still\": -4.171875\n",
       "     },\n",
       "     {\n",
       "       \" CO\": -3.35546875,\n",
       "       \" global\": -3.57421875,\n",
       "       \" infectious\": -3.62890625,\n",
       "       \" pand\": -3.509765625,\n",
       "       \" the\": -1.7802734375\n",
       "     },\n",
       "     {\n",
       "       \" global\": -3.478515625,\n",
       "       \" human\": -2.720703125,\n",
       "       \" pand\": -3.0625,\n",
       "       \" virus\": -2.365234375,\n",
       "       \" world\": -1.8515625\n",
       "     },\n",
       "     {\n",
       "       \" is\": -1.21484375,\n",
       "       \" looks\": -3.654296875,\n",
       "       \" remains\": -3.19140625,\n",
       "       \"'s\": -1.814453125,\n",
       "       \"\\ufffd\": -1.89453125\n",
       "     },\n",
       "     {\n",
       "       \"\\ufffd\": -10.5546875\n",
       "     },\n",
       "     {\n",
       "       \" s\": -11.3671875,\n",
       "       \"bytes:'\\\\n'\": -11.8671875,\n",
       "       \"ll\": -12.3125,\n",
       "       \"s\": -0.00013065338134765625,\n",
       "       \"\\ufffd\": -11.3125\n",
       "     }\n",
       "   ]\n",
       " },\n",
       " 'text': ' the future of the world’s',\n",
       " 'token_index': 0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because only 1 completion per prompt, resps is a dict instead of a list of \n",
    "# dicts.\n",
    "resps2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:44:25.174986Z",
     "start_time": "2022-04-05T02:44:24.549929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    hf_res = gpt.query('I want', engine_i=1, max_tokens=5, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:44:26.068677Z",
     "start_time": "2022-04-05T02:44:26.034625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results(text=['to give you one big', 'to show you some pictures'], full=[{'generated_text': ' to give you one big'}, {'generated_text': ' to show you some pictures'}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results(text=hf_res[0], full=hf_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better interface?\n",
    "# texts, full_resps = gpt.query([p1, p2, p3], n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test streaming mode\n",
    "\n",
    "Need a better understanding of what using streaming mode is like before I decide about streaming interface for np or nc > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:52.683198Z",
     "start_time": "2022-04-09T22:31:52.654076Z"
    }
   },
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "from jabberwocky.openai_utils import query_gpt3, query_gpt_huggingface, \\\n",
    "    query_gpt_banana, query_gpt_j, query_gpt_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:52.937264Z",
     "start_time": "2022-04-09T22:31:52.905322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Was toying with idea of adding this to gpt.query warnings to make the \n",
    "# messages unique, in the hope that this would ensure they're always shown\n",
    "# rather than just once. But a. I'm not sure if that's how they define \n",
    "# duplicates, and b. I'm seeing code defined in nb seems to always show \n",
    "# warnings, not just once, so I'm not sure what to make of that. Still \n",
    "# eventually want to write a func like this (maybe moreso for creating new\n",
    "# file paths when encountering collisions) but that should have a more limited\n",
    "# set of possible characters.\n",
    "def random_str(length, lower=True):\n",
    "    rand = b64encode(os.urandom(length)).decode()[:length]\n",
    "    return rand.lower() if lower else rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:00.135475Z",
     "start_time": "2022-04-05T02:45:00.081330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 d\n",
      "2 1g\n",
      "3 p4u\n",
      "4 dpvk\n",
      "5 ijbgo\n",
      "6 a1hztl\n",
      "7 7qzb4lz\n",
      "8 cxcnot6l\n",
      "9 jthktr7ne\n",
      "10 lueazbofkx\n",
      "11 brpiuhixuva\n",
      "12 /j6h2vyqih8m\n",
      "13 rnv/a+azvmlsr\n",
      "14 row/b8sipg+zfr\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    rand = random_str(i)\n",
    "    print(i, rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:03.668606Z",
     "start_time": "2022-04-05T02:45:03.636388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' \\x97\\xc1\\xc3\\x0eB29\\x9b\\xf7'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.urandom(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:16:53.362657Z",
     "start_time": "2022-04-05T03:16:53.318087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "Switching  backend back to \"huggingface\".\n",
      "I  {'index': 0, 'finish_reason': None}\n",
      "want  {'index': 0, 'finish_reason': None}\n",
      "to  {'index': 0, 'finish_reason': None}\n",
      "go  {'index': 0, 'finish_reason': None}\n",
      "to  {'index': 0, 'finish_reason': None}\n",
      "there.  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    repeat_res = gpt.query('I want to go to there.',\n",
    "                           max_tokens=5, stream=True)\n",
    "\n",
    "for txt_, full_ in repeat_res:\n",
    "    print(txt_, full_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:16:43.476606Z",
     "start_time": "2022-04-05T03:16:43.442206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "Switching  backend back to \"huggingface\".\n",
      "I  {'index': 0, 'finish_reason': None}\n",
      "want  {'index': 0, 'finish_reason': None}\n",
      "to  {'index': 0, 'finish_reason': None}\n",
      "go  {'index': 0, 'finish_reason': None}\n",
      "to  {'index': 0, 'finish_reason': None}\n",
      "there.  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "I  {'index': 1, 'finish_reason': None}\n",
      "want  {'index': 1, 'finish_reason': None}\n",
      "to  {'index': 1, 'finish_reason': None}\n",
      "go  {'index': 1, 'finish_reason': None}\n",
      "to  {'index': 1, 'finish_reason': None}\n",
      "there.  {'index': 1, 'finish_reason': 'dummy'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    repeat_res = gpt.query('I want to go to there.',\n",
    "                           max_tokens=5, stream=True, n=2)\n",
    "\n",
    "for txt_, full_ in repeat_res:\n",
    "    print(txt_, full_)\n",
    "    if full_['finish_reason']:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:17:21.347273Z",
     "start_time": "2022-04-05T03:17:18.227244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "\n",
      "\n",
      "I'm  {'id': 'aa2f677a-bc98-4e76-b3e7-ab8d6e75302f', 'message': 'success', 'created': 1649128641, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': \"\\n\\nI'm a\", 'input': 'Who are you?'}], 'index': 0, 'finish_reason': None}\n",
      "a  {'id': 'aa2f677a-bc98-4e76-b3e7-ab8d6e75302f', 'message': 'success', 'created': 1649128641, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': \"\\n\\nI'm a\", 'input': 'Who are you?'}], 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('banana'):\n",
    "    for txt_, full_ in gpt.query('Who are you?',\n",
    "                                 max_tokens=5, stream=True):\n",
    "        print(txt_, full_)\n",
    "        if full_['finish_reason']:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:18:15.585713Z",
     "start_time": "2022-04-05T03:18:14.702742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:278: UserWarning: query_gpt_huggingface received unused kwargs {'stream': True}.\n",
      "  warnings.warn('query_gpt_huggingface received unused kwargs '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I  {'generated_text': '\\n\\nI am a', 'index': 0, 'finish_reason': None}\n",
      "am  {'generated_text': '\\n\\nI am a', 'index': 0, 'finish_reason': None}\n",
      "a  {'generated_text': '\\n\\nI am a', 'index': 0, 'finish_reason': 'dummy'}\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    for txt_, full_ in gpt.query('Who are you?',\n",
    "                                 max_tokens=5, stream=True):\n",
    "        print(txt_, full_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T03:18:47.204416Z",
     "start_time": "2022-04-05T03:18:46.701482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "\n",
      "\n",
      "I'm  {'generated_text': \"\\n\\nI'm going\", 'index': 0, 'finish_reason': None}\n",
      "going  {'generated_text': \"\\n\\nI'm going\", 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "Are  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "you  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "being  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "honest  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': None}\n",
      "or  {'generated_text': ' Are you being honest or', 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    for txt_, full_ in gpt.query('Who are you?',\n",
    "                                 max_tokens=5, stream=True, n=2, engine_i=1):\n",
    "        print(txt_, full_)\n",
    "        if full_['finish_reason']:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T21:37:55.051099Z",
     "start_time": "2022-04-03T21:37:54.998748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason']),\n",
       " dict_keys(['text', 'index', 'logprobs', 'finish_reason'])]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.choices[0].keys() for row in load(C.mock_stream_paths[True])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with streaming text AND dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:14.112834Z",
     "start_time": "2022-04-05T02:45:14.074292Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:14.453818Z",
     "start_time": "2022-04-05T02:45:14.416411Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream_words(text):\n",
    "    \"\"\"Like stream_chars but splits on spaces. Realized stream_chars was a bad\n",
    "    idea because we risk giving SPEAKER turns like\n",
    "    \"This is over. W\" and \"hat are you doing next?\", neither of which would be\n",
    "    pronounced as intended. We yield with a space for consistency with the\n",
    "    other streaming interfaces which require no further postprocessing.\n",
    "    \"\"\"\n",
    "    for word in text.split(' '):\n",
    "        yield word + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:45:17.783386Z",
     "start_time": "2022-04-05T02:45:17.739551Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream_response(text:str, full:dict):\n",
    "    yield from zip(stream_words(text), cycle([full]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:18.171856Z",
     "start_time": "2022-04-05T02:53:18.121860Z"
    }
   },
   "outputs": [],
   "source": [
    "def containerize(*args):\n",
    "    res = []\n",
    "    for arg in args:\n",
    "        if listlike(arg):\n",
    "            res.append(arg)\n",
    "        else:\n",
    "            res.append([arg])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:18.336766Z",
     "start_time": "2022-04-05T02:53:18.304906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: this is probably massively over-engineered for mock streaming, but \n",
    "# I'll need to do something like this if I want to support real streaming \n",
    "# where nc and/or np > 1 so it was probably useful to work through this logic\n",
    "# anyway.\n",
    "def stream_multi_response(texts:list, fulls:list):\n",
    "    texts, fulls = containerize(texts, fulls)\n",
    "    for i, (text, full) in enumerate(zip(texts, fulls)):\n",
    "        queue = deque()\n",
    "        gen = stream_response(text, \n",
    "                              {**full, 'index': i, 'finish_reason': None})\n",
    "        done = False\n",
    "        # Yield items while checking if we're at the last item so we can mark\n",
    "        # it with a finish_reason. This lets us know when one completion ends.\n",
    "        while True:\n",
    "            try:\n",
    "                tok, tok_full = next(gen)\n",
    "                queue.append((tok, tok_full))\n",
    "            except StopIteration:\n",
    "                done = True\n",
    "            \n",
    "            while len(queue) > 1:\n",
    "                tok, tok_full = queue.popleft()\n",
    "                yield tok, tok_full\n",
    "            if done: break\n",
    "        tok, tok_full = queue.popleft()\n",
    "        tok_full['finish_reason'] = 'dummy'    \n",
    "        yield tok, tok_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:19.330156Z",
     "start_time": "2022-04-05T02:53:19.297219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abc'], [{'text': 'def'}]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containerize('abc', {'text': 'def'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:19.482525Z",
     "start_time": "2022-04-05T02:53:19.448745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abc'], [{'text': 'def'}]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containerize(['abc'], [{'text': 'def'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:19.876063Z",
     "start_time": "2022-04-05T02:53:19.831675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abc', 'hij'], [{'text': 'def'}, {'text': 'aka'}]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containerize(['abc', 'hij'], [{'text': 'def'}, {'text': 'aka'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:22.112368Z",
     "start_time": "2022-04-05T02:53:22.080809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Santa '\n",
      "'is '\n",
      "'coming '\n",
      "'to '\n",
      "'town. '\n"
     ]
    }
   ],
   "source": [
    "txt = 'Santa is coming to town.'\n",
    "for tok in stream_words(txt):\n",
    "    print(repr(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:25.043479Z",
     "start_time": "2022-04-05T02:53:25.008566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Santa ' {}\n",
      "'is ' {}\n",
      "'coming ' {}\n",
      "'to ' {}\n",
      "'town. ' {}\n"
     ]
    }
   ],
   "source": [
    "for tok, full in stream_response(txt, {}):\n",
    "    print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:06:01.902403Z",
     "start_time": "2022-04-03T22:06:01.416816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1\n",
    "with gpt('huggingface'):\n",
    "    hf_res = gpt.query('I want', engine_i=1, max_tokens=5, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:31.922090Z",
     "start_time": "2022-04-05T02:53:31.891452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['to give you one big', 'to show you some pictures'],\n",
       " [{'generated_text': ' to give you one big'},\n",
       "  {'generated_text': ' to show you some pictures'}])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:35.170612Z",
     "start_time": "2022-04-05T02:53:35.136715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>  to  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  give  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  you  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  one  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': None}\n",
      ">>>  big  {'generated_text': ' to give you one big', 'index': 0, 'finish_reason': 'dummy'}\n",
      ">>>  to  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  show  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  you  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  some  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': None}\n",
      ">>>  pictures  {'generated_text': ' to show you some pictures', 'index': 1, 'finish_reason': 'dummy'}\n"
     ]
    }
   ],
   "source": [
    "for tok, full in stream_multi_response(*hf_res):\n",
    "    print('>>> ', tok, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:38.019901Z",
     "start_time": "2022-04-05T02:53:37.989912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>  so  {'response': 'so', 'index': 0, 'finish_reason': 'dummy'}\n"
     ]
    }
   ],
   "source": [
    "# nc = 1, already containerized.\n",
    "for tok, full in stream_multi_response(['so'], [{'response': 'so'}]):\n",
    "    print('>>> ', tok, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:54:12.600410Z",
     "start_time": "2022-04-05T02:54:12.549155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>  so  {'response': 'so', 'index': 0, 'finish_reason': 'dummy'}\n"
     ]
    }
   ],
   "source": [
    "# nc = 1, not yet containerized.\n",
    "for tok, full in stream_multi_response('so', {'response': 'so'}):\n",
    "    print('>>> ', tok, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T02:53:39.638234Z",
     "start_time": "2022-04-05T02:53:39.608034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Empty response.\n",
    "for tok, full in stream_multi_response([], []):\n",
    "    print('>>> ', tok, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T22:09:56.344237Z",
     "start_time": "2022-04-10T22:09:51.741167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "['Yesterday was', 'How many'] 2 True\n",
      "['Yesterday was', 'How many'] 2 False\n",
      "['Yesterday was', 'How many'] 1 True\n",
      "['Yesterday was', 'How many'] 1 False\n",
      "Yesterday was 2 True\n",
      "Yesterday was 2 False\n",
      "Yesterday was 1 True\n",
      "Yesterday was 1 False\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('gooseai')\n",
    "txts = ['Yesterday was', 'How many']\n",
    "\n",
    "# Key: (multi_in, multi_out, stream)\n",
    "responses = {}\n",
    "for multi_in in (True, False):\n",
    "    for multi_out in (True, False):\n",
    "        for stream in (True, False):\n",
    "            prompt = txts if multi_in else txts[0]\n",
    "            nc = 1 + multi_out\n",
    "            print(prompt, nc, stream)\n",
    "            res = openai.Completion.create(\n",
    "                prompt=prompt,\n",
    "                engine=GPTBackend.engine(0),\n",
    "                max_tokens=3,\n",
    "                logprobs=3,\n",
    "                n=nc,\n",
    "                stream=stream\n",
    "            )      \n",
    "            if stream: res = list(res)\n",
    "            responses[multi_in, multi_out, stream] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T22:12:21.688531Z",
     "start_time": "2022-04-10T22:12:21.637942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to data/misc/gooseai_sample_responses.pkl.\n"
     ]
    }
   ],
   "source": [
    "save(responses, 'data/misc/gooseai_sample_responses.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T22:04:47.401473Z",
     "start_time": "2022-04-10T22:04:47.363846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(True, True, True): 'test',\n",
       " (True, True, False): 'test',\n",
       " (True, False, True): 'test',\n",
       " (True, False, False): 'test',\n",
       " (False, True, True): 'test',\n",
       " (False, True, False): 'test',\n",
       " (False, False, True): 'test',\n",
       " (False, False, False): 'test'}"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T20:59:17.669052Z",
     "start_time": "2022-04-09T20:59:17.127807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "# np = 1, nc > 1, stream=True\n",
    "txt = 'Santa is coming to town.'\n",
    "with gpt('gooseai'):\n",
    "    goose_res = openai.Completion.create(\n",
    "        prompt=txt,\n",
    "        engine=GPTBackend.engine(0),\n",
    "        max_tokens=5,\n",
    "        logprobs=3,\n",
    "        n=2,\n",
    "        stream=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:46:34.780227Z",
     "start_time": "2022-04-09T21:46:33.997765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "Switching  backend back to \"banana\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1, nc > 1, stream=True\n",
    "txts = ['Yesterday was', 'How many']\n",
    "with gpt('gooseai'):\n",
    "    goose_res_multi = openai.Completion.create(\n",
    "        prompt=txts,\n",
    "        engine=GPTBackend.engine(0),\n",
    "        max_tokens=5,\n",
    "        logprobs=3,\n",
    "        n=2,\n",
    "        stream=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:46:42.849635Z",
     "start_time": "2022-04-09T21:46:42.815359Z"
    }
   },
   "outputs": [],
   "source": [
    "_goose_res_multi = list(goose_res_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:19:22.778898Z",
     "start_time": "2022-04-10T20:19:20.538576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1, nc > 1, stream=False\n",
    "txts = ['Yesterday was', 'How many']\n",
    "with gpt('gooseai'):\n",
    "    goose_res_multi_static = openai.Completion.create(\n",
    "        prompt=txts,\n",
    "        engine=GPTBackend.engine(0),\n",
    "        max_tokens=5,\n",
    "        logprobs=3,\n",
    "        n=2,\n",
    "        stream=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:20:03.049865Z",
     "start_time": "2022-04-10T20:20:03.021070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 7, 2, 5, 6]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmap(len, *goose_res_multi_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:20:57.347812Z",
     "start_time": "2022-04-10T20:20:57.318477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 5]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmap(len, *goose_res_multi_static.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:26.675931Z",
     "start_time": "2022-04-03T22:53:25.148802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"openai\".\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "# np > 1, stream=True\n",
    "with gpt('openai'):\n",
    "    open_res = openai.Completion.create(\n",
    "        prompt=txt,\n",
    "        engine=GPTBackend.engine(0),\n",
    "        max_tokens=5,\n",
    "        logprobs=3,\n",
    "        n=2,\n",
    "        stream=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T20:59:38.229945Z",
     "start_time": "2022-04-09T20:59:38.184369Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          0\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -3.74609375\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" This\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" And\": -2.826171875,\n",
      "            \" The\": -3.029296875,\n",
      "            \"bytes:'\\\\n'\": -1.3095703125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" This\",\n",
      "      \"token_index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          5\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -1.263671875\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" year\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" is\": -2.001953125,\n",
      "            \" time\": -2.638671875,\n",
      "            \" year\": -1.263671875\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" year\",\n",
      "      \"token_index\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          10\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -11.7734375\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Lisa\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"'s\": -3.095703125,\n",
      "            \",\": -0.7060546875,\n",
      "            \"\\ufffd\": -2.1328125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Lisa\",\n",
      "      \"token_index\": 2\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          15\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -3.330078125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \",\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" and\": -2.134765625,\n",
      "            \" is\": -3.26953125,\n",
      "            \",\": -3.330078125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \",\",\n",
      "      \"token_index\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          16\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -6.7890625\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Sara\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" a\": -2.7890625,\n",
      "            \" our\": -3.2421875,\n",
      "            \" the\": -2.599609375\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Sara\",\n",
      "      \"token_index\": 4\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"8f5441e8-d9a6-4dd0-8f23-a13289a194a9\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          0\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -13.8515625\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Lt\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" And\": -2.826171875,\n",
      "            \" The\": -3.029296875,\n",
      "            \"bytes:'\\\\n'\": -1.3095703125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Lt\",\n",
      "      \"token_index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          3\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -0.15576171875\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \".\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" Col\": -4.10546875,\n",
      "            \" Colonel\": -5.61328125,\n",
      "            \".\": -0.15576171875\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \".\",\n",
      "      \"token_index\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          4\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -10.28125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Tay\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" Col\": -2.677734375,\n",
      "            \" Colonel\": -2.896484375,\n",
      "            \" Commander\": -2.751953125\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Tay\",\n",
      "      \"token_index\": 2\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          8\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -6.1796875\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \"ana\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"e\": -2.953125,\n",
      "            \"ler\": -1.7470703125,\n",
      "            \"st\": -2.91015625\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \"ana\",\n",
      "      \"token_index\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          11\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -4.9453125\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" B\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" and\": -4.0234375,\n",
      "            \" is\": -3.548828125,\n",
      "            \",\": -3.458984375\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" B\",\n",
      "      \"token_index\": 4\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649537957,\n",
      "  \"id\": \"99efffc2-2412-47c3-bb65-7f5521174953\",\n",
      "  \"model\": \"gpt-neo-2-7b\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_goose_res = []\n",
    "for obj in goose_res:\n",
    "    print(obj)\n",
    "    _goose_res.append(obj)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:42.976851Z",
     "start_time": "2022-04-03T22:53:42.924028Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          24\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -8.621929\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Feeling\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"\\n\": -2.4186804,\n",
      "            \" I\": -2.6238666,\n",
      "            \" She\": -2.4244554\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Feeling\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          32\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -5.186215\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" her\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" a\": -2.1202018,\n",
      "            \" like\": -2.6433406,\n",
      "            \" the\": -2.8912876\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" her\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          24\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.6238666\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" I\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"\\n\": -2.4186804,\n",
      "            \" I\": -2.6238666,\n",
      "            \" She\": -2.4244554\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" I\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          26\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.2295423\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \"'m\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" don\": -2.8276584,\n",
      "            \"'m\": -2.2295423,\n",
      "            \"bytes:\\\\xe2\\\\x80\": -2.7309577\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \"'m\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          28\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.6193585\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" sure\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" going\": -1.7995286,\n",
      "            \" not\": -2.3888402,\n",
      "            \" sure\": -2.6193585\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" sure\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          36\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -3.293669\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" presence\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" up\": -2.2750113,\n",
      "            \" way\": -2.5486472,\n",
      "            \",\": -3.0275166\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" presence\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          33\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -1.7617589\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" you\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" he\": -2.186368,\n",
      "            \" she\": -1.4318895,\n",
      "            \" you\": -1.7617589\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" you\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          45\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -1.0045006\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \",\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" in\": -2.5863404,\n",
      "            \" is\": -2.647669,\n",
      "            \",\": -1.0045006\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \",\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          46\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -8.532746\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" Des\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" I\": -1.8522211,\n",
      "            \" he\": -2.4737713,\n",
      "            \" the\": -2.7948432\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" Des\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          37\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -2.7016406\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" can\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"'ll\": -1.5801843,\n",
      "            \"'re\": -2.1810775,\n",
      "            \"'ve\": -1.8903823\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" can\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1649026406,\n",
      "  \"id\": \"cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_open_res = []\n",
    "for obj in open_res:\n",
    "    print(obj)\n",
    "    _open_res.append(obj)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:47:35.605569Z",
     "start_time": "2022-04-09T21:47:35.575491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 20)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_goose_res), len(_open_res), len(_goose_res_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:47:38.062817Z",
     "start_time": "2022-04-09T21:47:38.027917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([' This'], None),\n",
       " ([' year'], None),\n",
       " ([' Lisa'], None),\n",
       " ([','], None),\n",
       " ([' Sara'], 'length'),\n",
       " ([' Lt'], None),\n",
       " (['.'], None),\n",
       " ([' Tay'], None),\n",
       " (['ana'], None),\n",
       " ([' B'], 'length')]"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(row.choices[0].logprobs.tokens, row.choices[0].finish_reason) \n",
    " for row in _goose_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T21:48:10.643859Z",
     "start_time": "2022-04-09T21:48:10.611827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([' one'], None, 0),\n",
       " ([' of'], None, 0),\n",
       " ([' the'], None, 0),\n",
       " ([' most'], None, 0),\n",
       " ([' important'], 'length', 0),\n",
       " ([' of'], None, 2),\n",
       " ([' you'], None, 2),\n",
       " ([' have'], None, 2),\n",
       " ([' heard'], None, 2),\n",
       " ([' or'], 'length', 2),\n",
       " ([' thoughts'], None, 3),\n",
       " ([' go'], None, 3),\n",
       " ([' through'], None, 3),\n",
       " ([' your'], None, 3),\n",
       " ([' head'], 'length', 3),\n",
       " ([' the'], None, 1),\n",
       " ([' end'], None, 1),\n",
       " ([' of'], None, 1),\n",
       " ([' fashion'], None, 1),\n",
       " ([' week'], 'length', 1)]"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(row.choices[0].logprobs.tokens, \n",
    "  row.choices[0].finish_reason, \n",
    "  row.choices[0].index) \n",
    " for row in _goose_res_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:53:56.046309Z",
     "start_time": "2022-04-03T22:53:55.996554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([' Feeling'], None),\n",
       " ([' her'], None),\n",
       " ([' I'], None),\n",
       " ([\"'m\"], None),\n",
       " ([' sure'], None),\n",
       " ([' presence'], None),\n",
       " ([' you'], None),\n",
       " ([','], None),\n",
       " ([' Des'], 'length'),\n",
       " ([' can'], 'length')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(row.choices[0].logprobs.tokens, row.choices[0].finish_reason) \n",
    " for row in _open_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T22:54:26.430884Z",
     "start_time": "2022-04-03T22:54:26.374792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Feeling',\n",
       " ' her',\n",
       " ' I',\n",
       " \"'m\",\n",
       " ' sure',\n",
       " ' presence',\n",
       " ' you',\n",
       " ',',\n",
       " ' Des',\n",
       " ' can']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.choices[0].text for row in _open_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:55:09.775032Z",
     "start_time": "2022-04-03T23:55:09.726986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD',\n",
       " 'cmpl-4t3O2F9Xf3GFJFEVA8q86pCQpTCaD']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thought we might be able to use id to reconstruct each completion but that\n",
    "# doesn't work.\n",
    "[row['id'] for row in _open_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:57:34.622597Z",
     "start_time": "2022-04-03T23:57:34.587532Z"
    }
   },
   "outputs": [],
   "source": [
    "# index points to which completion each new token belongs to.\n",
    "completions = defaultdict(list)\n",
    "for row in _open_res:\n",
    "    completions[row['choices'][0]['index']].append(row['choices'][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:57:37.089716Z",
     "start_time": "2022-04-03T23:57:37.038623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [' Feeling', ' her', ' presence', ',', ' Des'],\n",
       "             1: [' I', \"'m\", ' sure', ' you', ' can']})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:02:18.244513Z",
     "start_time": "2022-04-04T00:02:18.199128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None\n",
      "0 None\n",
      "0 None\n",
      "0 None\n",
      "0 length\n",
      "1 None\n",
      "1 None\n",
      "1 None\n",
      "1 None\n",
      "1 length\n"
     ]
    }
   ],
   "source": [
    "# index points to which completion each new token belongs to.\n",
    "# completions = defaultdict(list)\n",
    "for row in _goose_res:\n",
    "    print(row['choices'][0]['index'], row['choices'][0]['finish_reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:02:34.306496Z",
     "start_time": "2022-04-04T00:02:34.265497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None\n",
      "0 None\n",
      "1 None\n",
      "1 None\n",
      "1 None\n",
      "0 None\n",
      "1 None\n",
      "0 None\n",
      "0 length\n",
      "1 length\n"
     ]
    }
   ],
   "source": [
    "# index points to which completion each new token belongs to.\n",
    "# completions = defaultdict(list)\n",
    "for row in _open_res:\n",
    "    print(row['choices'][0]['index'], row['choices'][0]['finish_reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:05:31.960786Z",
     "start_time": "2022-04-04T00:05:31.568056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Switching openai backend to \"repeat\".\n",
      "stream=False\n",
      " ('Santa is coming to town.', {})\n",
      "\n",
      "stream=True\n",
      "'Santa ' {}\n",
      "'is ' {}\n",
      "'coming ' {}\n",
      "'to ' {}\n",
      "'town. ' {}\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:923: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn('strip_output=True is not supported in stream '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:928: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'Streaming mode does not support manual truncation of '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:391: UserWarning: Unused kwargs {'stream': True} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    print('stream=False\\n', gpt.query(txt))\n",
    "    print('\\nstream=True')\n",
    "    for tok, full in gpt.query(txt, stream=True):\n",
    "        print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:05:48.562505Z",
     "start_time": "2022-04-04T00:05:48.530971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "stream=False\n",
      " (['Santa is coming to town.', 'Santa is coming to town.', 'Santa is coming to town.'], [{}, {}, {}])\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('repeat'):\n",
    "    print('stream=False\\n', gpt.query(txt, n=3))\n",
    "#     print('\\nstream=True')\n",
    "#     for tok, full in gpt.query(txt, stream=True):\n",
    "#         print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:42:45.885181Z",
     "start_time": "2022-04-03T23:42:44.090140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Switching openai backend to \"huggingface\".\n",
      "stream=False\n",
      " ('The city is', {'generated_text': '\\n\\nThe city is'})\n",
      "\n",
      "stream=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:923: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn('strip_output=True is not supported in stream '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:928: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'Streaming mode does not support manual truncation of '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:275: UserWarning: query_gpt_huggingface received unused kwargs {'stream': True}.\n",
      "  warnings.warn('query_gpt_huggingface received unused kwargs '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n\\nThe ' {'generated_text': '\\n\\nThe city is'}\n",
      "'city ' {'generated_text': '\\n\\nThe city is'}\n",
      "'is ' {'generated_text': '\\n\\nThe city is'}\n",
      "Switching  backend back to \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('huggingface'):\n",
    "    tmp = gpt.query(txt, max_tokens=5)\n",
    "    print('stream=False\\n', tmp)\n",
    "    print('\\nstream=True')\n",
    "    for tok, full in gpt.query(txt, stream=True, max_tokens=5):\n",
    "        print(repr(tok), full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:30:53.742399Z",
     "start_time": "2022-04-03T23:30:53.668410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '\\n\\nThe city is'} The \n",
      "{'generated_text': '\\n\\nThe city is'} city \n",
      "{'generated_text': '\\n\\nThe city is'} is \n"
     ]
    }
   ],
   "source": [
    "for tok, full in stream_response(tmp[0], tmp[1]):\n",
    "    print(full, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T23:27:57.764767Z",
     "start_time": "2022-04-03T23:27:57.714951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The ', {'generated_text': '\\n\\nThe city is'})\n",
      "('city ', {'generated_text': '\\n\\nThe city is'})\n",
      "('is ', {'generated_text': '\\n\\nThe city is'})\n"
     ]
    }
   ],
   "source": [
    "for row in stream_response(*tmp):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:09:04.872102Z",
     "start_time": "2022-04-04T00:09:04.417421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['to give you one big', 'to show you some pictures'],\n",
       " [{'generated_text': ' to give you one big'},\n",
       "  {'generated_text': ' to show you some pictures'}])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T00:07:51.956064Z",
     "start_time": "2022-04-04T00:07:51.881109Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-63fca387ae31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhf_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-dd51190cc58a>\u001b[0m in \u001b[0;36mstream_response\u001b[0;34m(text, full)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstream_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-c66818f51168>\u001b[0m in \u001b[0;36mstream_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mother\u001b[0m \u001b[0mstreaming\u001b[0m \u001b[0minterfaces\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mrequire\u001b[0m \u001b[0mno\u001b[0m \u001b[0mfurther\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "for row in stream_response(*hf_res):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try using ReturningThread to prototype handling multiple prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:05:53.341168Z",
     "start_time": "2022-04-10T21:05:53.294902Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import multiprocessing\n",
    "from threading import Lock\n",
    "\n",
    "from jabberwocky.utils import with_signature, JsonlinesLogger, load_api_key,\\\n",
    "    strip, squeeze, JsonlinesFormatter, touch, stream_multi_response, \\\n",
    "    containerize\n",
    "from jabberwocky.openai_utils import truncate_at_first_stop, \\\n",
    "    MockFunctionException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:05:53.919439Z",
     "start_time": "2022-04-10T21:05:53.886768Z"
    }
   },
   "outputs": [],
   "source": [
    "def thread_starmap(func, kwargs_list=None):\n",
    "    kwargs_list = kwargs_list or [{}]\n",
    "    threads = [ReturningThread(target=func, kwargs=kwargs)\n",
    "               for kwargs in tolist(kwargs_list)]\n",
    "    for thread in threads: thread.start()\n",
    "    return [thread.join() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:35:19.351923Z",
     "start_time": "2022-04-10T21:35:19.297815Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPTBackend:\n",
    "    \"\"\"\n",
    "    Examples\n",
    "    --------\n",
    "    gpt = GPTBackend()\n",
    "\n",
    "    # Default backend is openai.\n",
    "    openai_res = gpt.query(**kwargs)\n",
    "\n",
    "    with gpt('gooseai'):\n",
    "        # Now we're using the gooseai backend.\n",
    "        gooseai_res = gpt.query(**kwargs)\n",
    "\n",
    "    # Now we're back to using openai.\n",
    "    openai_res_2 = gpt.query(**kwargs)\n",
    "\n",
    "    # Now we'll switch to gooseai and changes will persist since we're not\n",
    "    # using a context manager.\n",
    "    gpt.switch('gooseai')\n",
    "    gooseai_res_2 = gpt.query(**kwargs)\n",
    "    \"\"\"\n",
    "\n",
    "    logger = JsonlinesLogger(\n",
    "        f'./data/logs/{datetime.today().strftime(\"%Y.%m.%d\")}.jsonlines'\n",
    "    )\n",
    "    lock = Lock()\n",
    "\n",
    "    # Only include backends here that actually should change the\n",
    "    # openai.api_base value (these will probably be backends that require no\n",
    "    # or minimal mock_funcs).\n",
    "    name2base = {\n",
    "        'openai': 'https://api.openai.com',\n",
    "        'gooseai': 'https://api.goose.ai/v1',\n",
    "    }\n",
    "\n",
    "    # Order matters: keep openai first so name2key initialization works.\n",
    "    name2func = {\n",
    "        'openai': query_gpt3,\n",
    "        'gooseai': query_gpt3,\n",
    "        'huggingface': query_gpt_huggingface,\n",
    "        'hobby': query_gpt_j,\n",
    "        'repeat': query_gpt_repeat,\n",
    "        'banana': query_gpt_banana\n",
    "    }\n",
    "\n",
    "    # Names of backends that perform stop word truncation how we want (i.e.\n",
    "    # allow us to specify stop phrases AND truncate before the phrase rather\n",
    "    # than after, if we encounter one).\n",
    "    skip_trunc = {'openai'}\n",
    "\n",
    "    name2key = {}\n",
    "    for name in name2func:\n",
    "        if name in {'hobby', 'repeat'}:\n",
    "            name2key[name] = f'<{name.upper()} BACKEND: FAKE API KEY>'\n",
    "        else:\n",
    "            name2key[name] = load_api_key(name)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.new_name = ''\n",
    "        self.old_name = ''\n",
    "        self.old_key = ''\n",
    "\n",
    "    def __call__(self, name):\n",
    "        \"\"\"__enter__ can't take arguments so we need to specify this here.\n",
    "        Notice that name is auto-lowercased and spaces are removed.\n",
    "        \"\"\"\n",
    "        new_name = name.lower().replace(' ', '')\n",
    "        if new_name not in self.name2func:\n",
    "            raise ValueError(f'Invalid name {name}. Valid options are: '\n",
    "                             f'{list(self.name2func)}')\n",
    "\n",
    "        self.new_name = new_name\n",
    "        self.old_name = self.current()\n",
    "        return self\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Change backend to the one specified in __call__, which is\n",
    "        automatically called first when using `with` syntax.\n",
    "        \"\"\"\n",
    "        print(f'Switching openai backend to \"{self.new_name}\".')\n",
    "        # Store an attribute on openai itself to reduce risk of bugs caused by\n",
    "        # GPTBackend being deleted or recreated. Previously used a\n",
    "        # self.base2name mapping to retrieve the current name but that doesn't\n",
    "        # work when multiple names use the same base (e.g. huggingface and\n",
    "        # hobby API backends can't be identified just by their base with\n",
    "        # this implementation).\n",
    "        openai.curr_name = self.new_name\n",
    "        self.old_key, openai.api_key = openai.api_key, \\\n",
    "            self.name2key[self.new_name]\n",
    "        if self.new_name in self.name2base:\n",
    "            openai.api_base = self.name2base[self.new_name]\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, traceback):\n",
    "        \"\"\"Revert to previously used backend on contextmanager exit.\"\"\"\n",
    "        print(f'Switching  backend back to \"{self.old_name}\".')\n",
    "        openai.api_key = self.old_key\n",
    "        if self.old_name in self.name2base:\n",
    "            openai.api_base = self.name2base[self.old_name]\n",
    "        openai.curr_name = self.old_name\n",
    "        self.clear()\n",
    "\n",
    "    @classmethod\n",
    "    def ls(cls):\n",
    "        \"\"\"Print current state of the backend: api_base, api_key, and \n",
    "        mock_func. Mostly useful for debugging and sanity checks.\n",
    "        \"\"\"\n",
    "        print('\\nBase:', openai.api_base)\n",
    "        print('Query func:', cls._get_query_func())\n",
    "\n",
    "    @classmethod\n",
    "    def backends(cls):\n",
    "        \"\"\"List all valid backend names. We could always access these via a\n",
    "        class attribute but this name is easier to remember.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[str]\n",
    "        \"\"\"\n",
    "        return list(cls.name2func)\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Reset instance variables tracking that were used to restore\n",
    "        previous backend.\n",
    "        \"\"\"\n",
    "        self.old_key = self.old_name = self.new_name = ''\n",
    "\n",
    "    def switch(self, name):\n",
    "        \"\"\"Switch backend and make changes persist, unlike in context manager\n",
    "        where we reset them on exit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name: str\n",
    "            One of (openai, gooseai).\n",
    "        \"\"\"\n",
    "        self(name=name).__enter__()\n",
    "        self.clear()\n",
    "\n",
    "    @staticmethod\n",
    "    def current():\n",
    "        \"\"\"Get current backend name, e.g. \"gooseai\". If we've ever switched\n",
    "        backend with GPTBackend, openai.curr_name\n",
    "        should exist. If not, the backend should be the default.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "        \"\"\"\n",
    "        return getattr(openai, 'curr_name', 'openai')\n",
    "\n",
    "    @classmethod\n",
    "    def _get_query_func(cls, backend=None):\n",
    "        \"\"\"Return current mock function (callable or None).\"\"\"\n",
    "        return cls.name2func[backend or cls.current()]\n",
    "\n",
    "    @classmethod\n",
    "    def key(cls):\n",
    "        \"\"\"Return current API key. In some cases this is a mock value since\n",
    "        some modes don't have a key.\n",
    "        \"\"\"\n",
    "        # More reliable than checking name2key because the openai attribute\n",
    "        # is what's actually used (at least for openai vs. gooseai -\n",
    "        # huggingface mock_func technically uses a global).\n",
    "        return openai.api_key\n",
    "\n",
    "    @classmethod\n",
    "    def engine(cls, engine_i, backend=None):\n",
    "        \"\"\"Get appropriate engine name depending on current api backend and\n",
    "        selected engine_i.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        engine_i: int\n",
    "            Number from 0-3 (inclusive) specifying which model to use. The two\n",
    "            backends *should* perform similar for values of 0-2, but openai's\n",
    "            3 (davinci, 175 billion parameters) is a much bigger model than\n",
    "            gooseai's 3 (NEO-X, 20 billion parameters). Mostly used in\n",
    "            query_gpt3().\n",
    "        backend: str or None\n",
    "            If provided, should be the name of a backend (e.g. 'huggingface'\n",
    "            or any of the keys in GPTBackend.backends()).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str: Name of an engine, e.g. \"davinci\" if we're in openai mode or\n",
    "        \"gpt-neo-20b\" if we're in gooseai mode.\n",
    "        \"\"\"\n",
    "        engines = C.backend_engines[backend or cls.current()]\n",
    "\n",
    "        # Adds better error message if user passes in a number too big for the\n",
    "        # current backend.\n",
    "        try:\n",
    "            return engines[engine_i]\n",
    "        except IndexError:\n",
    "            raise ValueError(f'Encountered invalid engine_i value: {engine_i}.'\n",
    "                             f'Should be one of {list(range(len(engines)))} '\n",
    "                             f'when using backend {cls.current()}.')\n",
    "\n",
    "    # Decorator order matters - doesn't work if we flip these.\n",
    "    @classmethod\n",
    "    @with_signature(query_gpt3)\n",
    "    @add_docstring(query_gpt3)\n",
    "    def query(cls, prompt, strip_output=True, log=True, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prompt\n",
    "        strip_output\n",
    "        log: bool or str\n",
    "            If True, the logfile defaults to a path like\n",
    "            './data/logs/2022.04.07.jsonlines' (current year, month, day).\n",
    "            If str, use that as the log path. If False or None, do not log.\n",
    "        kwargs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[str, dict]\n",
    "        \"\"\"\n",
    "        # TODO: testing\n",
    "#         if not isinstance(prompt, str):\n",
    "#             raise NotImplementedError(\n",
    "#                 f'Prompt must be str, not {type(prompt).__name__}.'\n",
    "#             )\n",
    "\n",
    "        # Keep trunc_full definition here so we can provide warnings if user\n",
    "        # is in stream mode.\n",
    "        query_func = cls._get_query_func()\n",
    "        trunc_full = cls.current() not in cls.skip_trunc\n",
    "        stream = kwargs.get('stream', False)\n",
    "        if stream:\n",
    "            if strip_output:\n",
    "                warnings.warn('strip_output=True is not supported in stream '\n",
    "                              'mode. Automatically setting it to False.')\n",
    "                strip_output = False\n",
    "            if trunc_full:\n",
    "                warnings.warn(\n",
    "                    'Streaming mode does not support manual truncation of '\n",
    "                    'stop phrases and your current backend has limited '\n",
    "                    'support for truncation.'\n",
    "                )\n",
    "\n",
    "        # V2 library no longer supports user passing in mock_func. We want to\n",
    "        # remove this from the kwargs we pass to our actual function.\n",
    "        kwargs_func = kwargs.pop('mock_func', None)\n",
    "        if kwargs_func:\n",
    "            raise ValueError(\n",
    "                f'Encountered unexpected mock_func {kwargs_func} with this '\n",
    "                'interface. This was part of the v1 library but is no longer '\n",
    "                'supported.'\n",
    "            )\n",
    "\n",
    "        start_i = kwargs.pop('start_i', 0)\n",
    "        n = kwargs.get('n', 1)\n",
    "        kwargs['prompt'] = prompt\n",
    "        cls._log_query_kwargs(log=log, query_func=query_func, **kwargs)\n",
    "        func_params = params(query_func)\n",
    "        \n",
    "        # Possibly easier for caller to check for errors this way? Mostly a\n",
    "        # holdover from v1 library design, but I'm not 100% sure if the\n",
    "        # benefits still hold given the new design.\n",
    "        try:\n",
    "#             text, full_response = query_func(**kwargs)\n",
    "            if n > 1 and n not in func_params:\n",
    "                del kwargs['n']\n",
    "                # If current query function doesn't natively support multiple\n",
    "                # completions, we can make multiple threaded requests. Need\n",
    "                # to unzip afterwards to regain the (texts, full_responses)\n",
    "                # structure.\n",
    "                response = thread_starmap(query_func, \n",
    "                                          [kwargs for _ in range(n)])\n",
    "                response = list(zip(*response))\n",
    "            else:\n",
    "                response = query_func(**kwargs)\n",
    "        except Exception as e:\n",
    "            raise MockFunctionException(str(e)) from None\n",
    "        if stream:\n",
    "#             if 'stream' in params(query_func):\n",
    "#                 return text, full_response\n",
    "#             # TODO: this isn't yet compatible w/ backends w/ native streaming\n",
    "#             # functionality. Think it should be simple to tweak though since\n",
    "#             # they provide 99% of what I want.\n",
    "#             return stream_multi_response(text, full_response, start_i=start_i)\n",
    "\n",
    "            if 'stream' in func_params:\n",
    "                return response\n",
    "            return stream_multi_response(*response, start_i=start_i)\n",
    "\n",
    "        text, full_response = containerize(*response)\n",
    "        # Manually check for stop phrases because most backends either don't\n",
    "        # or truncate AFTER the stop phrase which is rarely what we want.\n",
    "        stop = kwargs.get('stop', [])\n",
    "        clean_text = []\n",
    "        clean_full = []\n",
    "        for i, (text_, resp_) in enumerate(zip(text, full_response)):\n",
    "            text_ = truncate_at_first_stop(\n",
    "                text_,\n",
    "                stop_phrases=stop,\n",
    "                finish_reason=resp_.get('finish_reason', ''),\n",
    "                trunc_full=trunc_full,\n",
    "                trunc_partial=True\n",
    "            )\n",
    "            clean_text.append(strip(text_, strip_output))\n",
    "            clean_full.append({**resp_, 'prompt_index': i // n})\n",
    "\n",
    "        return clean_text, clean_full  # TODO: try keeping lists always\n",
    "#         return squeeze(clean_text, full_response, n=n)\n",
    "\n",
    "    @classmethod\n",
    "    def _log_query_kwargs(cls, log, query_func=None, **kwargs):\n",
    "        \"\"\"Log kwargs for troubleshooting purposes.\"\"\"\n",
    "        if log:\n",
    "            # Meta key is used to store any info we want to log but that should\n",
    "            # not be passed to the actual query_gpt3 call.\n",
    "            kwargs['meta'] = {\n",
    "                'backend_name': cls.current(),\n",
    "                'query_func': func_name(query_func) if query_func else None\n",
    "            }\n",
    "            with cls.lock:\n",
    "                if not isinstance(log, (str, Path)):\n",
    "                    log = cls.logger.path\n",
    "            \n",
    "            # If log file was deleted, we must recreate it AND use \n",
    "            # change_path to reopen the file object.\n",
    "                if not os.path.exists(log):\n",
    "                    touch(log)\n",
    "                    cls.logger.path = None\n",
    "                if log != cls.logger.path:\n",
    "                    cls.logger.change_path(log)\n",
    "            cls.logger.info(kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{func_name(self)} <current_name: {self.current()}>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:35:19.753759Z",
     "start_time": "2022-04-10T21:35:19.719151Z"
    }
   },
   "outputs": [],
   "source": [
    "@with_signature(query_gpt3)\n",
    "@add_docstring(query_gpt3)\n",
    "def query_batch(prompt, strip_output=True, log=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    # TODO: update. this is wrong now.\n",
    "    list: k tuples where the k'th tuple is the result of calling \n",
    "    GPTBackend.query() on the k'th input prompt. If stream=True, we instead\n",
    "    yield a series of (token_text, full_response) tuples. Depending on the\n",
    "    backend, different prompts' completions may be interspersed. You can use\n",
    "    the 'index' key in full_response to identify which response a token \n",
    "    belongs to.\n",
    "    \"\"\"\n",
    "    kwargs.update(strip_output=strip_output, log=log)\n",
    "    # Setting start_i to i*n ensures that the 'index' returned in streamed\n",
    "    # responses is different for each prompt's completion(s). Otherwise, \n",
    "    # because each query is run separately, each prompt's completion(s) would\n",
    "    # start at 0.\n",
    "    n = kwargs.get('n', 1)\n",
    "    threads = [\n",
    "        ReturningThread(target=GPTBackend.query,\n",
    "                        args=(p,),\n",
    "                        kwargs={**kwargs, 'start_i': i * n})\n",
    "        for i, p in enumerate(prompt)\n",
    "    ]\n",
    "    for thread in threads: thread.start()\n",
    "    res = [thread.join() for thread in threads]\n",
    "    if kwargs.get('stream', False):\n",
    "        return chain(*res)\n",
    "    texts, fulls = map(list, zip(*res))\n",
    "    print('fulls:', fulls)\n",
    "    return sum(texts, []), \\\n",
    "        sum([[{**d, 'prompt_index': d.get('prompt_index', 0) + i} for d in row] for i, row in enumerate(fulls)], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:35:20.005579Z",
     "start_time": "2022-04-10T21:35:19.970989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "fulls: [[{'prompt_index': 0}], [{'prompt_index': 0}], [{'prompt_index': 0}]]\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'I am so',\n",
    "    'Tomorrow is',\n",
    "    'The color'\n",
    "]\n",
    "gpt = GPTBackend()\n",
    "gpt.switch('repeat')\n",
    "batch_res = query_batch(prompts, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:35:20.434241Z",
     "start_time": "2022-04-10T21:35:20.395885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I AM SO', 'TOMORROW IS', 'THE COLOR'],\n",
       " [{'prompt_index': 0}, {'prompt_index': 1}, {'prompt_index': 2}])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:35:20.793608Z",
     "start_time": "2022-04-10T21:35:20.744227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'I am so', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'prompt': 'Tomorrow is', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'prompt': 'The color', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "fulls: [[{'prompt_index': 0}], [{'prompt_index': 0}], [{'prompt_index': 0}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['I AM SO', 'TOMORROW IS', 'THE COLOR'],\n",
       " [{'prompt_index': 0}, {'prompt_index': 1}, {'prompt_index': 2}])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_batch(prompts, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:57:08.729390Z",
     "start_time": "2022-04-10T21:57:08.691857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'I am so', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "fulls: [[{'prompt_index': 0}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['I AM SO'], [{'prompt_index': 0}])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_batch([prompts[0]], log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:35:21.078120Z",
     "start_time": "2022-04-10T21:35:20.911247Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm {GPTBackend.logger.path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:35:22.362956Z",
     "start_time": "2022-04-10T21:35:22.196861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /Users/hmamin/jabberwocky/data/logs/2022.04.10.jsonlines: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat {GPTBackend.logger.path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:51:16.946433Z",
     "start_time": "2022-04-10T21:51:16.904841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "Must provide an 'engine' parameter to create a <class 'openai.api_resources.completion.Completion'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-448-6f3eebf3a335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gooseai'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     tmp1 = gpt.query(prompts[0], n=3, max_tokens=3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtmp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, idempotency_key, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_required\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             raise error.InvalidRequestError(\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0;34m\"Must provide an 'engine' parameter to create a %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"engine\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             )\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Must provide an 'engine' parameter to create a <class 'openai.api_resources.completion.Completion'>"
     ]
    }
   ],
   "source": [
    "with gpt('gooseai'):\n",
    "#     tmp1 = gpt.query(prompts[0], n=3, max_tokens=3)\n",
    "    tmp1 = openai.Completion.create(prompts[0], n=3, max_tokens=3, engine=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:38:52.963771Z",
     "start_time": "2022-04-10T21:38:52.933209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I AM SO', 'I AM SO', 'I AM SO'],\n",
       " [{'prompt_index': 0}, {'prompt_index': 0}, {'prompt_index': 0}])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:36:48.445265Z",
     "start_time": "2022-04-10T21:36:44.613417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"gooseai\".\n",
      "{'n': 1, 'prompt': ['I am so', 'Tomorrow is', 'The color'], 'meta': {'backend_name': 'gooseai', 'query_func': 'query_gpt3'}}\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['glad you came by to see me.\" \"I wasn\\'t sure.\" \"I hadn\\'t heard from you.\" \"I thought you were still in New York.\" \"I was.\" \"I just got back this morning.\" \"How are you, Mr',\n",
       "  'the day. I feel it in my bones. This is going to be the day.\\n\\nI’m going to be done. I’m done with this.\\n\\nI’m not going to Suck it Up',\n",
       "  'of your skin isn’t the only part of your body that may be changing, according to new research.\\n\\nWhat’s more, it’s not necessarily a good thing, health experts say.\\n\\n“We'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x11f86ee60> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       5,\n",
       "       9,\n",
       "       14,\n",
       "       17,\n",
       "       20,\n",
       "       24,\n",
       "       27,\n",
       "       29,\n",
       "       31,\n",
       "       32,\n",
       "       37,\n",
       "       39,\n",
       "       44,\n",
       "       46,\n",
       "       48,\n",
       "       49,\n",
       "       54,\n",
       "       56,\n",
       "       62,\n",
       "       67,\n",
       "       71,\n",
       "       73,\n",
       "       75,\n",
       "       76,\n",
       "       84,\n",
       "       88,\n",
       "       93,\n",
       "       99,\n",
       "       102,\n",
       "       106,\n",
       "       111,\n",
       "       113,\n",
       "       115,\n",
       "       116,\n",
       "       120,\n",
       "       122,\n",
       "       124,\n",
       "       125,\n",
       "       130,\n",
       "       134,\n",
       "       139,\n",
       "       144,\n",
       "       152,\n",
       "       154,\n",
       "       156,\n",
       "       159,\n",
       "       163,\n",
       "       167,\n",
       "       168\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" glad\",\n",
       "       \" you\",\n",
       "       \" came\",\n",
       "       \" by\",\n",
       "       \" to\",\n",
       "       \" see\",\n",
       "       \" me\",\n",
       "       \".\\\"\",\n",
       "       \" \\\"\",\n",
       "       \"I\",\n",
       "       \" wasn\",\n",
       "       \"'t\",\n",
       "       \" sure\",\n",
       "       \".\\\"\",\n",
       "       \" \\\"\",\n",
       "       \"I\",\n",
       "       \" hadn\",\n",
       "       \"'t\",\n",
       "       \" heard\",\n",
       "       \" from\",\n",
       "       \" you\",\n",
       "       \".\\\"\",\n",
       "       \" \\\"\",\n",
       "       \"I\",\n",
       "       \" thought\",\n",
       "       \" you\",\n",
       "       \" were\",\n",
       "       \" still\",\n",
       "       \" in\",\n",
       "       \" New\",\n",
       "       \" York\",\n",
       "       \".\\\"\",\n",
       "       \" \\\"\",\n",
       "       \"I\",\n",
       "       \" was\",\n",
       "       \".\\\"\",\n",
       "       \" \\\"\",\n",
       "       \"I\",\n",
       "       \" just\",\n",
       "       \" got\",\n",
       "       \" back\",\n",
       "       \" this\",\n",
       "       \" morning\",\n",
       "       \".\\\"\",\n",
       "       \" \\\"\",\n",
       "       \"How\",\n",
       "       \" are\",\n",
       "       \" you\",\n",
       "       \",\",\n",
       "       \" Mr\"\n",
       "     ]\n",
       "   },\n",
       "   'text': ' glad you came by to see me.\" \"I wasn\\'t sure.\" \"I hadn\\'t heard from you.\" \"I thought you were still in New York.\" \"I was.\" \"I just got back this morning.\" \"How are you, Mr',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x11f86ef68> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       4,\n",
       "       8,\n",
       "       9,\n",
       "       11,\n",
       "       16,\n",
       "       19,\n",
       "       22,\n",
       "       25,\n",
       "       31,\n",
       "       32,\n",
       "       37,\n",
       "       40,\n",
       "       46,\n",
       "       49,\n",
       "       52,\n",
       "       56,\n",
       "       60,\n",
       "       61,\n",
       "       62,\n",
       "       63,\n",
       "       64,\n",
       "       64,\n",
       "       65,\n",
       "       66,\n",
       "       72,\n",
       "       75,\n",
       "       78,\n",
       "       83,\n",
       "       84,\n",
       "       86,\n",
       "       86,\n",
       "       87,\n",
       "       88,\n",
       "       93,\n",
       "       98,\n",
       "       103,\n",
       "       104,\n",
       "       105,\n",
       "       106,\n",
       "       107,\n",
       "       107,\n",
       "       108,\n",
       "       109,\n",
       "       113,\n",
       "       119,\n",
       "       122,\n",
       "       124,\n",
       "       127,\n",
       "       130\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" the\",\n",
       "       \" day\",\n",
       "       \".\",\n",
       "       \" I\",\n",
       "       \" feel\",\n",
       "       \" it\",\n",
       "       \" in\",\n",
       "       \" my\",\n",
       "       \" bones\",\n",
       "       \".\",\n",
       "       \" This\",\n",
       "       \" is\",\n",
       "       \" going\",\n",
       "       \" to\",\n",
       "       \" be\",\n",
       "       \" the\",\n",
       "       \" day\",\n",
       "       \".\",\n",
       "       \"bytes:'\\\\n'\",\n",
       "       \"bytes:'\\\\n'\",\n",
       "       \"I\",\n",
       "       \"\\ufffd\",\n",
       "       \"\\ufffd\",\n",
       "       \"m\",\n",
       "       \" going\",\n",
       "       \" to\",\n",
       "       \" be\",\n",
       "       \" done\",\n",
       "       \".\",\n",
       "       \" I\",\n",
       "       \"\\ufffd\",\n",
       "       \"\\ufffd\",\n",
       "       \"m\",\n",
       "       \" done\",\n",
       "       \" with\",\n",
       "       \" this\",\n",
       "       \".\",\n",
       "       \"bytes:'\\\\n'\",\n",
       "       \"bytes:'\\\\n'\",\n",
       "       \"I\",\n",
       "       \"\\ufffd\",\n",
       "       \"\\ufffd\",\n",
       "       \"m\",\n",
       "       \" not\",\n",
       "       \" going\",\n",
       "       \" to\",\n",
       "       \" S\",\n",
       "       \"uck\",\n",
       "       \" it\",\n",
       "       \" Up\"\n",
       "     ]\n",
       "   },\n",
       "   'text': ' the day. I feel it in my bones. This is going to be the day.\\n\\nI’m going to be done. I’m done with this.\\n\\nI’m not going to Suck it Up',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 1},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 2,\n",
       "   'logprobs': <OpenAIObject at 0x11f86e2b0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       3,\n",
       "       8,\n",
       "       13,\n",
       "       17,\n",
       "       17,\n",
       "       18,\n",
       "       19,\n",
       "       23,\n",
       "       28,\n",
       "       33,\n",
       "       36,\n",
       "       41,\n",
       "       46,\n",
       "       51,\n",
       "       55,\n",
       "       58,\n",
       "       67,\n",
       "       68,\n",
       "       78,\n",
       "       81,\n",
       "       85,\n",
       "       94,\n",
       "       95,\n",
       "       96,\n",
       "       97,\n",
       "       101,\n",
       "       101,\n",
       "       102,\n",
       "       103,\n",
       "       108,\n",
       "       109,\n",
       "       112,\n",
       "       112,\n",
       "       113,\n",
       "       114,\n",
       "       118,\n",
       "       130,\n",
       "       132,\n",
       "       137,\n",
       "       143,\n",
       "       144,\n",
       "       151,\n",
       "       159,\n",
       "       163,\n",
       "       164,\n",
       "       165,\n",
       "       166,\n",
       "       166,\n",
       "       167\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" of\",\n",
       "       \" your\",\n",
       "       \" skin\",\n",
       "       \" isn\",\n",
       "       \"\\ufffd\",\n",
       "       \"\\ufffd\",\n",
       "       \"t\",\n",
       "       \" the\",\n",
       "       \" only\",\n",
       "       \" part\",\n",
       "       \" of\",\n",
       "       \" your\",\n",
       "       \" body\",\n",
       "       \" that\",\n",
       "       \" may\",\n",
       "       \" be\",\n",
       "       \" changing\",\n",
       "       \",\",\n",
       "       \" according\",\n",
       "       \" to\",\n",
       "       \" new\",\n",
       "       \" research\",\n",
       "       \".\",\n",
       "       \"bytes:'\\\\n'\",\n",
       "       \"bytes:'\\\\n'\",\n",
       "       \"What\",\n",
       "       \"\\ufffd\",\n",
       "       \"\\ufffd\",\n",
       "       \"s\",\n",
       "       \" more\",\n",
       "       \",\",\n",
       "       \" it\",\n",
       "       \"\\ufffd\",\n",
       "       \"\\ufffd\",\n",
       "       \"s\",\n",
       "       \" not\",\n",
       "       \" necessarily\",\n",
       "       \" a\",\n",
       "       \" good\",\n",
       "       \" thing\",\n",
       "       \",\",\n",
       "       \" health\",\n",
       "       \" experts\",\n",
       "       \" say\",\n",
       "       \".\",\n",
       "       \"bytes:'\\\\n'\",\n",
       "       \"bytes:'\\\\n'\",\n",
       "       \"\\ufffd\",\n",
       "       \"\\ufffd\",\n",
       "       \"We\"\n",
       "     ]\n",
       "   },\n",
       "   'text': ' of your skin isn’t the only part of your body that may be changing, according to new research.\\n\\nWhat’s more, it’s not necessarily a good thing, health experts say.\\n\\n“We',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 2}])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gpt('gooseai'):\n",
    "    tmp = gpt.query(prompts, n=1)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:37:06.908116Z",
     "start_time": "2022-04-10T21:37:06.874152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glad you came by to see me.\" \"I wasn\\'t sure.\" \"I hadn\\'t heard from you.\" \"I thought you were still in New York.\" \"I was.\" \"I just got back this morning.\" \"How are you, Mr',\n",
       " 'the day. I feel it in my bones. This is going to be the day.\\n\\nI’m going to be done. I’m done with this.\\n\\nI’m not going to Suck it Up',\n",
       " 'of your skin isn’t the only part of your body that may be changing, according to new research.\\n\\nWhat’s more, it’s not necessarily a good thing, health experts say.\\n\\n“We']"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:37:49.604417Z",
     "start_time": "2022-04-10T21:37:49.573094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 0, 'prompt_index': 0},\n",
       " {'index': 1, 'prompt_index': 1},\n",
       " {'index': 2, 'prompt_index': 2}]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[select(row, keep=['index', 'prompt_index']) for row in tmp[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:33:53.803597Z",
     "start_time": "2022-04-10T21:33:53.764935Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 3, 'prompt': 'I am so', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 3, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 3, 'prompt': 'The color', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "fulls: [[{'prompt_index': 0}, {'prompt_index': 0}, {'prompt_index': 0}], [{'prompt_index': 0}, {'prompt_index': 0}, {'prompt_index': 0}], [{'prompt_index': 0}, {'prompt_index': 0}, {'prompt_index': 0}]]\n"
     ]
    }
   ],
   "source": [
    "tmp = query_batch(prompts, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:33:54.608639Z",
     "start_time": "2022-04-10T21:33:54.558232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I AM SO',\n",
       "  'I AM SO',\n",
       "  'I AM SO',\n",
       "  'TOMORROW IS',\n",
       "  'TOMORROW IS',\n",
       "  'TOMORROW IS',\n",
       "  'THE COLOR',\n",
       "  'THE COLOR',\n",
       "  'THE COLOR'],\n",
       " [{'prompt_index': 0},\n",
       "  {'prompt_index': 0},\n",
       "  {'prompt_index': 0},\n",
       "  {'prompt_index': 1},\n",
       "  {'prompt_index': 1},\n",
       "  {'prompt_index': 1},\n",
       "  {'prompt_index': 2},\n",
       "  {'prompt_index': 2},\n",
       "  {'prompt_index': 2}])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:33:59.895616Z",
     "start_time": "2022-04-10T21:33:59.845819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 1, 'stream': True, 'prompt': 'I am so', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 1, 'stream': True, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 1, 'stream': True, 'prompt': 'The color', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "I  {'index': 0, 'finish_reason': None}\n",
      "AM  {'index': 0, 'finish_reason': None}\n",
      "SO  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "TOMORROW  {'index': 1, 'finish_reason': None}\n",
      "IS  {'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "THE  {'index': 2, 'finish_reason': None}\n",
      "COLOR  {'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:232: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:237: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n"
     ]
    }
   ],
   "source": [
    "for tok_, full_ in query_batch(prompts, n=1, stream=True):\n",
    "    print(tok_, full_)\n",
    "    if full_['finish_reason']: \n",
    "        print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:34:11.064368Z",
     "start_time": "2022-04-10T21:34:11.028305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  {'index': 0, 'finish_reason': None}\n",
      "AM  {'index': 0, 'finish_reason': None}\n",
      "SO  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "I  {'index': 1, 'finish_reason': None}\n",
      "AM  {'index': 1, 'finish_reason': None}\n",
      "SO  {'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "TOMORROW  {'index': 2, 'finish_reason': None}\n",
      "IS  {'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "TOMORROW  {'index': 3, 'finish_reason': None}\n",
      "IS  {'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "THE  {'index': 4, 'finish_reason': None}\n",
      "COLOR  {'index': 4, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "THE  {'index': 5, 'finish_reason': None}\n",
      "COLOR  {'index': 5, 'finish_reason': 'dummy'}\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:232: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:237: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n"
     ]
    }
   ],
   "source": [
    "for tok_, full_ in query_batch(prompts, n=2, log=False, stream=True):\n",
    "    print(tok_, full_)\n",
    "    if full_['finish_reason']: \n",
    "        print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:09:48.879805Z",
     "start_time": "2022-04-10T21:09:47.539612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "{'max_tokens': 5, 'prompt': 'I am so', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'max_tokens': 5, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'max_tokens': 5, 'prompt': 'The color', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "RESP: (' the last day of the', {'id': 'c8c01ca9-5fc0-49d3-8f5b-91d11935cc72', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the last day of the', 'input': 'Tomorrow is'}]})\n",
      "AFTER CONTAINERIZE: [' the last day of the'] [{'id': 'c8c01ca9-5fc0-49d3-8f5b-91d11935cc72', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the last day of the', 'input': 'Tomorrow is'}]}]\n",
      "RESP: (' excited to share my favorite', {'id': '32153ba1-cd85-48ce-8bae-d5466802ef3d', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' excited to share my favorite', 'input': 'I am so'}]})\n",
      "AFTER CONTAINERIZE: [' excited to share my favorite'] [{'id': '32153ba1-cd85-48ce-8bae-d5466802ef3d', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' excited to share my favorite', 'input': 'I am so'}]}]\n",
      "RESP: (' of a piece of fruit', {'id': '990d70aa-1fb6-4891-b3d3-59093654c78a', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of a piece of fruit', 'input': 'The color'}]})\n",
      "AFTER CONTAINERIZE: [' of a piece of fruit'] [{'id': '990d70aa-1fb6-4891-b3d3-59093654c78a', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of a piece of fruit', 'input': 'The color'}]}]\n",
      "R: [(['excited to share my favorite'], [{'id': '32153ba1-cd85-48ce-8bae-d5466802ef3d', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' excited to share my favorite', 'input': 'I am so'}]}]), (['the last day of the'], [{'id': 'c8c01ca9-5fc0-49d3-8f5b-91d11935cc72', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the last day of the', 'input': 'Tomorrow is'}]}]), (['of a piece of fruit'], [{'id': '990d70aa-1fb6-4891-b3d3-59093654c78a', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of a piece of fruit', 'input': 'The color'}]}])]\n",
      "T: [['excited to share my favorite'], ['the last day of the'], ['of a piece of fruit']] F: [[{'id': '32153ba1-cd85-48ce-8bae-d5466802ef3d', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' excited to share my favorite', 'input': 'I am so'}]}], [{'id': 'c8c01ca9-5fc0-49d3-8f5b-91d11935cc72', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the last day of the', 'input': 'Tomorrow is'}]}], [{'id': '990d70aa-1fb6-4891-b3d3-59093654c78a', 'message': 'success', 'created': 1649624988, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of a piece of fruit', 'input': 'The color'}]}]]\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('banana'):\n",
    "    banana_batch_res = query_batch(prompts, max_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:09:55.472811Z",
     "start_time": "2022-04-10T21:09:55.444271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['excited to share my favorite',\n",
       "  'the last day of the',\n",
       "  'of a piece of fruit'],\n",
       " [{'id': '32153ba1-cd85-48ce-8bae-d5466802ef3d',\n",
       "   'message': 'success',\n",
       "   'created': 1649624988,\n",
       "   'apiVersion': '26 Nov 2021',\n",
       "   'modelOutputs': [{'output': ' excited to share my favorite',\n",
       "     'input': 'I am so'}]},\n",
       "  {'id': 'c8c01ca9-5fc0-49d3-8f5b-91d11935cc72',\n",
       "   'message': 'success',\n",
       "   'created': 1649624988,\n",
       "   'apiVersion': '26 Nov 2021',\n",
       "   'modelOutputs': [{'output': ' the last day of the',\n",
       "     'input': 'Tomorrow is'}]},\n",
       "  {'id': '990d70aa-1fb6-4891-b3d3-59093654c78a',\n",
       "   'message': 'success',\n",
       "   'created': 1649624988,\n",
       "   'apiVersion': '26 Nov 2021',\n",
       "   'modelOutputs': [{'output': ' of a piece of fruit',\n",
       "     'input': 'The color'}]}])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_batch_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:10:13.178484Z",
     "start_time": "2022-04-10T21:10:11.277506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "{'n': 2, 'max_tokens': 5, 'prompt': 'I am so', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'max_tokens': 5, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'max_tokens': 5, 'prompt': 'The color', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "RESP: [(' the day when the long', ' the day of reckoning for'), ({'id': '970410cd-d149-43c1-a7a3-411e9f0d578c', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the day when the long', 'input': 'Tomorrow is'}]}, {'id': '5ac5b6b8-424f-4f73-90af-0dbd2019d59f', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the day of reckoning for', 'input': 'Tomorrow is'}]})]\n",
      "AFTER CONTAINERIZE: [' the day when the long', ' the day of reckoning for'] [{'id': '970410cd-d149-43c1-a7a3-411e9f0d578c', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the day when the long', 'input': 'Tomorrow is'}]}, {'id': '5ac5b6b8-424f-4f73-90af-0dbd2019d59f', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the day of reckoning for', 'input': 'Tomorrow is'}]}]\n",
      "RESP: [(' glad that I came across', ' excited to have a chance'), ({'id': 'b06ca4b9-7fed-4419-ac3d-cf4a4b8dac1a', 'message': 'success', 'created': 1649625011, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' glad that I came across', 'input': 'I am so'}]}, {'id': 'c6d85f99-fdb4-4dab-8bf0-8ec21d02d1b3', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' excited to have a chance', 'input': 'I am so'}]})]\n",
      "AFTER CONTAINERIZE: [' glad that I came across', ' excited to have a chance'] [{'id': 'b06ca4b9-7fed-4419-ac3d-cf4a4b8dac1a', 'message': 'success', 'created': 1649625011, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' glad that I came across', 'input': 'I am so'}]}, {'id': 'c6d85f99-fdb4-4dab-8bf0-8ec21d02d1b3', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' excited to have a chance', 'input': 'I am so'}]}]\n",
      "RESP: [(' of this set of children', ' of the chart represents the'), ({'id': 'e8b73924-315c-4ecc-a1b1-bcb1cccce101', 'message': 'success', 'created': 1649625013, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of this set of children', 'input': 'The color'}]}, {'id': '6b3583aa-be10-4831-9813-43b5afd6ba24', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of the chart represents the', 'input': 'The color'}]})]\n",
      "AFTER CONTAINERIZE: [' of this set of children', ' of the chart represents the'] [{'id': 'e8b73924-315c-4ecc-a1b1-bcb1cccce101', 'message': 'success', 'created': 1649625013, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of this set of children', 'input': 'The color'}]}, {'id': '6b3583aa-be10-4831-9813-43b5afd6ba24', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of the chart represents the', 'input': 'The color'}]}]\n",
      "R: [(['glad that I came across', 'excited to have a chance'], [{'id': 'b06ca4b9-7fed-4419-ac3d-cf4a4b8dac1a', 'message': 'success', 'created': 1649625011, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' glad that I came across', 'input': 'I am so'}]}, {'id': 'c6d85f99-fdb4-4dab-8bf0-8ec21d02d1b3', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' excited to have a chance', 'input': 'I am so'}]}]), (['the day when the long', 'the day of reckoning for'], [{'id': '970410cd-d149-43c1-a7a3-411e9f0d578c', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the day when the long', 'input': 'Tomorrow is'}]}, {'id': '5ac5b6b8-424f-4f73-90af-0dbd2019d59f', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the day of reckoning for', 'input': 'Tomorrow is'}]}]), (['of this set of children', 'of the chart represents the'], [{'id': 'e8b73924-315c-4ecc-a1b1-bcb1cccce101', 'message': 'success', 'created': 1649625013, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of this set of children', 'input': 'The color'}]}, {'id': '6b3583aa-be10-4831-9813-43b5afd6ba24', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of the chart represents the', 'input': 'The color'}]}])]\n",
      "T: [['glad that I came across', 'excited to have a chance'], ['the day when the long', 'the day of reckoning for'], ['of this set of children', 'of the chart represents the']] F: [[{'id': 'b06ca4b9-7fed-4419-ac3d-cf4a4b8dac1a', 'message': 'success', 'created': 1649625011, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' glad that I came across', 'input': 'I am so'}]}, {'id': 'c6d85f99-fdb4-4dab-8bf0-8ec21d02d1b3', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' excited to have a chance', 'input': 'I am so'}]}], [{'id': '970410cd-d149-43c1-a7a3-411e9f0d578c', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the day when the long', 'input': 'Tomorrow is'}]}, {'id': '5ac5b6b8-424f-4f73-90af-0dbd2019d59f', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the day of reckoning for', 'input': 'Tomorrow is'}]}], [{'id': 'e8b73924-315c-4ecc-a1b1-bcb1cccce101', 'message': 'success', 'created': 1649625013, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of this set of children', 'input': 'The color'}]}, {'id': '6b3583aa-be10-4831-9813-43b5afd6ba24', 'message': 'success', 'created': 1649625012, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of the chart represents the', 'input': 'The color'}]}]]\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('banana'):\n",
    "    banana_batch_res = query_batch(prompts, n=2, max_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:10:13.539141Z",
     "start_time": "2022-04-10T21:10:13.495333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(banana_batch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:10:17.817271Z",
     "start_time": "2022-04-10T21:10:17.757869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glad that I came across',\n",
       " 'excited to have a chance',\n",
       " 'the day when the long',\n",
       " 'the day of reckoning for',\n",
       " 'of this set of children',\n",
       " 'of the chart represents the']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_batch_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:10:21.313440Z",
     "start_time": "2022-04-10T21:10:21.274777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'b06ca4b9-7fed-4419-ac3d-cf4a4b8dac1a',\n",
       "  'message': 'success',\n",
       "  'created': 1649625011,\n",
       "  'apiVersion': '26 Nov 2021',\n",
       "  'modelOutputs': [{'output': ' glad that I came across',\n",
       "    'input': 'I am so'}]},\n",
       " {'id': 'c6d85f99-fdb4-4dab-8bf0-8ec21d02d1b3',\n",
       "  'message': 'success',\n",
       "  'created': 1649625012,\n",
       "  'apiVersion': '26 Nov 2021',\n",
       "  'modelOutputs': [{'output': ' excited to have a chance',\n",
       "    'input': 'I am so'}]},\n",
       " {'id': '970410cd-d149-43c1-a7a3-411e9f0d578c',\n",
       "  'message': 'success',\n",
       "  'created': 1649625012,\n",
       "  'apiVersion': '26 Nov 2021',\n",
       "  'modelOutputs': [{'output': ' the day when the long',\n",
       "    'input': 'Tomorrow is'}]},\n",
       " {'id': '5ac5b6b8-424f-4f73-90af-0dbd2019d59f',\n",
       "  'message': 'success',\n",
       "  'created': 1649625012,\n",
       "  'apiVersion': '26 Nov 2021',\n",
       "  'modelOutputs': [{'output': ' the day of reckoning for',\n",
       "    'input': 'Tomorrow is'}]},\n",
       " {'id': 'e8b73924-315c-4ecc-a1b1-bcb1cccce101',\n",
       "  'message': 'success',\n",
       "  'created': 1649625013,\n",
       "  'apiVersion': '26 Nov 2021',\n",
       "  'modelOutputs': [{'output': ' of this set of children',\n",
       "    'input': 'The color'}]},\n",
       " {'id': '6b3583aa-be10-4831-9813-43b5afd6ba24',\n",
       "  'message': 'success',\n",
       "  'created': 1649625012,\n",
       "  'apiVersion': '26 Nov 2021',\n",
       "  'modelOutputs': [{'output': ' of the chart represents the',\n",
       "    'input': 'The color'}]}]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_batch_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:52:27.157637Z",
     "start_time": "2022-04-09T22:52:25.192411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "gpt GPTBackend <current_name: banana>\n",
      "{'n': 2, 'max_tokens': 5, 'stream': True, 'prompt': 'The color', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'max_tokens': 5, 'stream': True, 'prompt': 'Tomorrow is', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'max_tokens': 5, 'stream': True, 'prompt': 'I am so', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:237: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "/Users/hmamin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:232: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:371: UserWarning: query_gpt_banana received unused kwargs {'stream': True}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  {'index': 0, 'finish_reason': None}\n",
      "happy  {'index': 0, 'finish_reason': None}\n",
      "I  {'index': 0, 'finish_reason': None}\n",
      "found  {'index': 0, 'finish_reason': None}\n",
      "this  {'index': 0, 'finish_reason': None}\n",
      "forum  {'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 1, 'finish_reason': None}\n",
      "happy  {'index': 1, 'finish_reason': None}\n",
      "I  {'index': 1, 'finish_reason': None}\n",
      "found  {'index': 1, 'finish_reason': None}\n",
      "this  {'index': 1, 'finish_reason': None}\n",
      "article  {'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 2, 'finish_reason': None}\n",
      "the  {'index': 2, 'finish_reason': None}\n",
      "last  {'index': 2, 'finish_reason': None}\n",
      "day  {'index': 2, 'finish_reason': None}\n",
      "of  {'index': 2, 'finish_reason': None}\n",
      "my  {'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 3, 'finish_reason': None}\n",
      "the  {'index': 3, 'finish_reason': None}\n",
      "day  {'index': 3, 'finish_reason': None}\n",
      "of  {'index': 3, 'finish_reason': None}\n",
      "the  {'index': 3, 'finish_reason': None}\n",
      "annual  {'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 4, 'finish_reason': None}\n",
      "of  {'index': 4, 'finish_reason': None}\n",
      "a  {'index': 4, 'finish_reason': None}\n",
      "peacock�  {'index': 4, 'finish_reason': 'dummy'}\n",
      "\n",
      "  {'index': 5, 'finish_reason': None}\n",
      "of  {'index': 5, 'finish_reason': None}\n",
      "this  {'index': 5, 'finish_reason': None}\n",
      "fabric  {'index': 5, 'finish_reason': None}\n",
      "is  {'index': 5, 'finish_reason': None}\n",
      "so  {'index': 5, 'finish_reason': 'dummy'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('banana')\n",
    "for tok_, full_ in query_batch(prompts, n=2, max_tokens=5, stream=True):\n",
    "    print(tok_, select(full_, keep=['index', 'finish_reason']))\n",
    "    if full_['finish_reason']: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T20:55:04.958881Z",
     "start_time": "2022-04-09T20:55:04.898385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reconstruct what query_gpt3 would have returned here.\n",
    "chunks = _open_res\n",
    "texts = (chunk['choices'][0]['text'] for chunk in chunks)\n",
    "chunks = (dict(chunk['choices'][0]) for chunk in chunks)\n",
    "open_res = list(zip(texts, chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T20:55:05.432293Z",
     "start_time": "2022-04-09T20:55:05.387360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Feeling', {'finish_reason': None, 'index': 0, 'logprobs': {'text_offset': [24], 'token_logprobs': [-8.621929], 'tokens': [' Feeling'], 'top_logprobs': [{'\\n': -2.4186804, ' I': -2.6238666, ' She': -2.4244554}]}, 'text': ' Feeling'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' her', {'finish_reason': None, 'index': 0, 'logprobs': {'text_offset': [32], 'token_logprobs': [-5.186215], 'tokens': [' her'], 'top_logprobs': [{' a': -2.1202018, ' like': -2.6433406, ' the': -2.8912876}]}, 'text': ' her'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' I', {'finish_reason': None, 'index': 1, 'logprobs': {'text_offset': [24], 'token_logprobs': [-2.6238666], 'tokens': [' I'], 'top_logprobs': [{'\\n': -2.4186804, ' I': -2.6238666, ' She': -2.4244554}]}, 'text': ' I'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(\"'m\", {'finish_reason': None, 'index': 1, 'logprobs': {'text_offset': [26], 'token_logprobs': [-2.2295423], 'tokens': [\"'m\"], 'top_logprobs': [{' don': -2.8276584, \"'m\": -2.2295423, 'bytes:\\\\xe2\\\\x80': -2.7309577}]}, 'text': \"'m\"})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' sure', {'finish_reason': None, 'index': 1, 'logprobs': {'text_offset': [28], 'token_logprobs': [-2.6193585], 'tokens': [' sure'], 'top_logprobs': [{' going': -1.7995286, ' not': -2.3888402, ' sure': -2.6193585}]}, 'text': ' sure'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' presence', {'finish_reason': None, 'index': 0, 'logprobs': {'text_offset': [36], 'token_logprobs': [-3.293669], 'tokens': [' presence'], 'top_logprobs': [{' up': -2.2750113, ' way': -2.5486472, ',': -3.0275166}]}, 'text': ' presence'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' you', {'finish_reason': None, 'index': 1, 'logprobs': {'text_offset': [33], 'token_logprobs': [-1.7617589], 'tokens': [' you'], 'top_logprobs': [{' he': -2.186368, ' she': -1.4318895, ' you': -1.7617589}]}, 'text': ' you'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(',', {'finish_reason': None, 'index': 0, 'logprobs': {'text_offset': [45], 'token_logprobs': [-1.0045006], 'tokens': [','], 'top_logprobs': [{' in': -2.5863404, ' is': -2.647669, ',': -1.0045006}]}, 'text': ','})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' Des', {'finish_reason': 'length', 'index': 0, 'logprobs': {'text_offset': [46], 'token_logprobs': [-8.532746], 'tokens': [' Des'], 'top_logprobs': [{' I': -1.8522211, ' he': -2.4737713, ' the': -2.7948432}]}, 'text': ' Des'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "(' can', {'finish_reason': 'length', 'index': 1, 'logprobs': {'text_offset': [37], 'token_logprobs': [-2.7016406], 'tokens': [' can'], 'top_logprobs': [{\"'ll\": -1.5801843, \"'re\": -2.1810775, \"'ve\": -1.8903823}]}, 'text': ' can'})\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in open_res:\n",
    "    print(row)\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-09T22:31:22.757142Z",
     "start_time": "2022-04-09T22:31:22.726046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to data/tmp/_goose_res_multi.pkl.\n",
      "Writing data to data/tmp/_goose_res.pkl.\n",
      "Writing data to data/tmp/_open_res.pkl.\n"
     ]
    }
   ],
   "source": [
    "save(_goose_res_multi, 'data/tmp/_goose_res_multi.pkl')\n",
    "save(_goose_res, 'data/tmp/_goose_res.pkl')\n",
    "save(_open_res, 'data/tmp/_open_res.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:06:43.856860Z",
     "start_time": "2022-04-10T20:06:43.818492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/tmp/_goose_res_multi.pkl.\n",
      "Object loaded from data/tmp/_goose_res.pkl.\n",
      "Object loaded from data/tmp/_open_res.pkl.\n"
     ]
    }
   ],
   "source": [
    "_goose_res_multi = load('data/tmp/_goose_res_multi.pkl')\n",
    "_goose_res = load('data/tmp/_goose_res.pkl')\n",
    "_open_res = load('data/tmp/_open_res.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:10:57.334392Z",
     "start_time": "2022-04-10T20:10:57.306108Z"
    }
   },
   "outputs": [],
   "source": [
    "chunks_multi = _goose_res_multi\n",
    "texts_multi = (chunk['choices'][0]['text'] for chunk in chunks_multi)\n",
    "chunks_multi = (dict(chunk['choices'][0]) for chunk in chunks_multi)\n",
    "goose_res_multi = list(zip(texts_multi, chunks_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:11:49.858540Z",
     "start_time": "2022-04-10T20:11:49.818389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' one',\n",
       "  {'finish_reason': None,\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x11f4d92b0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -3.693359375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" one\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' one',\n",
       "   'token_index': 0}),\n",
       " (' of',\n",
       "  {'finish_reason': None,\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x11f4d90f8> JSON: {\n",
       "     \"text_offset\": [\n",
       "       4\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -0.1173095703125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" of\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" for\": -4.04296875,\n",
       "         \" hell\": -3.841796875,\n",
       "         \" of\": -0.1173095703125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' of',\n",
       "   'token_index': 1}),\n",
       " (' the',\n",
       "  {'finish_reason': None,\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x11f4d9518> JSON: {\n",
       "     \"text_offset\": [\n",
       "       7\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.0634765625\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" the\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" my\": -2.708984375,\n",
       "         \" the\": -1.0634765625,\n",
       "         \" those\": -0.6142578125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' the',\n",
       "   'token_index': 2}),\n",
       " (' most',\n",
       "  {'finish_reason': None,\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x11f4d98e0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       11\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -0.98974609375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" most\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" best\": -2.453125,\n",
       "         \" most\": -0.98974609375,\n",
       "         \" worst\": -2.39453125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' most',\n",
       "   'token_index': 3}),\n",
       " (' important',\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x11f4d9a98> JSON: {\n",
       "     \"text_offset\": [\n",
       "       16\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -3.12890625\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" important\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" exciting\": -2.642578125,\n",
       "         \" important\": -3.12890625,\n",
       "         \" memorable\": -3.162109375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' important',\n",
       "   'token_index': 4}),\n",
       " (' of',\n",
       "  {'finish_reason': None,\n",
       "   'index': 2,\n",
       "   'logprobs': <OpenAIObject at 0x11f4d9c50> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -2.251953125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" of\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" of\": -2.251953125,\n",
       "         \" people\": -2.943359375,\n",
       "         \" times\": -1.8173828125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' of',\n",
       "   'token_index': 0}),\n",
       " (' you',\n",
       "  {'finish_reason': None,\n",
       "   'index': 2,\n",
       "   'logprobs': <OpenAIObject at 0x11f4d9e08> JSON: {\n",
       "     \"text_offset\": [\n",
       "       3\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -0.462646484375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" you\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" these\": -2.923828125,\n",
       "         \" us\": -1.86328125,\n",
       "         \" you\": -0.462646484375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' you',\n",
       "   'token_index': 1}),\n",
       " (' have',\n",
       "  {'finish_reason': None,\n",
       "   'index': 2,\n",
       "   'logprobs': <OpenAIObject at 0x11f4d9fc0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       7\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.345703125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" have\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" are\": -2.0859375,\n",
       "         \" have\": -1.345703125,\n",
       "         \" know\": -3.22265625\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' have',\n",
       "   'token_index': 2}),\n",
       " (' heard',\n",
       "  {'finish_reason': None,\n",
       "   'index': 2,\n",
       "   'logprobs': <OpenAIObject at 0x11f6911a8> JSON: {\n",
       "     \"text_offset\": [\n",
       "       12\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -2.71875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" heard\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" been\": -2.169921875,\n",
       "         \" heard\": -2.71875,\n",
       "         \" seen\": -2.775390625\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' heard',\n",
       "   'token_index': 3}),\n",
       " (' or',\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 2,\n",
       "   'logprobs': <OpenAIObject at 0x11f691360> JSON: {\n",
       "     \"text_offset\": [\n",
       "       18\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -3.859375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" or\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" about\": -2.1171875,\n",
       "         \" of\": -0.8701171875,\n",
       "         \" the\": -1.583984375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' or',\n",
       "   'token_index': 4}),\n",
       " (' thoughts',\n",
       "  {'finish_reason': None,\n",
       "   'index': 3,\n",
       "   'logprobs': <OpenAIObject at 0x11f691518> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -8.1796875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" thoughts\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" of\": -2.251953125,\n",
       "         \" people\": -2.943359375,\n",
       "         \" times\": -1.8173828125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' thoughts',\n",
       "   'token_index': 0}),\n",
       " (' go',\n",
       "  {'finish_reason': None,\n",
       "   'index': 3,\n",
       "   'logprobs': <OpenAIObject at 0x11f6916d0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       9\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -2.677734375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" go\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" are\": -2.419921875,\n",
       "         \" do\": -2.04296875,\n",
       "         \" have\": -2.630859375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' go',\n",
       "   'token_index': 1}),\n",
       " (' through',\n",
       "  {'finish_reason': None,\n",
       "   'index': 3,\n",
       "   'logprobs': <OpenAIObject at 0x11f691888> JSON: {\n",
       "     \"text_offset\": [\n",
       "       12\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -0.252197265625\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" through\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" in\": -4.25390625,\n",
       "         \" into\": -2.1796875,\n",
       "         \" through\": -0.252197265625\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' through',\n",
       "   'token_index': 2}),\n",
       " (' your',\n",
       "  {'finish_reason': None,\n",
       "   'index': 3,\n",
       "   'logprobs': <OpenAIObject at 0x11f691a40> JSON: {\n",
       "     \"text_offset\": [\n",
       "       20\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -0.84033203125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" your\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.7099609375,\n",
       "         \" the\": -2.32421875,\n",
       "         \" your\": -0.84033203125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' your',\n",
       "   'token_index': 3}),\n",
       " (' head',\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 3,\n",
       "   'logprobs': <OpenAIObject at 0x11f691bf8> JSON: {\n",
       "     \"text_offset\": [\n",
       "       25\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -0.80712890625\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" head\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" brain\": -4.33203125,\n",
       "         \" head\": -0.80712890625,\n",
       "         \" mind\": -0.65283203125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' head',\n",
       "   'token_index': 4}),\n",
       " (' the',\n",
       "  {'finish_reason': None,\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x11f691db0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.8720703125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" the\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' the',\n",
       "   'token_index': 0}),\n",
       " (' end',\n",
       "  {'finish_reason': None,\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x11f691f68> JSON: {\n",
       "     \"text_offset\": [\n",
       "       4\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -4.8203125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" end\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" day\": -2.28125,\n",
       "         \" first\": -1.69921875,\n",
       "         \" last\": -2.41015625\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' end',\n",
       "   'token_index': 1}),\n",
       " (' of',\n",
       "  {'finish_reason': None,\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x11f6b2150> JSON: {\n",
       "     \"text_offset\": [\n",
       "       8\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -0.049285888671875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" of\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" for\": -5.015625,\n",
       "         \" of\": -0.049285888671875,\n",
       "         \"-\": -4.55859375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' of',\n",
       "   'token_index': 2}),\n",
       " (' fashion',\n",
       "  {'finish_reason': None,\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x11f6b2308> JSON: {\n",
       "     \"text_offset\": [\n",
       "       11\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -10.765625\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" fashion\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.9921875,\n",
       "         \" my\": -2.173828125,\n",
       "         \" the\": -0.9775390625\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' fashion',\n",
       "   'token_index': 3}),\n",
       " (' week',\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x11f6b24c0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       19\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -0.394287109375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" week\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" week\": -0.394287109375,\n",
       "         \",\": -3.2265625,\n",
       "         \".\": -3.078125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' week',\n",
       "   'token_index': 4})]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goose_res_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:17:33.722322Z",
     "start_time": "2022-04-10T20:17:33.692008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n"
     ]
    }
   ],
   "source": [
    "open_res_static = load(C.mock_stream_paths[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:17:58.832094Z",
     "start_time": "2022-04-10T20:17:58.800516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<MOCK> 4/11/21\\n\\nInput: 01/20/2017\\nOutput: January 20, 2017\\n\\nInput: 2017\\nOutput: 2017\\n\\nInput: 07/01/2017\\nOutput: 07/01/2017\\n\\nInput</MOCK>'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_res_static.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:23:45.444125Z",
     "start_time": "2022-04-10T20:23:45.415235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yesterday was', 'How many']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T20:27:13.064547Z",
     "start_time": "2022-04-10T20:27:13.026408Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess_gpt_response(response, stream=False):\n",
    "    # Extract text and return. Zip maintains lazy evaluation.\n",
    "    if stream:\n",
    "        # Each item in zipped object is (str, dict-like).\n",
    "        texts = (chunk['choices'][0]['text'] for chunk in response)\n",
    "        chunks = (dict(chunk['choices'][0]) for chunk in response)\n",
    "        # Yields (str, dict) tuples.\n",
    "        return zip(texts, chunks)\n",
    "\n",
    "    # Structure: (List[str], List[dict])\n",
    "    return [row.text for row in response.choices], \\\n",
    "           [dict(choice) for choice in response.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:45:46.917666Z",
     "start_time": "2022-04-10T21:45:46.888506Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def postprocess_response(response, n, trunc_full=True, strip_output=True, \n",
    "                         **kwargs):\n",
    "    text, full_response = containerize(*response)\n",
    "    # Manually check for stop phrases because most backends either don't\n",
    "    # or truncate AFTER the stop phrase which is rarely what we want.\n",
    "    stop = kwargs.get('stop', [])\n",
    "    clean_text = []\n",
    "    clean_full = []\n",
    "    for i, (text_, resp_) in enumerate(zip(text, full_response)):\n",
    "        text_ = truncate_at_first_stop(\n",
    "            text_,\n",
    "            stop_phrases=stop,\n",
    "            finish_reason=resp_.get('finish_reason', ''),\n",
    "            trunc_full=trunc_full,\n",
    "            trunc_partial=True\n",
    "        )\n",
    "        clean_text.append(strip(text_, strip_output))\n",
    "        clean_full.append({**resp_, 'prompt_index': i // n})\n",
    "\n",
    "    return clean_text, clean_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:45:14.658599Z",
     "start_time": "2022-04-10T21:45:14.623945Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "texts, fulls = postprocess_gpt_response(goose_res_multi_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:45:14.954260Z",
     "start_time": "2022-04-10T21:45:14.903076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' my first day working at',\n",
       " ' Kickstarter and I’',\n",
       " ' daily visitors do you have',\n",
       " ' times a week do you']"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:45:15.169745Z",
     "start_time": "2022-04-10T21:45:15.140308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[full['index'] for full in fulls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T21:45:53.276349Z",
     "start_time": "2022-04-10T21:45:53.233991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['my first day working at',\n",
       "  'Kickstarter and I’',\n",
       "  'daily visitors do you have',\n",
       "  'times a week do you'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x11f762360> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       3,\n",
       "       9,\n",
       "       13,\n",
       "       21\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -2.384765625,\n",
       "       -1.3818359375,\n",
       "       -1.455078125,\n",
       "       -3.572265625,\n",
       "       -1.19921875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" my\",\n",
       "       \" first\",\n",
       "       \" day\",\n",
       "       \" working\",\n",
       "       \" at\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" birthday\": -2.107421875,\n",
       "         \" first\": -1.3818359375,\n",
       "         \" last\": -2.130859375\n",
       "       },\n",
       "       {\n",
       "         \" day\": -1.455078125,\n",
       "         \" ever\": -3.21484375,\n",
       "         \" time\": -2.197265625\n",
       "       },\n",
       "       {\n",
       "         \" at\": -1.6396484375,\n",
       "         \" back\": -1.5322265625,\n",
       "         \" of\": -1.4755859375\n",
       "       },\n",
       "       {\n",
       "         \" as\": -2.125,\n",
       "         \" at\": -1.19921875,\n",
       "         \" for\": -2.271484375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' my first day working at',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x11f762678> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       12,\n",
       "       16,\n",
       "       18,\n",
       "       18\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -11.5859375,\n",
       "       -2.87109375,\n",
       "       -1.8896484375,\n",
       "       -1.9775390625,\n",
       "       -0.0015888214111328125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" Kickstarter\",\n",
       "       \" and\",\n",
       "       \" I\",\n",
       "       \"\\ufffd\",\n",
       "       \"\\ufffd\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \",\": -1.7646484375,\n",
       "         \".\": -2.4140625,\n",
       "         \"\\ufffd\": -1.7041015625\n",
       "       },\n",
       "       {\n",
       "         \" I\": -1.8896484375,\n",
       "         \" the\": -2.236328125,\n",
       "         \" today\": -2.1953125\n",
       "       },\n",
       "       {\n",
       "         \" am\": -2.771484375,\n",
       "         \" was\": -2.201171875,\n",
       "         \"\\ufffd\": -1.9775390625\n",
       "       },\n",
       "       {\n",
       "         \"\\ufffd\": -9.640625\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' Kickstarter and I’',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 2,\n",
       "   'logprobs': <OpenAIObject at 0x11f7628e0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       6,\n",
       "       15,\n",
       "       18,\n",
       "       22\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -9.4921875,\n",
       "       -3.365234375,\n",
       "       -1.3984375,\n",
       "       -0.12066650390625,\n",
       "       -0.326171875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" daily\",\n",
       "       \" visitors\",\n",
       "       \" do\",\n",
       "       \" you\",\n",
       "       \" have\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" of\": -2.251953125,\n",
       "         \" people\": -2.943359375,\n",
       "         \" times\": -1.8173828125\n",
       "       },\n",
       "       {\n",
       "         \" doses\": -2.841796875,\n",
       "         \" users\": -2.435546875,\n",
       "         \" visitors\": -3.365234375\n",
       "       },\n",
       "       {\n",
       "         \" are\": -2.3984375,\n",
       "         \" do\": -1.3984375,\n",
       "         \" does\": -1.482421875\n",
       "       },\n",
       "       {\n",
       "         \" I\": -3.837890625,\n",
       "         \" we\": -3.07421875,\n",
       "         \" you\": -0.12066650390625\n",
       "       },\n",
       "       {\n",
       "         \" get\": -2.087890625,\n",
       "         \" have\": -0.326171875,\n",
       "         \" need\": -3.798828125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' daily visitors do you have',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 1},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 3,\n",
       "   'logprobs': <OpenAIObject at 0x11f762b48> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       6,\n",
       "       8,\n",
       "       13,\n",
       "       16\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.8173828125,\n",
       "       -3.623046875,\n",
       "       -1.9912109375,\n",
       "       -0.52490234375,\n",
       "       -0.12548828125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" times\",\n",
       "       \" a\",\n",
       "       \" week\",\n",
       "       \" do\",\n",
       "       \" you\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" of\": -2.251953125,\n",
       "         \" people\": -2.943359375,\n",
       "         \" times\": -1.8173828125\n",
       "       },\n",
       "       {\n",
       "         \" did\": -2.203125,\n",
       "         \" has\": -2.447265625,\n",
       "         \" have\": -0.912109375\n",
       "       },\n",
       "       {\n",
       "         \" day\": -0.849609375,\n",
       "         \" week\": -1.9912109375,\n",
       "         \" year\": -1.6357421875\n",
       "       },\n",
       "       {\n",
       "         \" are\": -2.828125,\n",
       "         \" do\": -0.52490234375,\n",
       "         \" does\": -2.33203125\n",
       "       },\n",
       "       {\n",
       "         \" people\": -4.1328125,\n",
       "         \" we\": -4.2265625,\n",
       "         \" you\": -0.12548828125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' times a week do you',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 1}])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess_response((texts, fulls), n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
