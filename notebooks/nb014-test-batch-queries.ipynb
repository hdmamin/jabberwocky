{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Test out new batch query functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:36.673802Z",
     "start_time": "2022-04-15T23:31:36.655347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:54.382690Z",
     "start_time": "2022-04-15T23:31:54.321619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/gooseai_sample_responses.pkl.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.openai_utils import load_prompt, load_openai_api_key, \\\n",
    "    GPTBackend, query_kwargs_grid, MOCKS, postprocess_gpt_response, \\\n",
    "    containerize, truncate_at_first_stop, query_gpt_mock\n",
    "from jabberwocky.utils import strip\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:55.074861Z",
     "start_time": "2022-04-15T23:31:55.041276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:55.606365Z",
     "start_time": "2022-04-15T23:31:55.564340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTBackend()\n",
    "gpt.switch('repeat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:56.233296Z",
     "start_time": "2022-04-15T23:31:56.191727Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess_response(response, n, trunc_full=True, strip_output=True, \n",
    "                         **kwargs):\n",
    "    text, full_response = containerize(*response)\n",
    "    print('TEXT:', text)\n",
    "    print('FULL:', full_response)\n",
    "\n",
    "    # Manually check for stop phrases because most backends either don't\n",
    "    # or truncate AFTER the stop phrase which is rarely what we want.\n",
    "    stop = kwargs.get('stop', [])\n",
    "    clean_text = []\n",
    "    clean_full = []\n",
    "    for i, (text_, resp_) in enumerate(zip(text, full_response)):\n",
    "        text_ = truncate_at_first_stop(\n",
    "            text_,\n",
    "            stop_phrases=stop,\n",
    "            finish_reason=resp_.get('finish_reason', ''),\n",
    "            trunc_full=trunc_full,\n",
    "            trunc_partial=True\n",
    "        )\n",
    "        clean_text.append(strip(text_, strip_output))\n",
    "        clean_full.append({**resp_, 'prompt_index': i // n})\n",
    "\n",
    "    return clean_text, clean_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:38.403079Z",
     "start_time": "2022-04-15T23:53:38.029473Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_backend(backend, results=None):\n",
    "    def gen():\n",
    "        yield from results.kwargs\n",
    "            \n",
    "    full_keys = ('prompt_index', 'index', 'finish_reason')\n",
    "    with gpt(backend):\n",
    "        kwargs_list = []\n",
    "        resp_list = []\n",
    "        gen_kwargs = gen if results else query_kwargs_grid\n",
    "        for i, kwargs in enumerate(gen_kwargs(), start=1):\n",
    "            print(f'\\n\\n{i}.')\n",
    "            if isinstance(kwargs['prompt'], str):\n",
    "                np = 1\n",
    "            else:\n",
    "                np = len(kwargs['prompt'])\n",
    "            expected_prompt_idx = list(range(np))\n",
    "                \n",
    "            # Get completion, either from new query or cached result.\n",
    "            if results:\n",
    "                res = results.res[i - 1]\n",
    "            else:\n",
    "                res = gpt.query(**kwargs)\n",
    "                \n",
    "            # Print results.\n",
    "            if kwargs['stream']:\n",
    "                cur = []\n",
    "                actual_prompt_idx = []\n",
    "                for tok, tok_full in res:\n",
    "                    cur.append((tok, tok_full))\n",
    "                    print(tok)\n",
    "                    print('\\t' + str(select(tok_full, keep=full_keys)) + '\\n')\n",
    "                    if tok_full['finish_reason']: print('\\n---\\n')\n",
    "                    actual_prompt_idx.append(tok_full.get('prompt_index', -1))\n",
    "            else:\n",
    "                texts, fulls = res\n",
    "                print('TEXTS:', texts)\n",
    "                print('FULLS:',\n",
    "                      [{key: full.get(key) for key in full_keys}\n",
    "                       for full in fulls])\n",
    "                actual_prompt_idx = [full['prompt_index'] for full in fulls]\n",
    "                cur = res\n",
    "            \n",
    "            actual_prompt_idx = sorted(set(actual_prompt_idx))\n",
    "            assert actual_prompt_idx == expected_prompt_idx, \\\n",
    "                    f'Expected prompt indices {expected_prompt_idx}, ' \\\n",
    "                    f'got {actual_prompt_idx}'\n",
    "            print(spacer())\n",
    "\n",
    "            kwargs_list.append(kwargs)\n",
    "            resp_list.append(cur)\n",
    "    return Results(kwargs=kwargs_list, res=resp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues\n",
    "\n",
    "[X] - no prompt_index in stream=True mode for either repeat or banana (pretty sure not for paid backends either)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:38.926617Z",
     "start_time": "2022-04-15T23:53:38.818143Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "1.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "2.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "3.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS', 'YESTERDAY WAS']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "4.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "5.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS', 'HOW MANY']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "6.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOW \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "MANY \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "7.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS', 'YESTERDAY WAS', 'HOW MANY', 'HOW MANY']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "8.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOW \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "MANY \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOW \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "MANY \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:415: UserWarning: Unused kwargs {'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:415: UserWarning: Unused kwargs {'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:415: UserWarning: Unused kwargs {'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:415: UserWarning: Unused kwargs {'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n"
     ]
    }
   ],
   "source": [
    "repeat_res = test_backend('repeat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:43.918149Z",
     "start_time": "2022-04-15T23:53:39.309757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "1.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:470: UserWarning: query_gpt_banana received unused kwargs {'n': 1, 'stream': False, 'engine_i': 0, 'logprobs': 3}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTS: ['the one year']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "2.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:470: UserWarning: query_gpt_banana received unused kwargs {'n': 1, 'stream': True, 'engine_i': 0, 'logprobs': 3}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "3.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:470: UserWarning: query_gpt_banana received unused kwargs {'stream': False, 'engine_i': 0, 'logprobs': 3}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTS: ['a big day', 'my last day']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "4.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:470: UserWarning: query_gpt_banana received unused kwargs {'stream': True, 'engine_i': 0, 'logprobs': 3}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "birthday. \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "5.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "TEXTS: ['the first day', 'minutes are there']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "6.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "last \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "7.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "TEXTS: ['my last day', 'a great day', 'times have you', 'minutes are there']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "8.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "a \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "big \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "people \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "are \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "there \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n",
      "Writing data to data/tmp/banana_res.pkl.\n",
      "Object loaded from data/tmp/banana_res.pkl.\n"
     ]
    }
   ],
   "source": [
    "# Even though it's free, try to avoid hitting the API too much just in case \n",
    "# they have some rate limit.\n",
    "banana_res = test_backend('banana')\n",
    "save(banana_res, 'data/tmp/banana_res.pkl')\n",
    "banana_res = load('data/tmp/banana_res.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:50.144814Z",
     "start_time": "2022-04-15T23:53:50.091853Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "\n",
      "\n",
      "1.\n",
      "TEXTS: ['the one year']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "2.\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "3.\n",
      "TEXTS: ['a big day', 'my last day']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "4.\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "birthday. \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "5.\n",
      "TEXTS: ['the first day', 'minutes are there']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "6.\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "last \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "7.\n",
      "TEXTS: ['my last day', 'a great day', 'times have you', 'minutes are there']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "8.\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "a \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "big \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "people \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "are \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "there \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "banana_res = test_backend('banana', results=banana_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:50.902352Z",
     "start_time": "2022-04-15T23:53:50.835222Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"mock\".\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "1.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: ['a typical rainy']\n",
      "FULLS: [{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "2.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " a\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " bit\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " busy\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "3.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: ['my mother�', 'the Academy Awards']\n",
      "FULLS: [{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}, {'prompt_index': 0, 'index': 1, 'finish_reason': 'length'}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "4.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " a\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " positive\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " day\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " clearer\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      " and\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      " colder\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "5.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: ['24 years ago', 'months did it']\n",
      "FULLS: [{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}, {'prompt_index': 1, 'index': 1, 'finish_reason': 'length'}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "6.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " a\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " day\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " when\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " times\n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      " I\n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "'ve\n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "7.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: [\"Mozilla's ann\", 'reminded of this', 'lessons would it', 'teenagers are allowed']\n",
      "FULLS: [{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}, {'prompt_index': 0, 'index': 1, 'finish_reason': 'length'}, {'prompt_index': 1, 'index': 2, 'finish_reason': 'length'}, {'prompt_index': 1, 'index': 3, 'finish_reason': 'length'}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "8.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " ways\n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      " are\n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      " there\n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " a\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " grey\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " and\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " will\n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      " die\n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "?\n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " beautiful\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      ".\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      " It\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "mock_res = test_backend('mock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:01:50.914410Z",
     "start_time": "2022-04-15T23:01:50.871188Z"
    }
   },
   "outputs": [],
   "source": [
    "# hf_res = test_backend('huggingface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:25:47.312235Z",
     "start_time": "2022-04-13T03:25:47.269492Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 prompts, 1 completion per prompt, stream=False\n",
    "p2c1s0 = MOCKS[True, False, False]\n",
    "p1c2s0 = MOCKS[False, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:25:53.635408Z",
     "start_time": "2022-04-13T03:25:53.587866Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['24 years ago', 'months did it'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x125bacaf0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       3,\n",
       "       9\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -8.1484375,\n",
       "       -1.931640625,\n",
       "       -0.8271484375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" 24\",\n",
       "       \" years\",\n",
       "       \" ago\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" hours\": -1.21484375,\n",
       "         \" years\": -1.931640625,\n",
       "         \"th\": -1.8212890625\n",
       "       },\n",
       "       {\n",
       "         \" ago\": -0.8271484375,\n",
       "         \" since\": -1.0576171875,\n",
       "         \" to\": -2.794921875\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' 24 years ago',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x125bacca8> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       7,\n",
       "       11\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -5.46484375,\n",
       "       -5,\n",
       "       -1.4169921875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" months\",\n",
       "       \" did\",\n",
       "       \" it\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" of\": -2.251953125,\n",
       "         \" people\": -2.943359375,\n",
       "         \" times\": -1.8173828125\n",
       "       },\n",
       "       {\n",
       "         \" are\": -0.167724609375,\n",
       "         \" is\": -3.70703125,\n",
       "         \" will\": -3.78515625\n",
       "       },\n",
       "       {\n",
       "         \" it\": -1.4169921875,\n",
       "         \" the\": -3.44140625,\n",
       "         \" you\": -0.48876953125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' months did it',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 1}])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, fulls = containerize(*postprocess_gpt_response(p2c1s0))\n",
    "postprocess_response((texts, fulls), n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:41:52.515976Z",
     "start_time": "2022-04-13T03:41:52.450132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: [' my mother�', ' the Academy Awards']\n",
      "FULL: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x1245f1990> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    3,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.384765625,\n",
      "    -5.4375,\n",
      "    -0.44677734375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" my\",\n",
      "    \" mother\",\n",
      "    \"\\ufffd\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" birthday\": -2.107421875,\n",
      "      \" first\": -1.3818359375,\n",
      "      \" last\": -2.130859375\n",
      "    },\n",
      "    {\n",
      "      \"'s\": -1.509765625,\n",
      "      \"-\": -2.630859375,\n",
      "      \"\\ufffd\": -0.44677734375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' my mother�', 'token_index': 0}, {'finish_reason': 'length', 'index': 1, 'logprobs': <OpenAIObject at 0x1245f1b48> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    4,\n",
      "    12\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.8720703125,\n",
      "    -8.796875,\n",
      "    -0.1270751953125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" the\",\n",
      "    \" Academy\",\n",
      "    \" Awards\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" day\": -2.28125,\n",
      "      \" first\": -1.69921875,\n",
      "      \" last\": -2.41015625\n",
      "    },\n",
      "    {\n",
      "      \" Award\": -2.845703125,\n",
      "      \" Awards\": -0.1270751953125,\n",
      "      \"\\ufffd\": -4.12109375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' the Academy Awards', 'token_index': 0}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['my mother�', 'the Academy Awards'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x1245f1990> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       3,\n",
       "       10\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -2.384765625,\n",
       "       -5.4375,\n",
       "       -0.44677734375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" my\",\n",
       "       \" mother\",\n",
       "       \"\\ufffd\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" birthday\": -2.107421875,\n",
       "         \" first\": -1.3818359375,\n",
       "         \" last\": -2.130859375\n",
       "       },\n",
       "       {\n",
       "         \"'s\": -1.509765625,\n",
       "         \"-\": -2.630859375,\n",
       "         \"\\ufffd\": -0.44677734375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' my mother�',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x1245f1b48> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       4,\n",
       "       12\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.8720703125,\n",
       "       -8.796875,\n",
       "       -0.1270751953125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" the\",\n",
       "       \" Academy\",\n",
       "       \" Awards\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" day\": -2.28125,\n",
       "         \" first\": -1.69921875,\n",
       "         \" last\": -2.41015625\n",
       "       },\n",
       "       {\n",
       "         \" Award\": -2.845703125,\n",
       "         \" Awards\": -0.1270751953125,\n",
       "         \"\\ufffd\": -4.12109375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' the Academy Awards',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0}])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, fulls = postprocess_gpt_response(p1c2s0)\n",
    "postprocess_response((texts, fulls), n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:42:57.404628Z",
     "start_time": "2022-04-13T03:42:57.368447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' my mother�', ' the Academy Awards']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:58:46.231595Z",
     "start_time": "2022-04-13T03:58:46.199365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"mock\".\n",
      "{'n': 1, 'stream': False, 'prompt': 'ac', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      ">>> response: ([' a typical rainy'], [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x125a06e08> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    2,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375,\n",
      "    -6.06640625,\n",
      "    -5.671875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\",\n",
      "    \" typical\",\n",
      "    \" rainy\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    },\n",
      "    {\n",
      "      \" Monday\": -2.974609375,\n",
      "      \" day\": -1.095703125,\n",
      "      \" work\": -2.94140625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a typical rainy', 'token_index': 0}])\n",
      "TEXT: [' a typical rainy']\n",
      "FULL: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x125a06e08> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    2,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375,\n",
      "    -6.06640625,\n",
      "    -5.671875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\",\n",
      "    \" typical\",\n",
      "    \" rainy\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    },\n",
      "    {\n",
      "      \" Monday\": -2.974609375,\n",
      "      \" day\": -1.095703125,\n",
      "      \" work\": -2.94140625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a typical rainy', 'token_index': 0}]\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('mock'):\n",
    "    res = gpt.query('ac', n=1, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:58:50.103218Z",
     "start_time": "2022-04-13T03:58:50.052234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a typical rainy'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x125a06e08> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       2,\n",
       "       10\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.3818359375,\n",
       "       -6.06640625,\n",
       "       -5.671875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" a\",\n",
       "       \" typical\",\n",
       "       \" rainy\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" big\": -1.83984375,\n",
       "         \" day\": -2.53125,\n",
       "         \" very\": -2.916015625\n",
       "       },\n",
       "       {\n",
       "         \" Monday\": -2.974609375,\n",
       "         \" day\": -1.095703125,\n",
       "         \" work\": -2.94140625\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' a typical rainy',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0}])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PromptManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:43:17.178965Z",
     "start_time": "2022-04-19T02:43:17.090447Z"
    }
   },
   "outputs": [],
   "source": [
    "from jabberwocky.openai_utils import PromptManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:43:18.859342Z",
     "start_time": "2022-04-19T02:43:18.705037Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplify_ml: This uses the expensive davinci model and doesn't work so well without it. Temperature is set to 0.3 but this hasn't been extensively tuned.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "shortest: This prompt takes no input.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "extract_backend_slot: WARNING: This seemed to work okay when testing in the gooseai console but further testing suggests it needs more work.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "ml_abstract: I haven't tuned hyperparameters much - even engine_i=1 sometimes provides good resuls but it seems a bit inconsistent.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "translate: User input should consist of 1 line like 'Translate to German:' followed by a second line with a sentence in English. You may want to use a variable number of tokens defined as a function of the input length. Weaker engines sometimes work but sometimes translate into the wrong language.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "default: This uses the weakest engine by default.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "how_to: Should be a single line starting with the words \"How to\" and ending in a colon. You may need a stronger engine for good results.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "debate: Non-zero frequency penalty was initially included by accident, but in at least 1 test removing it noticeably worsened results.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "analyze_writing: Lower engines could get quite repetitive.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "mma: Haven't really experimented with hyperparameters.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "punctuate_transcription: You should probably adjust max_tokens based on the length of the input or just set max length high and let stop phrases do the rest. Bumping up to engine 2 or 3 might help a little, but engine 1 is serviceable (probably best to avoid 0 though).\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "punctuate_alexa: You should probably adjust max_tokens based on the length of the input or just set max length high and let stop phrases do the rest. Bumping up to engine 2 or 3 might help a little, but engine 1 is serviceable (probably best to avoid 0 though). We use a separate prompt for alexa because its transcriptions seem to handle casing a bit differently than the python speech_recognition package I use in the GUI.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "punctuate: You should probably adjust max_tokens based on the length of the input. Bumping up to engine 2 or 3 might help a little, but engine 1 is serviceable (probably best to avoid 0 though). You should probably try training a huggingface model to add punctuation instead of using gpt3 credits though.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "word2number: This was not designed to work with negatives or terms like 'half', 'quarter', etc.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "tldr: This sets max tokens to 64. You may wish to adjust that value. The default engine is Curie (2) but you could try a different engine.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "eli: This uses the expensive davinci model and doesn't work so well without it.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "conversation: Might want to try tweaking frequency penalty.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "short_dates: This prompt takes no input.\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manager = PromptManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:43:54.492303Z",
     "start_time": "2022-04-19T02:43:54.458094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n"
     ]
    }
   ],
   "source": [
    "gpt.switch('banana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:45:37.356161Z",
     "start_time": "2022-04-19T02:45:30.889481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'engine_i': 1, 'temperature': 0.3, 'stop': ['How to ', '10. '], 'max_tokens': 100, 'prompt': \"How to change a lightbulb:\\n1. Turn off the light switch. \\n2. Using a cloth, push the bulb in, furn anti clockwise and remove it.\\n3. Insert the new bulb and twist to find the right fit. Push it in and turn clockwise.\\n4. Turn on the light to test it.\\n\\nHow to tie a tie:\\n1. Place tie around your neck.\\n2. Cross the wide end over the thinner end.\\n3. Run wide end under tie and pull it across again.\\n4. Pull the wide end through the center.\\n5. Loop through the knot.\\n6. Tighten the knot.\\n\\nHow to fry an egg:\\n1. Crack the Eggs. \\n2. Add the Eggs to the Pan. \\n3. Cover When the Edges Turn White.\\n4. Wait until it's done.\\n5. Serve.\\n\\nHow to make a room smell good:\", 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana', 'datetime': 'Mon Apr 18 19:45:30 2022'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:468: UserWarning: query_gpt_banana received unused kwargs {'engine_i': 1, 'stop': ['How to ', '10. ']}.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "res = manager.query('how_to', 'How to make a room smell good:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:45:49.688070Z",
     "start_time": "2022-04-19T02:45:49.614553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Open all the windows. \n",
      "2. Turn on the fan. \n",
      "3. Put a pot of water on the stove. \n",
      "4. Add a few drops of essential oil. \n",
      "5. Wait for the water to boil. \n",
      "6. Turn off the stove. \n",
      "7. Wait for the water to cool. \n",
      "8. Pour the water into a bowl. \n",
      "9. Add the essential oil.\n"
     ]
    }
   ],
   "source": [
    "print(res[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:49:27.154795Z",
     "start_time": "2022-04-19T02:49:20.756785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'engine_i': 0, 'temperature': 0.7, 'max_tokens': 100, 'stream': True, 'prompt': 'I met my wife at', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana', 'datetime': 'Mon Apr 18 19:49:20 2022'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:720: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  strip_output = False\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:725: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'support for truncation.'\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:468: UserWarning: query_gpt_banana received unused kwargs {'engine_i': 0, 'stream': True}.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "the \n",
      "age \n",
      "of \n",
      "30. \n",
      "She \n",
      "was \n",
      "a \n",
      "nurse \n",
      "at \n",
      "the \n",
      "time. \n",
      "I \n",
      "was \n",
      "a \n",
      "salesman, \n",
      "a \n",
      "college \n",
      "graduate, \n",
      "and \n",
      "I \n",
      "was \n",
      "on \n",
      "the \n",
      "road \n",
      "selling \n",
      "life \n",
      "insurance. \n",
      "She \n",
      "was \n",
      "the \n",
      "one \n",
      "who \n",
      "made \n",
      "me \n",
      "want \n",
      "to \n",
      "go \n",
      "back \n",
      "to \n",
      "school. \n",
      "She \n",
      "was \n",
      "the \n",
      "one \n",
      "who \n",
      "told \n",
      "me \n",
      "I \n",
      "needed \n",
      "to \n",
      "get \n",
      "my \n",
      "degree. \n",
      "She \n",
      "was \n",
      "the \n",
      "one \n",
      "who \n",
      "wanted \n",
      "to \n",
      "marry \n",
      "me.\n",
      "\n",
      "I \n",
      "remember \n",
      "the \n",
      "day \n",
      "she \n",
      "came \n",
      "to \n",
      "my \n",
      "office. \n",
      "She \n",
      "was \n",
      "dressed \n",
      "in \n",
      "a \n",
      "nurse's \n",
      "uniform. \n",
      "She \n",
      "was \n",
      "so \n",
      "pretty. \n",
      "She \n",
      "was \n",
      "so \n",
      "smart \n"
     ]
    }
   ],
   "source": [
    "fulls = []\n",
    "for tok, full in manager.query('default', 'I met my wife at', stream=True):\n",
    "    print(tok)\n",
    "    fulls.append(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:49:39.756542Z",
     "start_time": "2022-04-19T02:49:39.724230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:50:54.020196Z",
     "start_time": "2022-04-19T02:50:53.975288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" the age of 30. She was a nurse at the time. I was a salesman, a college graduate, and I was on the road selling life insurance. She was the one who made me want to go back to school. She was the one who told me I needed to get my degree. She was the one who wanted to marry me.\\n\\nI remember the day she came to my office. She was dressed in a nurse's uniform. She was so pretty. She was so smart\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulls[0]['modelOutputs'][0]['output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
