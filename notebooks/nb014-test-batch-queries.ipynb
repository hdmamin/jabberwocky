{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Test out new batch query functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:40:10.406233Z",
     "start_time": "2022-04-15T02:40:10.386031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:40:17.942093Z",
     "start_time": "2022-04-15T02:40:15.689443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/gooseai_sample_responses.pkl.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.openai_utils import load_prompt, load_openai_api_key, \\\n",
    "    GPTBackend, query_kwargs_grid, MOCKS, postprocess_gpt_response, \\\n",
    "    containerize, truncate_at_first_stop, query_gpt_mock\n",
    "from jabberwocky.utils import strip\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:40:37.772901Z",
     "start_time": "2022-04-15T02:40:37.718788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:40:38.969759Z",
     "start_time": "2022-04-15T02:40:38.897968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTBackend()\n",
    "gpt.switch('repeat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:40:46.120463Z",
     "start_time": "2022-04-15T02:40:46.090720Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess_response(response, n, trunc_full=True, strip_output=True, \n",
    "                         **kwargs):\n",
    "    text, full_response = containerize(*response)\n",
    "    print('TEXT:', text)\n",
    "    print('FULL:', full_response)\n",
    "\n",
    "    # Manually check for stop phrases because most backends either don't\n",
    "    # or truncate AFTER the stop phrase which is rarely what we want.\n",
    "    stop = kwargs.get('stop', [])\n",
    "    clean_text = []\n",
    "    clean_full = []\n",
    "    for i, (text_, resp_) in enumerate(zip(text, full_response)):\n",
    "        text_ = truncate_at_first_stop(\n",
    "            text_,\n",
    "            stop_phrases=stop,\n",
    "            finish_reason=resp_.get('finish_reason', ''),\n",
    "            trunc_full=trunc_full,\n",
    "            trunc_partial=True\n",
    "        )\n",
    "        clean_text.append(strip(text_, strip_output))\n",
    "        clean_full.append({**resp_, 'prompt_index': i // n})\n",
    "\n",
    "    return clean_text, clean_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:40:46.544390Z",
     "start_time": "2022-04-15T02:40:46.492506Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_backend(backend, results=None):\n",
    "    def gen():\n",
    "        yield from results.kwargs\n",
    "        \n",
    "    with gpt(backend):\n",
    "        kwargs_list = []\n",
    "        resp_list = []\n",
    "        gen_kwargs = gen if results else query_kwargs_grid\n",
    "        for i, kwargs in enumerate(gen_kwargs(), start=1):\n",
    "            print(f'\\n\\n{i}.')\n",
    "            if isinstance(kwargs['prompt'], str):\n",
    "                np = 1\n",
    "            else:\n",
    "                np = len(kwargs['prompt'])\n",
    "            expected_prompt_idx = list(range(np))\n",
    "                \n",
    "            # Get completion, either from new query or cached result.\n",
    "            if results:\n",
    "                res = results.res[i - 1]\n",
    "            else:\n",
    "                res = gpt.query(**kwargs)\n",
    "                \n",
    "            # Print results.\n",
    "            if kwargs['stream']:\n",
    "                cur = []\n",
    "                actual_prompt_idx = []\n",
    "                for tok, tok_full in res:\n",
    "                    cur.append((tok, tok_full))\n",
    "                    print(tok)\n",
    "                    print('\\t' + str(tok_full) + '\\n')\n",
    "                    if tok_full['finish_reason']: print('\\n---\\n')\n",
    "                    actual_prompt_idx.append(tok_full.get('prompt_index', -1))\n",
    "            else:\n",
    "                texts, fulls = res\n",
    "                print('TEXTS:', texts)\n",
    "                print('FULLS:', fulls)\n",
    "                actual_prompt_idx = [full['prompt_index'] for full in fulls]\n",
    "                cur = res\n",
    "            \n",
    "            actual_prompt_idx = sorted(set(actual_prompt_idx))\n",
    "            assert actual_prompt_idx == expected_prompt_idx, \\\n",
    "                    f'Expected prompt indices {expected_prompt_idx}, ' \\\n",
    "                    f'got {actual_prompt_idx}'\n",
    "            print(spacer())\n",
    "\n",
    "            kwargs_list.append(kwargs)\n",
    "            resp_list.append(cur)\n",
    "    return Results(kwargs=kwargs_list, res=resp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues\n",
    "\n",
    "- no prompt_index in stream=True mode for either repeat or banana (pretty sure not for paid backends either)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:40:49.544842Z",
     "start_time": "2022-04-15T02:40:49.419872Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "1.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS']\n",
      "FULLS: [{'prompt_index': 0}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "2.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'index': 0, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "3.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS', 'YESTERDAY WAS']\n",
      "FULLS: [{'prompt_index': 0}, {'prompt_index': 0}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "4.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'index': 0, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "YESTERDAY \n",
      "\t{'index': 1, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'index': 1, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "5.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "fulls: [[{'prompt_index': 0}], [{'prompt_index': 0}]]\n",
      "TEXTS: ['YESTERDAY WAS', 'HOW MANY']\n",
      "FULLS: [{'prompt_index': 0}, {'prompt_index': 1}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "6.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'index': 0, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOW \n",
      "\t{'index': 1, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "MANY \n",
      "\t{'index': 1, 'prompt_index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "7.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "fulls: [[{'prompt_index': 0}, {'prompt_index': 0}], [{'prompt_index': 0}, {'prompt_index': 0}]]\n",
      "TEXTS: ['YESTERDAY WAS', 'YESTERDAY WAS', 'HOW MANY', 'HOW MANY']\n",
      "FULLS: [{'prompt_index': 0}, {'prompt_index': 0}, {'prompt_index': 1}, {'prompt_index': 1}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "8.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'index': 0, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "YESTERDAY \n",
      "\t{'index': 1, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'index': 1, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOW \n",
      "\t{'index': 2, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "MANY \n",
      "\t{'index': 2, 'prompt_index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOW \n",
      "\t{'index': 3, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "MANY \n",
      "\t{'index': 3, 'prompt_index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:375: UserWarning: Unused kwargs {'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:674: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn('strip_output=True is not supported in stream '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:679: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'Streaming mode does not support manual truncation of '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:375: UserWarning: Unused kwargs {'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:375: UserWarning: Unused kwargs {'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:375: UserWarning: Unused kwargs {'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n"
     ]
    }
   ],
   "source": [
    "repeat_res = test_backend('repeat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T03:24:01.017719Z",
     "start_time": "2022-04-14T03:24:00.939348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Even though it's free, try to avoid hitting the API too much just in case \n",
    "# they have some rate limit.\n",
    "# banana_res = test_backend('banana')\n",
    "# save(banana_res, 'data/tmp/banana_res.pkl')\n",
    "# banana_res = load('data/tmp/banana_res.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T03:24:04.382175Z",
     "start_time": "2022-04-14T03:24:04.334579Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "\n",
      "\n",
      "1.\n",
      "TEXTS: ['the first time']\n",
      "FULLS: [{'id': '73a2b095-3bd2-438b-9bc9-078aa4003226', 'message': 'success', 'created': 1649906615, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first time', 'input': 'Yesterday was'}], 'prompt_index': 0}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "2.\n",
      " \n",
      "\t{'id': '9478d077-a6e9-4b0a-8a12-3b7f971f8050', 'message': 'success', 'created': 1649906615, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a weird day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "a \n",
      "\t{'id': '9478d077-a6e9-4b0a-8a12-3b7f971f8050', 'message': 'success', 'created': 1649906615, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a weird day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "weird \n",
      "\t{'id': '9478d077-a6e9-4b0a-8a12-3b7f971f8050', 'message': 'success', 'created': 1649906615, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a weird day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'id': '9478d077-a6e9-4b0a-8a12-3b7f971f8050', 'message': 'success', 'created': 1649906615, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a weird day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "3.\n",
      "TEXTS: ['a big day', 'a really good']\n",
      "FULLS: [{'id': '4c0bf339-3309-4726-840a-25919921c77f', 'message': 'success', 'created': 1649906616, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a big day', 'input': 'Yesterday was'}], 'prompt_index': 0}, {'id': '30cf1126-6863-4519-8442-8acc7ef1817f', 'message': 'success', 'created': 1649906616, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a really good', 'input': 'Yesterday was'}], 'prompt_index': 0}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "4.\n",
      " \n",
      "\t{'id': 'b527cdf9-86ac-4c75-a5df-6d1f19769226', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the second day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'id': 'b527cdf9-86ac-4c75-a5df-6d1f19769226', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the second day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "second \n",
      "\t{'id': 'b527cdf9-86ac-4c75-a5df-6d1f19769226', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the second day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'id': 'b527cdf9-86ac-4c75-a5df-6d1f19769226', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the second day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'id': 'b3e2b244-b38c-438c-8ce4-430e0c6988d9', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a day of', 'input': 'Yesterday was'}], 'index': 1, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "a \n",
      "\t{'id': 'b3e2b244-b38c-438c-8ce4-430e0c6988d9', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a day of', 'input': 'Yesterday was'}], 'index': 1, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'id': 'b3e2b244-b38c-438c-8ce4-430e0c6988d9', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a day of', 'input': 'Yesterday was'}], 'index': 1, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "of \n",
      "\t{'id': 'b3e2b244-b38c-438c-8ce4-430e0c6988d9', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a day of', 'input': 'Yesterday was'}], 'index': 1, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "5.\n",
      "TEXTS: ['a great day', 'of us have']\n",
      "FULLS: [{'id': '1b3b4c9d-5f5c-44c5-b7ff-90271d99b2e9', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a great day', 'input': 'Yesterday was'}], 'prompt_index': 0}, {'id': '440a43b8-c505-4855-b0cf-160feb8afaa0', 'message': 'success', 'created': 1649906617, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of us have', 'input': 'How many'}], 'prompt_index': 1}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "6.\n",
      " \n",
      "\t{'id': 'a596074a-7abd-4b18-ad51-a47cc4a05a68', 'message': 'success', 'created': 1649906618, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a good day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "a \n",
      "\t{'id': 'a596074a-7abd-4b18-ad51-a47cc4a05a68', 'message': 'success', 'created': 1649906618, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a good day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "good \n",
      "\t{'id': 'a596074a-7abd-4b18-ad51-a47cc4a05a68', 'message': 'success', 'created': 1649906618, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a good day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'id': 'a596074a-7abd-4b18-ad51-a47cc4a05a68', 'message': 'success', 'created': 1649906618, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' a good day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'id': '21ab9dc5-da53-496d-af25-8f11134fbac9', 'message': 'success', 'created': 1649906618, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' times have you', 'input': 'How many'}], 'index': 1, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'id': '21ab9dc5-da53-496d-af25-8f11134fbac9', 'message': 'success', 'created': 1649906618, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' times have you', 'input': 'How many'}], 'index': 1, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'id': '21ab9dc5-da53-496d-af25-8f11134fbac9', 'message': 'success', 'created': 1649906618, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' times have you', 'input': 'How many'}], 'index': 1, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'id': '21ab9dc5-da53-496d-af25-8f11134fbac9', 'message': 'success', 'created': 1649906618, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' times have you', 'input': 'How many'}], 'index': 1, 'prompt_index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "7.\n",
      "TEXTS: ['my last day', 'the first day', 'times have you', 'of you are']\n",
      "FULLS: [{'id': 'ff712be4-97cb-4707-8ce7-baa92d53f374', 'message': 'success', 'created': 1649906619, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' my last day', 'input': 'Yesterday was'}], 'prompt_index': 0}, {'id': 'f8c7206a-1517-479d-ba36-3e60e2d25cba', 'message': 'success', 'created': 1649906619, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first day', 'input': 'Yesterday was'}], 'prompt_index': 0}, {'id': '047f53bc-7974-4353-96a8-73e89a2970dd', 'message': 'success', 'created': 1649906619, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' times have you', 'input': 'How many'}], 'prompt_index': 1}, {'id': '69fd91b3-6080-47e5-bf66-b19af870ff4a', 'message': 'success', 'created': 1649906619, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' of you are', 'input': 'How many'}], 'prompt_index': 1}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "8.\n",
      " \n",
      "\t{'id': 'b05d7a06-9152-4fad-99ee-432cf9985dd6', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'id': 'b05d7a06-9152-4fad-99ee-432cf9985dd6', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'id': 'b05d7a06-9152-4fad-99ee-432cf9985dd6', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'id': 'b05d7a06-9152-4fad-99ee-432cf9985dd6', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first day', 'input': 'Yesterday was'}], 'index': 0, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'id': '742f83f3-c77c-47f7-b7f8-535d0331fc1b', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first day', 'input': 'Yesterday was'}], 'index': 1, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'id': '742f83f3-c77c-47f7-b7f8-535d0331fc1b', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first day', 'input': 'Yesterday was'}], 'index': 1, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'id': '742f83f3-c77c-47f7-b7f8-535d0331fc1b', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first day', 'input': 'Yesterday was'}], 'index': 1, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'id': '742f83f3-c77c-47f7-b7f8-535d0331fc1b', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' the first day', 'input': 'Yesterday was'}], 'index': 1, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'id': 'bf905918-f350-4ba4-b240-5844398bcd5d', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' minutes are there', 'input': 'How many'}], 'index': 2, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "minutes \n",
      "\t{'id': 'bf905918-f350-4ba4-b240-5844398bcd5d', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' minutes are there', 'input': 'How many'}], 'index': 2, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "are \n",
      "\t{'id': 'bf905918-f350-4ba4-b240-5844398bcd5d', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' minutes are there', 'input': 'How many'}], 'index': 2, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "there \n",
      "\t{'id': 'bf905918-f350-4ba4-b240-5844398bcd5d', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' minutes are there', 'input': 'How many'}], 'index': 2, 'prompt_index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'id': '7a32298f-344c-40db-baf2-c8fc760f81aa', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' times have you', 'input': 'How many'}], 'index': 3, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'id': '7a32298f-344c-40db-baf2-c8fc760f81aa', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' times have you', 'input': 'How many'}], 'index': 3, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'id': '7a32298f-344c-40db-baf2-c8fc760f81aa', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' times have you', 'input': 'How many'}], 'index': 3, 'prompt_index': 1, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'id': '7a32298f-344c-40db-baf2-c8fc760f81aa', 'message': 'success', 'created': 1649906620, 'apiVersion': '26 Nov 2021', 'modelOutputs': [{'output': ' times have you', 'input': 'How many'}], 'index': 3, 'prompt_index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "banana_res = test_backend('banana', results=banana_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T03:46:36.302424Z",
     "start_time": "2022-04-14T03:46:35.925072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/gooseai_sample_responses.pkl.\n",
      "Switching openai backend to \"mock\".\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "1.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: ['a typical rainy']\n",
      "FULLS: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x1236e62b0> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    2,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375,\n",
      "    -6.06640625,\n",
      "    -5.671875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\",\n",
      "    \" typical\",\n",
      "    \" rainy\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    },\n",
      "    {\n",
      "      \" Monday\": -2.974609375,\n",
      "      \" day\": -1.095703125,\n",
      "      \" work\": -2.94140625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a typical rainy', 'token_index': 0, 'prompt_index': 0}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "2.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " a\n",
      "\t{'finish_reason': None, 'index': 0, 'logprobs': <OpenAIObject at 0x123614d58> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a', 'token_index': 0, 'prompt_index': 0}\n",
      "\n",
      " bit\n",
      "\t{'finish_reason': None, 'index': 0, 'logprobs': <OpenAIObject at 0x123614f10> JSON: {\n",
      "  \"text_offset\": [\n",
      "    2\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -4.75390625\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" bit\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' bit', 'token_index': 1, 'prompt_index': 0}\n",
      "\n",
      " busy\n",
      "\t{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x1236e60f8> JSON: {\n",
      "  \"text_offset\": [\n",
      "    6\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -5.09375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" busy\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" like\": -4.03515625,\n",
      "      \" more\": -3.875,\n",
      "      \" of\": -0.352294921875\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' busy', 'token_index': 2, 'prompt_index': 0}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "3.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: ['my mother�', 'the Academy Awards']\n",
      "FULLS: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x123614938> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    3,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.384765625,\n",
      "    -5.4375,\n",
      "    -0.44677734375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" my\",\n",
      "    \" mother\",\n",
      "    \"\\ufffd\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" birthday\": -2.107421875,\n",
      "      \" first\": -1.3818359375,\n",
      "      \" last\": -2.130859375\n",
      "    },\n",
      "    {\n",
      "      \"'s\": -1.509765625,\n",
      "      \"-\": -2.630859375,\n",
      "      \"\\ufffd\": -0.44677734375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' my mother�', 'token_index': 0, 'prompt_index': 0}, {'finish_reason': 'length', 'index': 1, 'logprobs': <OpenAIObject at 0x123614af0> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    4,\n",
      "    12\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.8720703125,\n",
      "    -8.796875,\n",
      "    -0.1270751953125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" the\",\n",
      "    \" Academy\",\n",
      "    \" Awards\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" day\": -2.28125,\n",
      "      \" first\": -1.69921875,\n",
      "      \" last\": -2.41015625\n",
      "    },\n",
      "    {\n",
      "      \" Award\": -2.845703125,\n",
      "      \" Awards\": -0.1270751953125,\n",
      "      \"\\ufffd\": -4.12109375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' the Academy Awards', 'token_index': 0, 'prompt_index': 0}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "4.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " a\n",
      "\t{'finish_reason': None, 'index': 0, 'logprobs': <OpenAIObject at 0x123602eb8> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a', 'token_index': 0, 'prompt_index': 0}\n",
      "\n",
      " positive\n",
      "\t{'finish_reason': None, 'index': 0, 'logprobs': <OpenAIObject at 0x1236140a0> JSON: {\n",
      "  \"text_offset\": [\n",
      "    2\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -8.34375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" positive\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' positive', 'token_index': 1, 'prompt_index': 0}\n",
      "\n",
      " day\n",
      "\t{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x123614258> JSON: {\n",
      "  \"text_offset\": [\n",
      "    11\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -0.4296875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" day\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" day\": -0.4296875,\n",
      "      \" experience\": -3.392578125,\n",
      "      \" one\": -3.216796875\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' day', 'token_index': 2, 'prompt_index': 0}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " clearer\n",
      "\t{'finish_reason': None, 'index': 1, 'logprobs': <OpenAIObject at 0x123614410> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -12.46875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" clearer\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' clearer', 'token_index': 0, 'prompt_index': 0}\n",
      "\n",
      " and\n",
      "\t{'finish_reason': None, 'index': 1, 'logprobs': <OpenAIObject at 0x1236145c8> JSON: {\n",
      "  \"text_offset\": [\n",
      "    8\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.021484375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" and\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" and\": -2.021484375,\n",
      "      \" than\": -0.619140625,\n",
      "      \",\": -2.5078125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' and', 'token_index': 1, 'prompt_index': 0}\n",
      "\n",
      " colder\n",
      "\t{'finish_reason': 'length', 'index': 1, 'logprobs': <OpenAIObject at 0x123614780> JSON: {\n",
      "  \"text_offset\": [\n",
      "    12\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -3.44140625\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" colder\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" brighter\": -1.658203125,\n",
      "      \" more\": -2.109375,\n",
      "      \" warmer\": -2.234375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' colder', 'token_index': 2, 'prompt_index': 0}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "5.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: ['24 years ago', 'months did it']\n",
      "FULLS: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x123602a98> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    3,\n",
      "    9\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -8.1484375,\n",
      "    -1.931640625,\n",
      "    -0.8271484375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" 24\",\n",
      "    \" years\",\n",
      "    \" ago\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" hours\": -1.21484375,\n",
      "      \" years\": -1.931640625,\n",
      "      \"th\": -1.8212890625\n",
      "    },\n",
      "    {\n",
      "      \" ago\": -0.8271484375,\n",
      "      \" since\": -1.0576171875,\n",
      "      \" to\": -2.794921875\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' 24 years ago', 'token_index': 0, 'prompt_index': 0}, {'finish_reason': 'length', 'index': 1, 'logprobs': <OpenAIObject at 0x123602c50> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    7,\n",
      "    11\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -5.46484375,\n",
      "    -5,\n",
      "    -1.4169921875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" months\",\n",
      "    \" did\",\n",
      "    \" it\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" of\": -2.251953125,\n",
      "      \" people\": -2.943359375,\n",
      "      \" times\": -1.8173828125\n",
      "    },\n",
      "    {\n",
      "      \" are\": -0.167724609375,\n",
      "      \" is\": -3.70703125,\n",
      "      \" will\": -3.78515625\n",
      "    },\n",
      "    {\n",
      "      \" it\": -1.4169921875,\n",
      "      \" the\": -3.44140625,\n",
      "      \" you\": -0.48876953125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' months did it', 'token_index': 0, 'prompt_index': 1}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "6.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a\n",
      "\t{'finish_reason': None, 'index': 0, 'logprobs': <OpenAIObject at 0x123602048> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a', 'token_index': 0, 'prompt_index': 0}\n",
      "\n",
      " day\n",
      "\t{'finish_reason': None, 'index': 0, 'logprobs': <OpenAIObject at 0x123602200> JSON: {\n",
      "  \"text_offset\": [\n",
      "    2\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.53125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" day\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' day', 'token_index': 1, 'prompt_index': 0}\n",
      "\n",
      " when\n",
      "\t{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x1236023b8> JSON: {\n",
      "  \"text_offset\": [\n",
      "    6\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.763671875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" when\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" for\": -2.296875,\n",
      "      \" of\": -1.1123046875,\n",
      "      \" that\": -2.380859375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' when', 'token_index': 2, 'prompt_index': 0}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " times\n",
      "\t{'finish_reason': None, 'index': 1, 'logprobs': <OpenAIObject at 0x123602570> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.8173828125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" times\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" of\": -2.251953125,\n",
      "      \" people\": -2.943359375,\n",
      "      \" times\": -1.8173828125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' times', 'token_index': 0, 'prompt_index': 1}\n",
      "\n",
      " I\n",
      "\t{'finish_reason': None, 'index': 1, 'logprobs': <OpenAIObject at 0x123602728> JSON: {\n",
      "  \"text_offset\": [\n",
      "    6\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -4.80078125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" I\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" did\": -2.203125,\n",
      "      \" has\": -2.447265625,\n",
      "      \" have\": -0.912109375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' I', 'token_index': 1, 'prompt_index': 1}\n",
      "\n",
      "'ve\n",
      "\t{'finish_reason': 'length', 'index': 1, 'logprobs': <OpenAIObject at 0x1236028e0> JSON: {\n",
      "  \"text_offset\": [\n",
      "    8\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.306640625\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \"'ve\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" have\": -1.5625,\n",
      "      \"'ve\": -1.306640625,\n",
      "      \"\\ufffd\": -1.9375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': \"'ve\", 'token_index': 2, 'prompt_index': 1}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "7.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: [\"Mozilla's ann\", 'reminded of this', 'lessons would it', 'teenagers are allowed']\n",
      "FULLS: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x1235ecaf0> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    8,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -8.6796875,\n",
      "    -1.6123046875,\n",
      "    -8.8515625\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" Mozilla\",\n",
      "    \"'s\",\n",
      "    \" ann\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" Day\": -3.55859375,\n",
      "      \"'s\": -1.6123046875,\n",
      "      \"\\ufffd\": -0.7822265625\n",
      "    },\n",
      "    {\n",
      "      \" annual\": -1.1337890625,\n",
      "      \" big\": -2.85546875,\n",
      "      \" first\": -2.888671875\n",
      "    }\n",
      "  ]\n",
      "}, 'text': \" Mozilla's ann\", 'token_index': 0, 'prompt_index': 0}, {'finish_reason': 'length', 'index': 1, 'logprobs': <OpenAIObject at 0x1235ecca8> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    9,\n",
      "    12\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -8.5859375,\n",
      "    -0.64453125,\n",
      "    -3.96484375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" reminded\",\n",
      "    \" of\",\n",
      "    \" this\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" by\": -2.802734375,\n",
      "      \" of\": -0.64453125,\n",
      "      \" that\": -1.806640625\n",
      "    },\n",
      "    {\n",
      "      \" a\": -2.478515625,\n",
      "      \" how\": -2.03125,\n",
      "      \" the\": -1.1337890625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' reminded of this', 'token_index': 0, 'prompt_index': 0}, {'finish_reason': 'length', 'index': 2, 'logprobs': <OpenAIObject at 0x1235ece60> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    8,\n",
      "    14\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -8.1171875,\n",
      "    -4.93359375,\n",
      "    -2.1171875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" lessons\",\n",
      "    \" would\",\n",
      "    \" it\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" of\": -2.251953125,\n",
      "      \" people\": -2.943359375,\n",
      "      \" times\": -1.8173828125\n",
      "    },\n",
      "    {\n",
      "      \" can\": -2.05078125,\n",
      "      \" do\": -2.10546875,\n",
      "      \" have\": -2.083984375\n",
      "    },\n",
      "    {\n",
      "      \" I\": -2.369140625,\n",
      "      \" it\": -2.1171875,\n",
      "      \" you\": -1.1865234375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' lessons would it', 'token_index': 0, 'prompt_index': 1}, {'finish_reason': 'length', 'index': 3, 'logprobs': <OpenAIObject at 0x1235c3db0> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    10,\n",
      "    14\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -8.5703125,\n",
      "    -2.22265625,\n",
      "    -6.15625\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" teenagers\",\n",
      "    \" are\",\n",
      "    \" allowed\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" of\": -2.251953125,\n",
      "      \" people\": -2.943359375,\n",
      "      \" times\": -1.8173828125\n",
      "    },\n",
      "    {\n",
      "      \" are\": -2.22265625,\n",
      "      \" do\": -1.8779296875,\n",
      "      \" have\": -2.16796875\n",
      "    },\n",
      "    {\n",
      "      \" in\": -2.75390625,\n",
      "      \" out\": -2.611328125,\n",
      "      \" there\": -2.60546875\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' teenagers are allowed', 'token_index': 0, 'prompt_index': 1}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "8.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " ways\n",
      "\t{'finish_reason': None, 'index': 2, 'logprobs': <OpenAIObject at 0x1237e0620> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -4.234375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" ways\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" of\": -2.251953125,\n",
      "      \" people\": -2.943359375,\n",
      "      \" times\": -1.8173828125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' ways', 'token_index': 0, 'prompt_index': 1}\n",
      "\n",
      " are\n",
      "\t{'finish_reason': None, 'index': 2, 'logprobs': <OpenAIObject at 0x1237e07d8> JSON: {\n",
      "  \"text_offset\": [\n",
      "    5\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.26953125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" are\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" are\": -1.26953125,\n",
      "      \" can\": -0.9296875,\n",
      "      \" to\": -2.728515625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' are', 'token_index': 1, 'prompt_index': 1}\n",
      "\n",
      " there\n",
      "\t{'finish_reason': 'length', 'index': 2, 'logprobs': <OpenAIObject at 0x1237e0990> JSON: {\n",
      "  \"text_offset\": [\n",
      "    9\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -0.0533447265625\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" there\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" there\": -0.0533447265625,\n",
      "      \" we\": -4.8984375,\n",
      "      \" you\": -3.93359375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' there', 'token_index': 2, 'prompt_index': 1}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " a\n",
      "\t{'finish_reason': None, 'index': 0, 'logprobs': <OpenAIObject at 0x1237e0b48> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a', 'token_index': 0, 'prompt_index': 0}\n",
      "\n",
      " grey\n",
      "\t{'finish_reason': None, 'index': 0, 'logprobs': <OpenAIObject at 0x1237e0d00> JSON: {\n",
      "  \"text_offset\": [\n",
      "    2\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -7.5390625\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" grey\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' grey', 'token_index': 1, 'prompt_index': 0}\n",
      "\n",
      " and\n",
      "\t{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x1237e0eb8> JSON: {\n",
      "  \"text_offset\": [\n",
      "    7\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.77734375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" and\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" and\": -1.77734375,\n",
      "      \" day\": -0.630859375,\n",
      "      \",\": -2.103515625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' and', 'token_index': 2, 'prompt_index': 0}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " will\n",
      "\t{'finish_reason': None, 'index': 3, 'logprobs': <OpenAIObject at 0x1235ec0a0> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -6.12890625\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" will\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" of\": -2.251953125,\n",
      "      \" people\": -2.943359375,\n",
      "      \" times\": -1.8173828125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' will', 'token_index': 0, 'prompt_index': 1}\n",
      "\n",
      " die\n",
      "\t{'finish_reason': None, 'index': 3, 'logprobs': <OpenAIObject at 0x1235ec258> JSON: {\n",
      "  \"text_offset\": [\n",
      "    5\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.4814453125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" die\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" be\": -2.11328125,\n",
      "      \" die\": -1.4814453125,\n",
      "      \" it\": -2.703125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' die', 'token_index': 1, 'prompt_index': 1}\n",
      "\n",
      "?\n",
      "\t{'finish_reason': 'length', 'index': 3, 'logprobs': <OpenAIObject at 0x1235ec410> JSON: {\n",
      "  \"text_offset\": [\n",
      "    9\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.220703125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \"?\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" in\": -1.728515625,\n",
      "      \"?\": -2.220703125,\n",
      "      \"?\\\"\": -2.248046875\n",
      "    }\n",
      "  ]\n",
      "}, 'text': '?', 'token_index': 2, 'prompt_index': 1}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " beautiful\n",
      "\t{'finish_reason': None, 'index': 1, 'logprobs': <OpenAIObject at 0x1235ec5c8> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -8.1484375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" beautiful\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' beautiful', 'token_index': 0, 'prompt_index': 0}\n",
      "\n",
      ".\n",
      "\t{'finish_reason': None, 'index': 1, 'logprobs': <OpenAIObject at 0x1235ec780> JSON: {\n",
      "  \"text_offset\": [\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.32421875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \".\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" and\": -1.703125,\n",
      "      \",\": -1.5087890625,\n",
      "      \".\": -1.32421875\n",
      "    }\n",
      "  ]\n",
      "}, 'text': '.', 'token_index': 1, 'prompt_index': 0}\n",
      "\n",
      " It\n",
      "\t{'finish_reason': 'length', 'index': 1, 'logprobs': <OpenAIObject at 0x1235ec938> JSON: {\n",
      "  \"text_offset\": [\n",
      "    11\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.126953125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" It\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" I\": -1.94140625,\n",
      "      \" It\": -2.126953125,\n",
      "      \" The\": -2.13671875\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' It', 'token_index': 2, 'prompt_index': 0}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:404: UserWarning: query_gpt_mock received unused kwargs: {'engine_i': 0, 'max_tokens': 3, 'logprobs': 3}\n",
      "  warnings.warn(f'query_gpt_mock received unused kwargs: {kwargs}')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:674: UserWarning: strip_output=True is not supported in stream mode. Automatically setting it to False.\n",
      "  warnings.warn('strip_output=True is not supported in stream '\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:679: UserWarning: Streaming mode does not support manual truncation of stop phrases and your current backend has limited support for truncation.\n",
      "  'Streaming mode does not support manual truncation of '\n"
     ]
    }
   ],
   "source": [
    "mock_res = test_backend('mock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T03:52:49.019629Z",
     "start_time": "2022-04-14T03:52:47.403780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"huggingface\".\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "1.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'huggingface', 'query_func': 'query_gpt_huggingface'}}\n",
      "TEXTS: ['the day that']\n",
      "FULLS: [{'generated_text': ' the day that', 'prompt_index': 0}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "2.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'huggingface', 'query_func': 'query_gpt_huggingface'}}\n",
      " \n",
      "\t{'generated_text': ' the day that', 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'generated_text': ' the day that', 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'generated_text': ' the day that', 'index': 0, 'prompt_index': 0, 'finish_reason': None}\n",
      "\n",
      "that \n",
      "\t{'generated_text': ' the day that', 'index': 0, 'prompt_index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "3.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'huggingface', 'query_func': 'query_gpt_huggingface'}}\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    },
    {
     "ename": "MockFunctionException",
     "evalue": "400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-125M",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMockFunctionException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c7d97b561d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhf_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huggingface'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-bf370276c478>\u001b[0m in \u001b[0;36mtest_backend\u001b[0;34m(backend, results)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Print results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonhm/htools/htools/meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1936\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1937\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(cls, prompt, strip_output, log, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMockFunctionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'stream'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunc_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMockFunctionException\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-125M"
     ]
    }
   ],
   "source": [
    "hf_res = test_backend('huggingface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:25:47.312235Z",
     "start_time": "2022-04-13T03:25:47.269492Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 prompts, 1 completion per prompt, stream=False\n",
    "p2c1s0 = MOCKS[True, False, False]\n",
    "p1c2s0 = MOCKS[False, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:25:53.635408Z",
     "start_time": "2022-04-13T03:25:53.587866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['24 years ago', 'months did it'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x125bacaf0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       3,\n",
       "       9\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -8.1484375,\n",
       "       -1.931640625,\n",
       "       -0.8271484375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" 24\",\n",
       "       \" years\",\n",
       "       \" ago\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" hours\": -1.21484375,\n",
       "         \" years\": -1.931640625,\n",
       "         \"th\": -1.8212890625\n",
       "       },\n",
       "       {\n",
       "         \" ago\": -0.8271484375,\n",
       "         \" since\": -1.0576171875,\n",
       "         \" to\": -2.794921875\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' 24 years ago',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x125bacca8> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       7,\n",
       "       11\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -5.46484375,\n",
       "       -5,\n",
       "       -1.4169921875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" months\",\n",
       "       \" did\",\n",
       "       \" it\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" of\": -2.251953125,\n",
       "         \" people\": -2.943359375,\n",
       "         \" times\": -1.8173828125\n",
       "       },\n",
       "       {\n",
       "         \" are\": -0.167724609375,\n",
       "         \" is\": -3.70703125,\n",
       "         \" will\": -3.78515625\n",
       "       },\n",
       "       {\n",
       "         \" it\": -1.4169921875,\n",
       "         \" the\": -3.44140625,\n",
       "         \" you\": -0.48876953125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' months did it',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 1}])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, fulls = containerize(*postprocess_gpt_response(p2c1s0))\n",
    "postprocess_response((texts, fulls), n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:41:52.515976Z",
     "start_time": "2022-04-13T03:41:52.450132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: [' my mother�', ' the Academy Awards']\n",
      "FULL: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x1245f1990> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    3,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.384765625,\n",
      "    -5.4375,\n",
      "    -0.44677734375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" my\",\n",
      "    \" mother\",\n",
      "    \"\\ufffd\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" birthday\": -2.107421875,\n",
      "      \" first\": -1.3818359375,\n",
      "      \" last\": -2.130859375\n",
      "    },\n",
      "    {\n",
      "      \"'s\": -1.509765625,\n",
      "      \"-\": -2.630859375,\n",
      "      \"\\ufffd\": -0.44677734375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' my mother�', 'token_index': 0}, {'finish_reason': 'length', 'index': 1, 'logprobs': <OpenAIObject at 0x1245f1b48> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    4,\n",
      "    12\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.8720703125,\n",
      "    -8.796875,\n",
      "    -0.1270751953125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" the\",\n",
      "    \" Academy\",\n",
      "    \" Awards\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" day\": -2.28125,\n",
      "      \" first\": -1.69921875,\n",
      "      \" last\": -2.41015625\n",
      "    },\n",
      "    {\n",
      "      \" Award\": -2.845703125,\n",
      "      \" Awards\": -0.1270751953125,\n",
      "      \"\\ufffd\": -4.12109375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' the Academy Awards', 'token_index': 0}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['my mother�', 'the Academy Awards'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x1245f1990> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       3,\n",
       "       10\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -2.384765625,\n",
       "       -5.4375,\n",
       "       -0.44677734375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" my\",\n",
       "       \" mother\",\n",
       "       \"\\ufffd\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" birthday\": -2.107421875,\n",
       "         \" first\": -1.3818359375,\n",
       "         \" last\": -2.130859375\n",
       "       },\n",
       "       {\n",
       "         \"'s\": -1.509765625,\n",
       "         \"-\": -2.630859375,\n",
       "         \"\\ufffd\": -0.44677734375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' my mother�',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x1245f1b48> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       4,\n",
       "       12\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.8720703125,\n",
       "       -8.796875,\n",
       "       -0.1270751953125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" the\",\n",
       "       \" Academy\",\n",
       "       \" Awards\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" day\": -2.28125,\n",
       "         \" first\": -1.69921875,\n",
       "         \" last\": -2.41015625\n",
       "       },\n",
       "       {\n",
       "         \" Award\": -2.845703125,\n",
       "         \" Awards\": -0.1270751953125,\n",
       "         \"\\ufffd\": -4.12109375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' the Academy Awards',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0}])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, fulls = postprocess_gpt_response(p1c2s0)\n",
    "postprocess_response((texts, fulls), n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:42:57.404628Z",
     "start_time": "2022-04-13T03:42:57.368447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' my mother�', ' the Academy Awards']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:58:46.231595Z",
     "start_time": "2022-04-13T03:58:46.199365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"mock\".\n",
      "{'n': 1, 'stream': False, 'prompt': 'ac', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      ">>> response: ([' a typical rainy'], [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x125a06e08> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    2,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375,\n",
      "    -6.06640625,\n",
      "    -5.671875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\",\n",
      "    \" typical\",\n",
      "    \" rainy\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    },\n",
      "    {\n",
      "      \" Monday\": -2.974609375,\n",
      "      \" day\": -1.095703125,\n",
      "      \" work\": -2.94140625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a typical rainy', 'token_index': 0}])\n",
      "TEXT: [' a typical rainy']\n",
      "FULL: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x125a06e08> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    2,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375,\n",
      "    -6.06640625,\n",
      "    -5.671875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\",\n",
      "    \" typical\",\n",
      "    \" rainy\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    },\n",
      "    {\n",
      "      \" Monday\": -2.974609375,\n",
      "      \" day\": -1.095703125,\n",
      "      \" work\": -2.94140625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a typical rainy', 'token_index': 0}]\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('mock'):\n",
    "    res = gpt.query('ac', n=1, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:58:50.103218Z",
     "start_time": "2022-04-13T03:58:50.052234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a typical rainy'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x125a06e08> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       2,\n",
       "       10\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.3818359375,\n",
       "       -6.06640625,\n",
       "       -5.671875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" a\",\n",
       "       \" typical\",\n",
       "       \" rainy\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" big\": -1.83984375,\n",
       "         \" day\": -2.53125,\n",
       "         \" very\": -2.916015625\n",
       "       },\n",
       "       {\n",
       "         \" Monday\": -2.974609375,\n",
       "         \" day\": -1.095703125,\n",
       "         \" work\": -2.94140625\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' a typical rainy',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0}])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
