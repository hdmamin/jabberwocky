{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Test out new batch query functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:36.673802Z",
     "start_time": "2022-04-15T23:31:36.655347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:54.382690Z",
     "start_time": "2022-04-15T23:31:54.321619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/sample_stream_response.pkl.\n",
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/gooseai_sample_responses.pkl.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from jabberwocky.config import C\n",
    "from jabberwocky.openai_utils import load_prompt, load_openai_api_key, \\\n",
    "    GPTBackend, query_kwargs_grid, MOCKS, postprocess_gpt_response, \\\n",
    "    containerize, truncate_at_first_stop, query_gpt_mock\n",
    "from jabberwocky.utils import strip\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:55.074861Z",
     "start_time": "2022-04-15T23:31:55.041276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/jabberwocky\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:55.606365Z",
     "start_time": "2022-04-15T23:31:55.564340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTBackend()\n",
    "gpt.switch('repeat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:31:56.233296Z",
     "start_time": "2022-04-15T23:31:56.191727Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess_response(response, n, trunc_full=True, strip_output=True, \n",
    "                         **kwargs):\n",
    "    text, full_response = containerize(*response)\n",
    "    print('TEXT:', text)\n",
    "    print('FULL:', full_response)\n",
    "\n",
    "    # Manually check for stop phrases because most backends either don't\n",
    "    # or truncate AFTER the stop phrase which is rarely what we want.\n",
    "    stop = kwargs.get('stop', [])\n",
    "    clean_text = []\n",
    "    clean_full = []\n",
    "    for i, (text_, resp_) in enumerate(zip(text, full_response)):\n",
    "        text_ = truncate_at_first_stop(\n",
    "            text_,\n",
    "            stop_phrases=stop,\n",
    "            finish_reason=resp_.get('finish_reason', ''),\n",
    "            trunc_full=trunc_full,\n",
    "            trunc_partial=True\n",
    "        )\n",
    "        clean_text.append(strip(text_, strip_output))\n",
    "        clean_full.append({**resp_, 'prompt_index': i // n})\n",
    "\n",
    "    return clean_text, clean_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:38.403079Z",
     "start_time": "2022-04-15T23:53:38.029473Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_backend(backend, results=None):\n",
    "    def gen():\n",
    "        yield from results.kwargs\n",
    "            \n",
    "    full_keys = ('prompt_index', 'index', 'finish_reason')\n",
    "    with gpt(backend):\n",
    "        kwargs_list = []\n",
    "        resp_list = []\n",
    "        gen_kwargs = gen if results else query_kwargs_grid\n",
    "        for i, kwargs in enumerate(gen_kwargs(), start=1):\n",
    "            print(f'\\n\\n{i}.')\n",
    "            if isinstance(kwargs['prompt'], str):\n",
    "                np = 1\n",
    "            else:\n",
    "                np = len(kwargs['prompt'])\n",
    "            expected_prompt_idx = list(range(np))\n",
    "                \n",
    "            # Get completion, either from new query or cached result.\n",
    "            if results:\n",
    "                res = results.res[i - 1]\n",
    "            else:\n",
    "                res = gpt.query(**kwargs)\n",
    "                \n",
    "            # Print results.\n",
    "            if kwargs['stream']:\n",
    "                cur = []\n",
    "                actual_prompt_idx = []\n",
    "                for tok, tok_full in res:\n",
    "                    cur.append((tok, tok_full))\n",
    "                    print(tok)\n",
    "                    print('\\t' + str(select(tok_full, keep=full_keys)) + '\\n')\n",
    "                    if tok_full['finish_reason']: print('\\n---\\n')\n",
    "                    actual_prompt_idx.append(tok_full.get('prompt_index', -1))\n",
    "            else:\n",
    "                texts, fulls = res\n",
    "                print('TEXTS:', texts)\n",
    "                print('FULLS:',\n",
    "                      [{key: full.get(key) for key in full_keys}\n",
    "                       for full in fulls])\n",
    "                actual_prompt_idx = [full['prompt_index'] for full in fulls]\n",
    "                cur = res\n",
    "            \n",
    "            actual_prompt_idx = sorted(set(actual_prompt_idx))\n",
    "            assert actual_prompt_idx == expected_prompt_idx, \\\n",
    "                    f'Expected prompt indices {expected_prompt_idx}, ' \\\n",
    "                    f'got {actual_prompt_idx}'\n",
    "            print(spacer())\n",
    "\n",
    "            kwargs_list.append(kwargs)\n",
    "            resp_list.append(cur)\n",
    "    return Results(kwargs=kwargs_list, res=resp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues\n",
    "\n",
    "[X] - no prompt_index in stream=True mode for either repeat or banana (pretty sure not for paid backends either)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:38.926617Z",
     "start_time": "2022-04-15T23:53:38.818143Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"repeat\".\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "1.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "2.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "3.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS', 'YESTERDAY WAS']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "4.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "5.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS', 'HOW MANY']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "6.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOW \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "MANY \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "7.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "TEXTS: ['YESTERDAY WAS', 'YESTERDAY WAS', 'HOW MANY', 'HOW MANY']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "8.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'repeat', 'query_func': 'query_gpt_repeat'}}\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "YESTERDAY \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "WAS \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOW \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "MANY \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOW \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "MANY \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:415: UserWarning: Unused kwargs {'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:415: UserWarning: Unused kwargs {'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:415: UserWarning: Unused kwargs {'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n",
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:415: UserWarning: Unused kwargs {'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3} received by query_gpt_repeat.\n",
      "  warnings.warn(f'Unused kwargs {kwargs} received by query_gpt_repeat.')\n"
     ]
    }
   ],
   "source": [
    "repeat_res = test_backend('repeat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:43.918149Z",
     "start_time": "2022-04-15T23:53:39.309757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "1.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:470: UserWarning: query_gpt_banana received unused kwargs {'n': 1, 'stream': False, 'engine_i': 0, 'logprobs': 3}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTS: ['the one year']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "2.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:470: UserWarning: query_gpt_banana received unused kwargs {'n': 1, 'stream': True, 'engine_i': 0, 'logprobs': 3}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "3.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:470: UserWarning: query_gpt_banana received unused kwargs {'stream': False, 'engine_i': 0, 'logprobs': 3}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTS: ['a big day', 'my last day']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "4.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:470: UserWarning: query_gpt_banana received unused kwargs {'stream': True, 'engine_i': 0, 'logprobs': 3}.\n",
      "  warnings.warn(f'query_gpt_banana received unused kwargs {kwargs}.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "birthday. \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "5.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "TEXTS: ['the first day', 'minutes are there']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "6.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "last \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "7.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "TEXTS: ['my last day', 'a great day', 'times have you', 'minutes are there']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "8.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'How many', 'meta': {'backend_name': 'banana', 'query_func': 'query_gpt_banana'}}\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "a \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "big \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "people \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "are \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "there \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n",
      "Writing data to data/tmp/banana_res.pkl.\n",
      "Object loaded from data/tmp/banana_res.pkl.\n"
     ]
    }
   ],
   "source": [
    "# Even though it's free, try to avoid hitting the API too much just in case \n",
    "# they have some rate limit.\n",
    "banana_res = test_backend('banana')\n",
    "save(banana_res, 'data/tmp/banana_res.pkl')\n",
    "banana_res = load('data/tmp/banana_res.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:50.144814Z",
     "start_time": "2022-04-15T23:53:50.091853Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"banana\".\n",
      "\n",
      "\n",
      "1.\n",
      "TEXTS: ['the one year']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "2.\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "3.\n",
      "TEXTS: ['a big day', 'my last day']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "4.\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "birthday. \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "5.\n",
      "TEXTS: ['the first day', 'minutes are there']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "6.\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "the \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "last \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "7.\n",
      "TEXTS: ['my last day', 'a great day', 'times have you', 'minutes are there']\n",
      "FULLS: [{'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 0, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}, {'prompt_index': 1, 'index': None, 'finish_reason': None}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "8.\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "a \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "big \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "my \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "first \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "day \n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "times \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "have \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      "you \n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "people \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "are \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "there \n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': 'dummy'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "banana_res = test_backend('banana', results=banana_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:53:50.902352Z",
     "start_time": "2022-04-15T23:53:50.835222Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"mock\".\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "1.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: ['a typical rainy']\n",
      "FULLS: [{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "2.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " a\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " bit\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " busy\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "3.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: ['my mother�', 'the Academy Awards']\n",
      "FULLS: [{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}, {'prompt_index': 0, 'index': 1, 'finish_reason': 'length'}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: False\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "4.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': 'Yesterday was', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " a\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " positive\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " day\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " clearer\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      " and\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      " colder\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: False\n",
      "\n",
      "\n",
      "5.\n",
      "{'n': 1, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: ['24 years ago', 'months did it']\n",
      "FULLS: [{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}, {'prompt_index': 1, 'index': 1, 'finish_reason': 'length'}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: False\n",
      "stream: True\n",
      "\n",
      "\n",
      "6.\n",
      "{'n': 1, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " a\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " day\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " when\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " times\n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      " I\n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': None}\n",
      "\n",
      "'ve\n",
      "\t{'prompt_index': 1, 'index': 1, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: False\n",
      "\n",
      "\n",
      "7.\n",
      "{'n': 2, 'stream': False, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      "TEXTS: [\"Mozilla's ann\", 'reminded of this', 'lessons would it', 'teenagers are allowed']\n",
      "FULLS: [{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}, {'prompt_index': 0, 'index': 1, 'finish_reason': 'length'}, {'prompt_index': 1, 'index': 2, 'finish_reason': 'length'}, {'prompt_index': 1, 'index': 3, 'finish_reason': 'length'}]\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "np>1: True\n",
      "nc>1: True\n",
      "stream: True\n",
      "\n",
      "\n",
      "8.\n",
      "{'n': 2, 'stream': True, 'engine_i': 0, 'max_tokens': 3, 'logprobs': 3, 'prompt': ['Yesterday was', 'How many'], 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      " ways\n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      " are\n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': None}\n",
      "\n",
      " there\n",
      "\t{'prompt_index': 1, 'index': 2, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " a\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " grey\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': None}\n",
      "\n",
      " and\n",
      "\t{'prompt_index': 0, 'index': 0, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " will\n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      " die\n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': None}\n",
      "\n",
      "?\n",
      "\t{'prompt_index': 1, 'index': 3, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      " beautiful\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      ".\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': None}\n",
      "\n",
      " It\n",
      "\t{'prompt_index': 0, 'index': 1, 'finish_reason': 'length'}\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "mock_res = test_backend('mock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T23:01:50.914410Z",
     "start_time": "2022-04-15T23:01:50.871188Z"
    }
   },
   "outputs": [],
   "source": [
    "# hf_res = test_backend('huggingface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:25:47.312235Z",
     "start_time": "2022-04-13T03:25:47.269492Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 prompts, 1 completion per prompt, stream=False\n",
    "p2c1s0 = MOCKS[True, False, False]\n",
    "p1c2s0 = MOCKS[False, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:25:53.635408Z",
     "start_time": "2022-04-13T03:25:53.587866Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['24 years ago', 'months did it'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x125bacaf0> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       3,\n",
       "       9\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -8.1484375,\n",
       "       -1.931640625,\n",
       "       -0.8271484375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" 24\",\n",
       "       \" years\",\n",
       "       \" ago\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" hours\": -1.21484375,\n",
       "         \" years\": -1.931640625,\n",
       "         \"th\": -1.8212890625\n",
       "       },\n",
       "       {\n",
       "         \" ago\": -0.8271484375,\n",
       "         \" since\": -1.0576171875,\n",
       "         \" to\": -2.794921875\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' 24 years ago',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x125bacca8> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       7,\n",
       "       11\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -5.46484375,\n",
       "       -5,\n",
       "       -1.4169921875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" months\",\n",
       "       \" did\",\n",
       "       \" it\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" of\": -2.251953125,\n",
       "         \" people\": -2.943359375,\n",
       "         \" times\": -1.8173828125\n",
       "       },\n",
       "       {\n",
       "         \" are\": -0.167724609375,\n",
       "         \" is\": -3.70703125,\n",
       "         \" will\": -3.78515625\n",
       "       },\n",
       "       {\n",
       "         \" it\": -1.4169921875,\n",
       "         \" the\": -3.44140625,\n",
       "         \" you\": -0.48876953125\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' months did it',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 1}])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, fulls = containerize(*postprocess_gpt_response(p2c1s0))\n",
    "postprocess_response((texts, fulls), n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:41:52.515976Z",
     "start_time": "2022-04-13T03:41:52.450132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: [' my mother�', ' the Academy Awards']\n",
      "FULL: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x1245f1990> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    3,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -2.384765625,\n",
      "    -5.4375,\n",
      "    -0.44677734375\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" my\",\n",
      "    \" mother\",\n",
      "    \"\\ufffd\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" birthday\": -2.107421875,\n",
      "      \" first\": -1.3818359375,\n",
      "      \" last\": -2.130859375\n",
      "    },\n",
      "    {\n",
      "      \"'s\": -1.509765625,\n",
      "      \"-\": -2.630859375,\n",
      "      \"\\ufffd\": -0.44677734375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' my mother�', 'token_index': 0}, {'finish_reason': 'length', 'index': 1, 'logprobs': <OpenAIObject at 0x1245f1b48> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    4,\n",
      "    12\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.8720703125,\n",
      "    -8.796875,\n",
      "    -0.1270751953125\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" the\",\n",
      "    \" Academy\",\n",
      "    \" Awards\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" day\": -2.28125,\n",
      "      \" first\": -1.69921875,\n",
      "      \" last\": -2.41015625\n",
      "    },\n",
      "    {\n",
      "      \" Award\": -2.845703125,\n",
      "      \" Awards\": -0.1270751953125,\n",
      "      \"\\ufffd\": -4.12109375\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' the Academy Awards', 'token_index': 0}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['my mother�', 'the Academy Awards'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x1245f1990> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       3,\n",
       "       10\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -2.384765625,\n",
       "       -5.4375,\n",
       "       -0.44677734375\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" my\",\n",
       "       \" mother\",\n",
       "       \"\\ufffd\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" birthday\": -2.107421875,\n",
       "         \" first\": -1.3818359375,\n",
       "         \" last\": -2.130859375\n",
       "       },\n",
       "       {\n",
       "         \"'s\": -1.509765625,\n",
       "         \"-\": -2.630859375,\n",
       "         \"\\ufffd\": -0.44677734375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' my mother�',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0},\n",
       "  {'finish_reason': 'length',\n",
       "   'index': 1,\n",
       "   'logprobs': <OpenAIObject at 0x1245f1b48> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       4,\n",
       "       12\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.8720703125,\n",
       "       -8.796875,\n",
       "       -0.1270751953125\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" the\",\n",
       "       \" Academy\",\n",
       "       \" Awards\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" day\": -2.28125,\n",
       "         \" first\": -1.69921875,\n",
       "         \" last\": -2.41015625\n",
       "       },\n",
       "       {\n",
       "         \" Award\": -2.845703125,\n",
       "         \" Awards\": -0.1270751953125,\n",
       "         \"\\ufffd\": -4.12109375\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' the Academy Awards',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0}])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, fulls = postprocess_gpt_response(p1c2s0)\n",
    "postprocess_response((texts, fulls), n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:42:57.404628Z",
     "start_time": "2022-04-13T03:42:57.368447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' my mother�', ' the Academy Awards']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:58:46.231595Z",
     "start_time": "2022-04-13T03:58:46.199365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching openai backend to \"mock\".\n",
      "{'n': 1, 'stream': False, 'prompt': 'ac', 'meta': {'backend_name': 'mock', 'query_func': 'query_gpt_mock'}}\n",
      ">>> response: ([' a typical rainy'], [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x125a06e08> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    2,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375,\n",
      "    -6.06640625,\n",
      "    -5.671875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\",\n",
      "    \" typical\",\n",
      "    \" rainy\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    },\n",
      "    {\n",
      "      \" Monday\": -2.974609375,\n",
      "      \" day\": -1.095703125,\n",
      "      \" work\": -2.94140625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a typical rainy', 'token_index': 0}])\n",
      "TEXT: [' a typical rainy']\n",
      "FULL: [{'finish_reason': 'length', 'index': 0, 'logprobs': <OpenAIObject at 0x125a06e08> JSON: {\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    2,\n",
      "    10\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -1.3818359375,\n",
      "    -6.06640625,\n",
      "    -5.671875\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \" a\",\n",
      "    \" typical\",\n",
      "    \" rainy\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \" a\": -1.3818359375,\n",
      "      \" my\": -2.384765625,\n",
      "      \" the\": -1.8720703125\n",
      "    },\n",
      "    {\n",
      "      \" big\": -1.83984375,\n",
      "      \" day\": -2.53125,\n",
      "      \" very\": -2.916015625\n",
      "    },\n",
      "    {\n",
      "      \" Monday\": -2.974609375,\n",
      "      \" day\": -1.095703125,\n",
      "      \" work\": -2.94140625\n",
      "    }\n",
      "  ]\n",
      "}, 'text': ' a typical rainy', 'token_index': 0}]\n",
      "Switching  backend back to \"repeat\".\n"
     ]
    }
   ],
   "source": [
    "with gpt('mock'):\n",
    "    res = gpt.query('ac', n=1, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:58:50.103218Z",
     "start_time": "2022-04-13T03:58:50.052234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a typical rainy'],\n",
       " [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': <OpenAIObject at 0x125a06e08> JSON: {\n",
       "     \"text_offset\": [\n",
       "       0,\n",
       "       2,\n",
       "       10\n",
       "     ],\n",
       "     \"token_logprobs\": [\n",
       "       -1.3818359375,\n",
       "       -6.06640625,\n",
       "       -5.671875\n",
       "     ],\n",
       "     \"tokens\": [\n",
       "       \" a\",\n",
       "       \" typical\",\n",
       "       \" rainy\"\n",
       "     ],\n",
       "     \"top_logprobs\": [\n",
       "       {\n",
       "         \" a\": -1.3818359375,\n",
       "         \" my\": -2.384765625,\n",
       "         \" the\": -1.8720703125\n",
       "       },\n",
       "       {\n",
       "         \" big\": -1.83984375,\n",
       "         \" day\": -2.53125,\n",
       "         \" very\": -2.916015625\n",
       "       },\n",
       "       {\n",
       "         \" Monday\": -2.974609375,\n",
       "         \" day\": -1.095703125,\n",
       "         \" work\": -2.94140625\n",
       "       }\n",
       "     ]\n",
       "   },\n",
       "   'text': ' a typical rainy',\n",
       "   'token_index': 0,\n",
       "   'prompt_index': 0}])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
