{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:51:59.992662Z",
     "start_time": "2021-08-19T03:51:56.470548Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from threading import Thread, Lock\n",
    "from queue import Queue\n",
    "import time\n",
    "\n",
    "from htools import *\n",
    "from jabberwocky.core import GuiTextChunker\n",
    "from jabberwocky.speech import Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:52:00.111219Z",
     "start_time": "2021-08-19T03:52:00.055257Z"
    }
   },
   "outputs": [],
   "source": [
    "SPEAKER = Speaker()\n",
    "CHUNKER = GuiTextChunker(max_chars=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:04:40.318909Z",
     "start_time": "2021-08-19T03:04:40.280265Z"
    }
   },
   "outputs": [],
   "source": [
    "text = 'Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you prefer. What is that?'\n",
    "text_simple = 'A dog is sad. He is alone.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:04:40.665759Z",
     "start_time": "2021-08-19T03:04:40.630503Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream(text):\n",
    "    for word in text.split(' '):\n",
    "        yield word + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:04:41.401578Z",
     "start_time": "2021-08-19T03:04:41.365786Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_response(response, errors=None, hide_on_exit=True):\n",
    "    print('\\nresponse:', response)\n",
    "    try:\n",
    "        for sent in sent_tokenize(response):\n",
    "            for chunk in sent.split('\\n\\n'):\n",
    "                SPEAKER.speak(chunk)\n",
    "                if errors:\n",
    "                    raise errors[0]\n",
    "    except RuntimeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:04:43.471559Z",
     "start_time": "2021-08-19T03:04:43.423969Z"
    }
   },
   "outputs": [],
   "source": [
    "def old_concurrent_speaking_typing(streamable, conv_mode=False, pause=.18):\n",
    "    # Stream function provides \"typing\" effect.\n",
    "    threads = []\n",
    "    errors = []\n",
    "    full_text = ''\n",
    "    curr_text = ''\n",
    "    for chunk in stream(streamable):\n",
    "        full_text += chunk\n",
    "        curr_text += chunk\n",
    "        chunked = CHUNKER.add('response', full_text)\n",
    "        print(chunked, end='')\n",
    "        if any(char in chunk for char in ('.', '!', '?', '\\n\\n')):\n",
    "            if not errors:\n",
    "                thread = Thread(target=read_response,\n",
    "                                args=(curr_text, errors, False))\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "            # Make sure this isn't reset until AFTER the speaker thread starts.\n",
    "            curr_text = ''\n",
    "        time.sleep(pause)\n",
    "    if curr_text and not errors:\n",
    "        read_response(curr_text)\n",
    "    for thread in threads: thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:04:53.080406Z",
     "start_time": "2021-08-19T03:04:44.068684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'response'\n",
      "Who \n",
      "Who are \n",
      "Who are you? \n",
      "response: Who are you? \n",
      "\n",
      "Who are you? I \n",
      "Who are you? I am \n",
      "Who are you? I am Mr. \n",
      "response: I am Mr. \n",
      "\n",
      "Who are you? I am Mr. Nichols. \n",
      "response: Nichols. \n",
      "\n",
      "Who are you? I am Mr. Nichols. You \n",
      "Who are you? I am Mr. Nichols. You can \n",
      "Who are you? I am Mr. Nichols. You can call \n",
      "Who are you? I am Mr. Nichols. You can call me \n",
      "Who are you? I am Mr. Nichols. You can call me John. \n",
      "response: You can call me John. \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or \n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., \n",
      "response: Or J.T., \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if \n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you \n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you\n",
      "prefer. \n",
      "response: if you prefer. \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you\n",
      "prefer. What \n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you\n",
      "prefer. What is \n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you\n",
      "prefer. What is that? \n",
      "response: What is that? \n"
     ]
    }
   ],
   "source": [
    "old_concurrent_speaking_typing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:09:21.635172Z",
     "start_time": "2021-08-19T03:09:21.595463Z"
    }
   },
   "outputs": [],
   "source": [
    "@coroutine\n",
    "def read_response_coro():\n",
    "    # Must send None as an extra last item so that this coroutine knows when \n",
    "    # we're done sending in new tokens so it can check for any unread text.\n",
    "    text = ''\n",
    "    while True:\n",
    "        token = yield\n",
    "        if token is None:\n",
    "            SPEAKER.speak(sents[0])\n",
    "        else:\n",
    "            text += token\n",
    "            sents = sent_tokenize(text)\n",
    "            if len(sents) > 1:\n",
    "                SPEAKER.speak(sents[0])\n",
    "                text = text.replace(sents[0], '', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:30:48.356803Z",
     "start_time": "2021-08-19T03:30:48.234113Z"
    }
   },
   "outputs": [],
   "source": [
    "# class CoroutinableThread(Thread):\n",
    "    \n",
    "#     def __init__(self, target, args=(), kwargs=None):\n",
    "#         super().__init__(target=target, args=args, kwargs=kwargs)\n",
    "#         self.target = target(*args, **(kwargs or {}))\n",
    "        \n",
    "#     def send(self, val):\n",
    "#         self.target.send(val)\n",
    "\n",
    "class CoroutinableThread(Thread):\n",
    "    \n",
    "    def __init__(self, target, queue, args=(), kwargs=None):\n",
    "        super().__init__(target=target, args=args, kwargs=kwargs)\n",
    "        self.target = target(*args, **(kwargs or {}))\n",
    "        self.queue = queue\n",
    "        \n",
    "    def run(self):\n",
    "        while True:\n",
    "            val = self.queue.get()\n",
    "            self.target.send(val)\n",
    "            # Must do this after send so our coroutine gets the sentinel.\n",
    "            if val is None: return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:31:03.317051Z",
     "start_time": "2021-08-19T03:30:59.769852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP: A \n",
      "LOOP: dog \n",
      "LOOP: is \n",
      "LOOP: sad. \n",
      "LOOP: He \n",
      "LOOP: is \n",
      "LOOP: alone. \n",
      "LOOP: None\n"
     ]
    }
   ],
   "source": [
    "q = Queue()\n",
    "thread = CoroutinableThread(target=read_response_coro, queue=q, args=())\n",
    "thread.start()\n",
    "\n",
    "for t in chain(stream(text_simple), [None]):\n",
    "    thread.queue.put(t)\n",
    "    print('LOOP:', t)\n",
    "    time.sleep(.2)\n",
    "    \n",
    "thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:12:04.594864Z",
     "start_time": "2021-08-19T03:12:04.541099Z"
    }
   },
   "outputs": [],
   "source": [
    "# thread = CoroutinableThread(target=read_response_coro, args=())\n",
    "# thread.start()\n",
    "\n",
    "# for t in chain(stream(text_simple), [None]):\n",
    "#     thread.send(t)\n",
    "#     print(t)\n",
    "#     time.sleep(.2)\n",
    "\n",
    "# thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:31:49.404791Z",
     "start_time": "2021-08-19T03:31:49.349022Z"
    }
   },
   "outputs": [],
   "source": [
    "# def concurrent_speaking_typing(streamable, conv_mode=False, pause=.18):\n",
    "#     # Stream function provides \"typing\" effect.\n",
    "#     threads = []\n",
    "#     full_text = ''\n",
    "#     thread = CoroutinableThread(target=read_response_coro, args=())\n",
    "#     thread.start()\n",
    "#     for chunk in chain(stream(streamable), [None]):\n",
    "#         if chunk is not None:\n",
    "#             full_text += chunk\n",
    "#             chunked = CHUNKER.add('response', full_text)\n",
    "#             print(chunked)\n",
    "#         thread.send(chunk)\n",
    "#         time.sleep(pause)\n",
    "#     thread.join()\n",
    "\n",
    "def concurrent_speaking_typing(streamable, conv_mode=False, pause=.18):\n",
    "    # Stream function provides \"typing\" effect.\n",
    "    full_text = ''\n",
    "    q = Queue()\n",
    "    thread = CoroutinableThread(target=read_response_coro, queue=q, args=())\n",
    "    thread.start()\n",
    "    for chunk in stream(streamable):\n",
    "        if conv_mode:\n",
    "            chunked = CHUNKER.add(\n",
    "                'conv_transcribed',\n",
    "                CONV_MANAGER.full_conversation(include_summary=False)\n",
    "            )\n",
    "        else:\n",
    "            chunked = CHUNKER.add('response', full_text)\n",
    "            full_text += chunk\n",
    "            print(chunked)\n",
    "        thread.queue.put(chunk)\n",
    "        time.sleep(pause)\n",
    "    thread.queue.put(None)\n",
    "    thread.join()\n",
    "#     hide_item(data['interrupt_id'])\n",
    "#     hide_item(data['query_msg_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:31:59.657248Z",
     "start_time": "2021-08-19T03:31:51.181492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Who \n",
      "\n",
      "Who are \n",
      "\n",
      "Who are you? \n",
      "\n",
      "Who are you? I \n",
      "\n",
      "Who are you? I am \n",
      "\n",
      "Who are you? I am Mr. \n",
      "\n",
      "Who are you? I am Mr. Nichols. \n",
      "\n",
      "Who are you? I am Mr. Nichols. You \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you\n",
      "prefer. \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you\n",
      "prefer. What \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you\n",
      "prefer. What is \n",
      "\n",
      "Who are you? I am Mr. Nichols. You can call me John. Or J.T., if you\n",
      "prefer. What is that? \n"
     ]
    }
   ],
   "source": [
    "concurrent_speaking_typing(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading scratch\n",
    "\n",
    "Question: can we start a thread within a thread?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:52:31.903204Z",
     "start_time": "2021-08-19T03:52:31.846888Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:53:42.255088Z",
     "start_time": "2021-08-19T03:53:42.204865Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def foo():\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        print(dt.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        time.sleep(1)\n",
    "        if time.time() - start >= 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:53:42.755870Z",
     "start_time": "2021-08-19T03:53:42.702348Z"
    }
   },
   "outputs": [],
   "source": [
    "def targ():\n",
    "    thread = Thread(target=foo, args=())\n",
    "    thread.start()\n",
    "    res = [i for i in sleepy_range(10, wait=.5)]\n",
    "    thread.join()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:53:48.448092Z",
     "start_time": "2021-08-19T03:53:43.375976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-18 20:53:43\n",
      "2021-08-18 20:53:44\n",
      "2021-08-18 20:53:45\n",
      "2021-08-18 20:53:46\n",
      "2021-08-18 20:53:47\n"
     ]
    }
   ],
   "source": [
    "res = targ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:53:50.975173Z",
     "start_time": "2021-08-19T03:53:50.941718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:55:17.570770Z",
     "start_time": "2021-08-19T03:55:12.486172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-18 20:55:12\n",
      "2021-08-18 20:55:13\n",
      "2021-08-18 20:55:14\n",
      "2021-08-18 20:55:15\n",
      "2021-08-18 20:55:16\n"
     ]
    }
   ],
   "source": [
    "thread = Thread(target=targ, args=())\n",
    "thread.start()\n",
    "main_res = [i for i in sleepy_range(5, wait=.1)]\n",
    "thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:55:18.462682Z",
     "start_time": "2021-08-19T03:55:18.426275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Looks like yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-neo troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T19:30:15.406311Z",
     "start_time": "2021-08-29T19:30:15.283797Z"
    }
   },
   "outputs": [],
   "source": [
    "from jabberwocky.openai_utils import query_gpt_neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:00:15.020292Z",
     "start_time": "2021-08-29T20:00:14.424973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\" \"What're you doing here?\" \"Well, I...\" \"I haven't been anywhere, sir, and I just wanted one more look.\" \" What's in the case, soldier?\" \" Oh, one book with a note.\" \"I've just opened it up.\" \"It's a letter from my daughter.\" \"I was just hoping that with your experience in the Orient, you would be able to tell me whether or not she wrote it.\" \"Now, wait.\" \"Now wait, I'll show you it, sir.\" \"Here, I don't want to take any chances.\" \"She wrote the book herself.\" \"All right, come on then, let's get that case out of here.\" \"What's more, soldier, I'll tell you what's in the case just the same.\" \"But if you're lying to me, then there's something very wrong, and I'm gonna get hold of that soldier immediately.\" \"All right.\" \"All right, Mr. President, I'll do it.\" \"You go tell them how good I am.\" \"Yes, sir.\" \"All right, come on out, come on out, come on out, come on out!\" \"Come on out!\" \"\n"
     ]
    }
   ],
   "source": [
    "# ~20s first time, <5s second time\n",
    "res = query_gpt_neo('Hello Mr. President')\n",
    "print(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:08:46.895141Z",
     "start_time": "2021-08-29T20:08:46.394219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "\n",
      "The United Nations, with the support the International Criminal Court (ICC) has of course, pursued its quest against alleged war criminals and criminals throughout the world; but the U.S. also has played a leading role in this. The United Nations, on its own motion, has put together a select commission for a fact-finding report to be made, and in its report the United Nations has found over thirty individuals, including at least twenty-five Americans, guilty. Twenty-four, in my personal opinion, are among the most culpable among the guilty. They are: William V. Ayres, a U.S. army staff Sergeant in the 3d Infantry Division in World War II, who had already received the Medal for bravery, a Purple Heart, and a Silver Star for his gallantry on December 8, 1945; James Thomas Bailey, Jr., son of John Lee Bailey, a Marine Corps General in the 2d Infantry Division, who had already received a Silver Star for gallantry for saving the life of a Marine officer in the Battle of Tarawa, on December 8, 1945; Francis Edward Butler, a colonel of infantry and a Medal of Honor recipient, who died after the Battle for Iwo Jim\n"
     ]
    }
   ],
   "source": [
    "# >1 min first time, <1s second time\n",
    "res = query_gpt_neo('Hello Mr. President', size='1.3B')\n",
    "print(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T19:59:29.695085Z",
     "start_time": "2021-08-29T19:59:29.311907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I have a question. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the way in which the United States is doing its job. I have a question about the\n"
     ]
    }
   ],
   "source": [
    "# ~17s first time, <1s second time\n",
    "res = query_gpt_neo('Hello Mr. President', size='125M')\n",
    "print(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free GPT-J api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:42:13.676176Z",
     "start_time": "2021-08-29T20:42:13.589060Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from jabberwocky.openai_utils import query_gpt3, MockFunctionException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:15:31.327570Z",
     "start_time": "2021-08-29T20:15:16.946106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'GPT-J-6B', 'compute_time': 8.96641230583191, 'text': '\\n\\nThere were actually 9 unicorns in the herd, and were eventually killed and skinned by the farmer, for their excellent hides, according to a study published on Monday.\\n\\nFor years, scientists had believed that the bovine population of South America was still in its infant stage of evolution. While they knew there was something special about the animals, they still could not figure out what it was.\\n\\nOnly when the bovine population became extinct, something which was recorded in the Book of Genesis, were the scientists able to finally figure out that these enormous, heavy-browed cattle were a new species of unicorn.\\n\\nScientists did find a way to preserve the history of these unicorns, when they noticed the family of men responsible for killing them, began speaking in perfect English.\\n\\nThrough a series of interviews, they found out that a man named Pedro Ferro had spent his life gathering \"fat unicorns\", although he did not know what he was doing when he started his hunt.\\n\\nThe conditions in the remote valley were dangerous, so the man decided to take a 40-person team to the Andes Mountains, which was home to a herd of powerful and fierce unicorns.\\n\\nAfter an extreme trek through the treacherous mountain terrain, the team was finally able to find the unicorn herd, a herd that numbered over 10,000 strong.\\n\\nUnfortunately, in the process of preserving the unicorns, the team also killed them, though they say that their deaths were not the result of intent.\\n\\nEven as more unicorns are killed, scientists are still trying to figure out why the animals are so dangerous.\\n\\nAccording to their study, unicorns are exceptionally strong and agile animals, and their evolutionary lineage is completely different from that of cattle, which makes them a separate species from their cousins.\\n\\nPeople who think they know everything about old gold rush ghost towns, think again. The newest ghost town in history is The Historic Day & Night Cemetery located in the South Shore mountains, Colorado. It was only founded in February 2014 and already it has a population of 44 people living there. Day & Night Cemetery (DNC) also has 736 graves filled with the bodies of well-to-do people.\\n\\nI was thinking the old west where you can get an honorable job like gold digging, so I looked it up on the internet to check it out, and I found the best job out there. They say that you can even buy land with that job!\\n', 'prompt': 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.', 'token_max_length': 512, 'temperature': 1.0, 'top_p': 0.9, 'stop_sequence': None}\n"
     ]
    }
   ],
   "source": [
    "# Copy-pasted from github example.\n",
    "context = \"In a shocking finding, scientist discovered a herd of unicorns \"\\\n",
    "\"living in a remote, previously unexplored valley, in the Andes Mountains. \"\\\n",
    "\"Even more surprising to the researchers was the fact that the unicorns \"\\\n",
    "\"spoke perfect English.\"\n",
    "payload = {\n",
    "    \"context\": context,\n",
    "    \"token_max_length\": 512,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "response = requests.post(\"http://api.vicgalle.net:5000/generate\", \n",
    "                         params=payload).json()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:16:06.600365Z",
     "start_time": "2021-08-29T20:16:06.549649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:15:51.647861Z",
     "start_time": "2021-08-29T20:15:51.605006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There were actually 9 unicorns in the herd, and were eventually killed and skinned by the farmer, for their excellent hides, according to a study published on Monday.\n",
      "\n",
      "For years, scientists had believed that the bovine population of South America was still in its infant stage of evolution. While they knew there was something special about the animals, they still could not figure out what it was.\n",
      "\n",
      "Only when the bovine population became extinct, something which was recorded in the Book of Genesis, were the scientists able to finally figure out that these enormous, heavy-browed cattle were a new species of unicorn.\n",
      "\n",
      "Scientists did find a way to preserve the history of these unicorns, when they noticed the family of men responsible for killing them, began speaking in perfect English.\n",
      "\n",
      "Through a series of interviews, they found out that a man named Pedro Ferro had spent his life gathering \"fat unicorns\", although he did not know what he was doing when he started his hunt.\n",
      "\n",
      "The conditions in the remote valley were dangerous, so the man decided to take a 40-person team to the Andes Mountains, which was home to a herd of powerful and fierce unicorns.\n",
      "\n",
      "After an extreme trek through the treacherous mountain terrain, the team was finally able to find the unicorn herd, a herd that numbered over 10,000 strong.\n",
      "\n",
      "Unfortunately, in the process of preserving the unicorns, the team also killed them, though they say that their deaths were not the result of intent.\n",
      "\n",
      "Even as more unicorns are killed, scientists are still trying to figure out why the animals are so dangerous.\n",
      "\n",
      "According to their study, unicorns are exceptionally strong and agile animals, and their evolutionary lineage is completely different from that of cattle, which makes them a separate species from their cousins.\n",
      "\n",
      "People who think they know everything about old gold rush ghost towns, think again. The newest ghost town in history is The Historic Day & Night Cemetery located in the South Shore mountains, Colorado. It was only founded in February 2014 and already it has a population of 44 people living there. Day & Night Cemetery (DNC) also has 736 graves filled with the bodies of well-to-do people.\n",
      "\n",
      "I was thinking the old west where you can get an honorable job like gold digging, so I looked it up on the internet to check it out, and I found the best job out there. They say that you can even buy land with that job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:17:52.644155Z",
     "start_time": "2021-08-29T20:17:50.016124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'GPT-J-6B', 'compute_time': 1.8529937267303467, 'text': ' They were also able to teleport to the lab where the study was being conducted and learn every word of the complex experiment.\\n\\nWhile looking into the study on unicorns, I found this interesting article about a study done to explain the prevalence of divination in various cultures and religions. There are those who call themselves “Herodian” or “Theonomist” but most of them make no such claim. They just believe they are following the will of God, and that', 'prompt': 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.', 'token_max_length': 100, 'temperature': 1.0, 'top_p': 0.9, 'stop_sequence': None}\n"
     ]
    }
   ],
   "source": [
    "# Streaming mode not supported (no error, but we won't get tokens streamed to\n",
    "# us).\n",
    "payload = {\n",
    "    \"context\": context,\n",
    "    \"token_max_length\": 100,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stream\": True\n",
    "}\n",
    "response = requests.post(\"http://api.vicgalle.net:5000/generate\", \n",
    "                         params=payload).json()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:28:15.816020Z",
     "start_time": "2021-08-29T20:28:13.404163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fascinating video of a group of 200 \"Gigantic\" sloths walking and climbing along the mountainside in Costa Rica.\n",
      "\n",
      "Exotic animal giraffes are normal-sized for their species, \n"
     ]
    }
   ],
   "source": [
    "# Stop sequence works slightly differently here.\n",
    "# 1. Use \"stop_sequence\" param, not \"stop\" like in openai api.\n",
    "# 2. Completion will include the stop sequence if it was generated. We should\n",
    "# strip it in postprocessing.\n",
    "payload = {\n",
    "    \"context\": context,\n",
    "    \"token_max_length\": 100,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stop_sequence\": ', '\n",
    "}\n",
    "response = requests.post(\"http://api.vicgalle.net:5000/generate\", \n",
    "                         params=payload).json()\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:29:49.495969Z",
     "start_time": "2021-08-29T20:29:49.449736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': <Parameter \"prompt\">,\n",
       " 'top_k': <Parameter \"top_k=None\">,\n",
       " 'top_p': <Parameter \"top_p=None\">,\n",
       " 'temperature': <Parameter \"temperature=1.0\">,\n",
       " 'repetition_penalty': <Parameter \"repetition_penalty=None\">,\n",
       " 'max_tokens': <Parameter \"max_tokens=250\">,\n",
       " 'api_key': <Parameter \"api_key=None\">,\n",
       " 'size': <Parameter \"size: ('125M', '1.3B', '2.7B') = '2.7B'\">,\n",
       " 'kwargs': <Parameter \"**kwargs\">}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(query_gpt_neo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T20:30:17.083702Z",
     "start_time": "2021-08-29T20:30:17.035823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': <Parameter \"prompt\">,\n",
       " 'engine_i': <Parameter \"engine_i=0\">,\n",
       " 'temperature': <Parameter \"temperature=0.7\">,\n",
       " 'frequency_penalty': <Parameter \"frequency_penalty=0.0\">,\n",
       " 'max_tokens': <Parameter \"max_tokens=50\">,\n",
       " 'logprobs': <Parameter \"logprobs=None\">,\n",
       " 'stream': <Parameter \"stream=False\">,\n",
       " 'mock': <Parameter \"mock=False\">,\n",
       " 'return_full': <Parameter \"return_full=False\">,\n",
       " 'strip_output': <Parameter \"strip_output=True\">,\n",
       " 'mock_func': <Parameter \"mock_func=None\">,\n",
       " 'mock_mode': <Parameter \"mock_mode: ('raise', 'warn', 'ignore') = 'raise'\">,\n",
       " 'kwargs': <Parameter \"**kwargs\">}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(query_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T02:24:46.841366Z",
     "start_time": "2021-09-01T02:24:46.494282Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_gpt_j(prompt, temperature=0.7, max_tokens=50, **kwargs):\n",
    "    params = {'context': prompt,\n",
    "              'token_max_length': max_tokens,\n",
    "              'temperature': temperature,\n",
    "              'top_p': kwargs.pop('top_p', 1.0)}\n",
    "    \n",
    "    # Ensure that we end up with a list AND that stop is still Falsy if user \n",
    "    # explicitly passes in stop=None.\n",
    "    stop = tolist(kwargs.pop('stop', None) or [])\n",
    "    if stop: params['stop_sequence'] = stop[0]\n",
    "        \n",
    "    # Must keep this after the block of stop-related logic above.\n",
    "    if kwargs: warnings.warn('GPT-J api does not support other kwargs.')\n",
    "    \n",
    "    try:\n",
    "        res = requests.post('http://api.vicgalle.net:5000/generate', \n",
    "                            params=params)\n",
    "        res.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise MockFunctionException(str(e)) from None\n",
    "    res = res.json()\n",
    "        \n",
    "    # Endpoint doesn't support multiple stop sequences so we have to \n",
    "    # postprocess. Even with a single stop sequence, it includes it while gpt3\n",
    "    # and my gpt-neo function exclude it, so we need to handle that here.\n",
    "    idx = min([i for i in map(res['text'].find, stop) if i >= 0] or [None])\n",
    "    completion = res['text'][:idx]\n",
    "    return res['prompt'], completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T02:24:49.664821Z",
     "start_time": "2021-09-01T02:24:47.818241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This is a conversation with Barack Obama.\\n\\nMe: Hello Barack.\\n\\nBarack Obama:',\n",
       " ' Hi.\\n\\n')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"This is a conversation with Barack Obama.\n",
    "\n",
    "Me: Hello Barack.\n",
    "\n",
    "Barack Obama:\"\"\"\n",
    "\n",
    "query_gpt_j(prompt, stop='Me: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T02:25:46.682090Z",
     "start_time": "2021-09-01T02:25:45.012977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This is a conversation with Barack Obama.\\n\\nMe: Hello Barack.\\n\\nBarack Obama:',\n",
       " ' Hello.\\n\\n')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gpt_j(prompt, stop='Me: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
